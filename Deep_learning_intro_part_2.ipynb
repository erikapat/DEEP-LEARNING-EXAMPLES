{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reuters dataset\n",
    "\n",
    "Classifying newswires: a multiclass classification example\n",
    "\n",
    "You’ll work with the Reuters dataset, a set of short newswires and their topics, published\n",
    "by Reuters in 1986. It’s a simple, widely used toy dataset for text classification. There\n",
    "are 46 different topics; some topics are more represented than others, but each topic\n",
    "has at least 10 examples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
    "num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decoding newswires back to text\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
    "train_data[0]])\n",
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 43,\n",
       " 10,\n",
       " 447,\n",
       " 5,\n",
       " 25,\n",
       " 207,\n",
       " 270,\n",
       " 5,\n",
       " 3095,\n",
       " 111,\n",
       " 16,\n",
       " 369,\n",
       " 186,\n",
       " 90,\n",
       " 67,\n",
       " 7,\n",
       " 89,\n",
       " 5,\n",
       " 19,\n",
       " 102,\n",
       " 6,\n",
       " 19,\n",
       " 124,\n",
       " 15,\n",
       " 90,\n",
       " 67,\n",
       " 84,\n",
       " 22,\n",
       " 482,\n",
       " 26,\n",
       " 7,\n",
       " 48,\n",
       " 4,\n",
       " 49,\n",
       " 8,\n",
       " 864,\n",
       " 39,\n",
       " 209,\n",
       " 154,\n",
       " 6,\n",
       " 151,\n",
       " 6,\n",
       " 83,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 155,\n",
       " 11,\n",
       " 15,\n",
       " 7,\n",
       " 48,\n",
       " 9,\n",
       " 4579,\n",
       " 1005,\n",
       " 504,\n",
       " 6,\n",
       " 258,\n",
       " 6,\n",
       " 272,\n",
       " 11,\n",
       " 15,\n",
       " 22,\n",
       " 134,\n",
       " 44,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 8,\n",
       " 197,\n",
       " 1245,\n",
       " 90,\n",
       " 67,\n",
       " 52,\n",
       " 29,\n",
       " 209,\n",
       " 30,\n",
       " 32,\n",
       " 132,\n",
       " 6,\n",
       " 109,\n",
       " 15,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The label associated with an example is an integer between 0 and 45—a topic index:\n",
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the data\n",
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "#Vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize the labels, there are two possibilities: you can cast the label list as an inte-\n",
    "ger tensor, or you can use one-hot encoding. One-hot encoding is a widely used for-\n",
    "mat for categorical data, also called categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the IMDB example, you used 16-dimensional intermediate layers, but a 16-dimensional space may\n",
    "be too limited to learn to separate 46 different classes: such small layers may act as infor-\n",
    "mation bottlenecks, permanently dropping relevant information.\n",
    "For this reason you’ll use larger layers. Let’s go with 64 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "#model = models.Sequential()\n",
    "#model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "#model.add(layers.Dense(64, activation='relu'))\n",
    "#model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "from keras import regularizers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two other things you should note about this architecture:\n",
    "    \n",
    "* You end the network with a Dense layer of size 46. This means for each input\n",
    "sample, the network will output a 46-dimensional vector. Each entry in this vec-\n",
    "tor (each dimension) will encode a different output class.\n",
    "* The last layer uses a softmax activation. You saw this pattern in the MNIST\n",
    "example. It means the network will output a probability distribution over the 46\n",
    "different output classes—for every input sample, the network will produce a 46-\n",
    "dimensional output vector, where output[i] is the probability that the sample\n",
    "belongs to class i . The 46 scores will sum to 1.\n",
    "\n",
    "The best loss function to use in this case is categorical_crossentropy . It measures\n",
    "the distance between two probability distributions: here, between the probability dis-\n",
    "tribution output by the network and the true distribution of the labels. By minimizing\n",
    "the distance between these two distributions, you train the network to output some-\n",
    "thing as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s 390us/step - loss: 15.6909 - acc: 0.4599 - val_loss: 6.2428 - val_acc: 0.5400\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 273us/step - loss: 5.8119 - acc: 0.5672 - val_loss: 5.2896 - val_acc: 0.5670\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 282us/step - loss: 5.0499 - acc: 0.6019 - val_loss: 4.6760 - val_acc: 0.6500\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s 282us/step - loss: 4.5335 - acc: 0.6193 - val_loss: 4.2128 - val_acc: 0.6530\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 291us/step - loss: 4.1241 - acc: 0.6447 - val_loss: 3.8657 - val_acc: 0.6580\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 288us/step - loss: 3.8179 - acc: 0.6501 - val_loss: 3.6958 - val_acc: 0.6850\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 292us/step - loss: 3.6051 - acc: 0.6620 - val_loss: 3.4518 - val_acc: 0.6740\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s 277us/step - loss: 3.4801 - acc: 0.6586 - val_loss: 3.3576 - val_acc: 0.6760\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s 311us/step - loss: 3.3899 - acc: 0.6680 - val_loss: 3.2716 - val_acc: 0.6820\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s 300us/step - loss: 3.3149 - acc: 0.6733 - val_loss: 3.1920 - val_acc: 0.6970\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 3s 338us/step - loss: 3.2573 - acc: 0.6769 - val_loss: 3.1424 - val_acc: 0.6940\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s 291us/step - loss: 3.2162 - acc: 0.6768 - val_loss: 3.1411 - val_acc: 0.6910\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 278us/step - loss: 3.2024 - acc: 0.6740 - val_loss: 3.0954 - val_acc: 0.6880\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 279us/step - loss: 3.1538 - acc: 0.6808 - val_loss: 3.0799 - val_acc: 0.6960\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s 275us/step - loss: 3.1366 - acc: 0.6794 - val_loss: 3.0586 - val_acc: 0.6900\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s 277us/step - loss: 3.1194 - acc: 0.6819 - val_loss: 3.0622 - val_acc: 0.6910\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 283us/step - loss: 3.0922 - acc: 0.6857 - val_loss: 3.0277 - val_acc: 0.7000\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 275us/step - loss: 3.0874 - acc: 0.6835 - val_loss: 3.0006 - val_acc: 0.7020\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 286us/step - loss: 3.0712 - acc: 0.6852 - val_loss: 2.9550 - val_acc: 0.7010\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 285us/step - loss: 3.0572 - acc: 0.6850 - val_loss: 2.9407 - val_acc: 0.7030\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "partial_y_train,\n",
    "epochs=20,\n",
    "batch_size=512,\n",
    "validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "#Clears the figure\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 1s 406us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.014612325259011, 0.6861086375779163]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network begins to overfit after nine epochs. Let’s train a new network from\n",
    "scratch for nine epochs and then evaluate it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 1s 163us/step - loss: 2.5383 - acc: 0.5227 - val_loss: 1.6846 - val_acc: 0.6490\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 1s 123us/step - loss: 1.3794 - acc: 0.7107 - val_loss: 1.2821 - val_acc: 0.7200\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 1s 115us/step - loss: 1.0204 - acc: 0.7783 - val_loss: 1.1335 - val_acc: 0.7520\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 1s 116us/step - loss: 0.8024 - acc: 0.8250 - val_loss: 1.0555 - val_acc: 0.7600\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 1s 121us/step - loss: 0.6417 - acc: 0.8621 - val_loss: 0.9812 - val_acc: 0.7970\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 1s 124us/step - loss: 0.5138 - acc: 0.8930 - val_loss: 0.9120 - val_acc: 0.8120\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 1s 114us/step - loss: 0.4148 - acc: 0.9141 - val_loss: 0.8955 - val_acc: 0.8230\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 1s 113us/step - loss: 0.3383 - acc: 0.9276 - val_loss: 0.8774 - val_acc: 0.8260\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s 112us/step - loss: 0.2824 - acc: 0.9356 - val_loss: 0.9424 - val_acc: 0.7990\n",
      "2246/2246 [==============================] - 0s 185us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "partial_y_train,\n",
    "epochs=9,\n",
    "batch_size=512,\n",
    "validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0225942767526248, 0.7778272484947504]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46,)\n",
      "1.0000002\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#Generating predictions for new data\n",
    "predictions = model.predict(x_test)\n",
    "#Each entry in predictions is a vector of length 46:\n",
    "print(predictions[0].shape)\n",
    "\n",
    "#The coefficients in this vector sum to 1:\n",
    "print(np.sum(predictions[0]))\n",
    "\n",
    "#The largest entry is the predicted class—the class with the highest probability:\n",
    "print(np.argmax(predictions[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 46)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 10000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A different way to handle the labels and the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "#The only thing this approach would change is the choice of the loss function. The loss\n",
    "#function used in listing 3.21, categorical_crossentropy , expects the labels to follow\n",
    "#a categorical encoding. With integer labels, you should use sparse_categorical_\n",
    "#crossentropy :\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='sparse_categorical_crossentropy',\n",
    "metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of having sufficiently large intermediate layers\n",
    "\n",
    "We mentioned earlier that because the final outputs are 46-dimensional, you should\n",
    "avoid intermediate layers with many fewer than 46 hidden units. Now let’s see what\n",
    "happens when you introduce an information bottleneck by having intermediate layers\n",
    "that are significantly less than 46-dimensional: for example, 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s 261us/step - loss: 3.1594 - acc: 0.2402 - val_loss: 2.5913 - val_acc: 0.2940\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s 201us/step - loss: 2.0399 - acc: 0.5537 - val_loss: 1.6958 - val_acc: 0.5850\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s 192us/step - loss: 1.4980 - acc: 0.6257 - val_loss: 1.5146 - val_acc: 0.6400\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s 187us/step - loss: 1.2881 - acc: 0.6931 - val_loss: 1.4181 - val_acc: 0.6730\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s 198us/step - loss: 1.1461 - acc: 0.7160 - val_loss: 1.3660 - val_acc: 0.6810\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s 203us/step - loss: 1.0414 - acc: 0.7315 - val_loss: 1.3453 - val_acc: 0.6950\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s 195us/step - loss: 0.9588 - acc: 0.7419 - val_loss: 1.3434 - val_acc: 0.6990\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 1s 183us/step - loss: 0.8902 - acc: 0.7519 - val_loss: 1.3397 - val_acc: 0.7050\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s 181us/step - loss: 0.8267 - acc: 0.7674 - val_loss: 1.3599 - val_acc: 0.7100\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s 183us/step - loss: 0.7654 - acc: 0.7895 - val_loss: 1.3805 - val_acc: 0.7070\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s 188us/step - loss: 0.7121 - acc: 0.8079 - val_loss: 1.4017 - val_acc: 0.7160\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s 184us/step - loss: 0.6665 - acc: 0.8181 - val_loss: 1.3995 - val_acc: 0.7200\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s 199us/step - loss: 0.6238 - acc: 0.8306 - val_loss: 1.4455 - val_acc: 0.7110\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s 207us/step - loss: 0.5871 - acc: 0.8358 - val_loss: 1.4728 - val_acc: 0.7170\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s 175us/step - loss: 0.5538 - acc: 0.8452 - val_loss: 1.5165 - val_acc: 0.7170\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s 182us/step - loss: 0.5225 - acc: 0.8548 - val_loss: 1.5544 - val_acc: 0.7160\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s 197us/step - loss: 0.4946 - acc: 0.8644 - val_loss: 1.5817 - val_acc: 0.7180\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s 192us/step - loss: 0.4704 - acc: 0.8792 - val_loss: 1.6356 - val_acc: 0.7170\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s 192us/step - loss: 0.4450 - acc: 0.8847 - val_loss: 1.6688 - val_acc: 0.7110\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s 191us/step - loss: 0.4276 - acc: 0.8891 - val_loss: 1.7259 - val_acc: 0.7170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90862a9278>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "partial_y_train,\n",
    "epochs=20,\n",
    "batch_size=128,\n",
    "validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 0s 170us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9532256251663793, 0.6883348174532502]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network now peaks at ~71% validation accuracy, an 8% absolute drop. This drop\n",
    "is mostly due to the fact that you’re trying to compress a lot of information (enough\n",
    "information to recover the separation hyperplanes of 46 classes) into an intermediate\n",
    "space that is too low-dimensional. The network is able to cram most of the necessary\n",
    "information into these eight-dimensional representations, but not all of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s what you should take away from this example:\n",
    "* If you’re trying to classify data points among N classes, your network should end with a Dense layer of size N .\n",
    "* In a single-label, multiclass classification problem, your network should end\n",
    "with a softmax activation so that it will output a probability distribution over the\n",
    "N output classes.\n",
    "* Categorical crossentropy is almost always the loss function you should use for\n",
    "such problems. It minimizes the distance between the probability distributions\n",
    "output by the network and the true distribution of the targets.\n",
    "* There are two ways to handle labels in multiclass classification:\n",
    "- Encoding the labels via categorical encoding (also known as one-hot encod-\n",
    "ing) and using categorical_crossentropy as a loss function\n",
    "- Encoding the labels as integers and using the sparse_categorical_crossentropy\n",
    "loss function\n",
    "* If you need to classify data into a large number of categories, you should avoid\n",
    "creating information bottlenecks in your network due to intermediate layers\n",
    "that are too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVOLUTIONAL NEURAL NETWORKS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT SEQUENCES\n",
    "\n",
    "Here we explores deep-learning models that can process text (understood as\n",
    "sequences of word or sequences of characters), timeseries, and sequence data in\n",
    "general. The two fundamental deep-learning algorithms for sequence processing\n",
    "are recurrent neural networks and **1D convnets**, the one-dimensional version of the 2D\n",
    "convnets that we covered in the previous chapters. We’ll discuss both of these\n",
    "approaches in this chapter.\n",
    "Applications of these algorithms include the following:\n",
    "* Document classification and timeseries classification, such as identifying the\n",
    "topic of an article or the author of a book\n",
    "* Timeseries comparisons, such as estimating how closely related two docu-\n",
    "ments or two stock tickers are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequence-to-sequence learning, such as decoding an English sentence into\n",
    "French\n",
    "* Sentiment analysis, such as classifying the sentiment of tweets or movie reviews\n",
    "as positive or negative\n",
    "* Timeseries forecasting, such as predicting the future weather at a certain loca-\n",
    "tion, given recent weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text data\n",
    "Text is one of the most widespread forms of sequence data. It can be understood as\n",
    "either a sequence of characters or a sequence of words, but it’s most common to work\n",
    "at the level of words\n",
    "\n",
    "wE can use text to produce a basic form of natural-language under-\n",
    "standing, sufficient for applications including document classification, sentiment\n",
    "analysis, author identification, and even question-answering ( QA ) (in a constrained\n",
    "context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing** text is the process of transforming text\n",
    "into numeric tensors. This can be done in multiple ways:\n",
    "* Segment text into words, and transform each word into a vector.\n",
    "* Segment text into characters, and transform each character into a vector.\n",
    "* Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
    "N -grams are overlapping groups of multiple consecutive words or characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collectively, the different units into which you can break down text (words, charac-\n",
    "ters, or n-grams) are called **tokens**, and breaking text into such tokens is called **tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All text-vectorization processes consist of applying some tokenization scheme and\n",
    "then associating numeric vectors with the generated tokens. These vectors, packed\n",
    "into sequence tensors, are fed into deep neural networks. There are multiple ways to\n",
    "associate a vector with a token. In this section, I’ll present two major ones: **one-hot\n",
    "encoding of tokens**, and **token embedding** (typically used exclusively for words, and called\n",
    "word embedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding n-grams and bag-of-words\n",
    "Word n-grams are groups of N (or fewer) consecutive words that you can extract from\n",
    "a sentence. The same concept may also be applied to characters instead of words.\n",
    "Here’s a simple example. Consider the sentence “The cat sat on the mat.” It may be\n",
    "decomposed into the following set of 2-grams:\n",
    "\n",
    "    {\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\",\n",
    "\"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\n",
    "\n",
    "It may also be decomposed into the following set of 3-grams:\n",
    "\n",
    "    {\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\",\n",
    "\"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\",\n",
    "\"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
    "\n",
    "    Such a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term bag\n",
    "here refers to the fact that you’re dealing with a set of tokens rather than a list or\n",
    "sequence: the tokens have no specific order. This family of tokenization methods is\n",
    "called **bag-of-words**.\n",
    "\n",
    "Because bag-of-words isn’t an order-preserving tokenization method (the tokens gen-\n",
    "erated are understood as a set, not a sequence, and the general structure of the sen-\n",
    "tences is lost), it tends to be used in shallow language-processing models rather than\n",
    "in deep-learning models. Extracting n-grams is a form of feature engineering, and\n",
    "deep learning does away with this kind of rigid, brittle approach, replacing it with hier-\n",
    "archical feature learning. One-dimensional convnets and recurrent neural networks, are capable of learning representations for groups of\n",
    "words and characters without being explicitly told about the existence of such groups,\n",
    "by looking at continuous word or character sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of words and characters\n",
    "One-hot encoding is the most common, most basic way to turn a token into a vector.\n",
    "You saw it in action in the initial IMDB and Reuters examples(done with\n",
    "words, in that case). It consists of associating a unique integer index with every word\n",
    "and then turning this integer index i into a binary vector of size N (the size of the\n",
    "vocabulary); the vector is all zeros except for the i th entry, which is 1.\n",
    "Of course, one-hot encoding can be done at the character level, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variant of one-hot encoding is the so-called one-hot hashing trick, which you can use\n",
    "when the number of unique tokens in your vocabulary is too large to handle explicitly. Instead of explicitly assigning an index to each word and keeping a reference of these\n",
    "indices in a dictionary, you can hash words into vectors of fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "dimensionality = 1000\n",
    "max_length = 10\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings\n",
    "Another popular and powerful way to associate a vector with a word is the use of dense\n",
    "word vectors, also called word embeddings. Whereas the vectors obtained through one-hot\n",
    "encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same\n",
    "dimensionality as the number of words in the vocabulary), word embeddings are low-\n",
    "dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vec-\n",
    "tors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to obtain word embeddings:\n",
    "* Learn word embeddings jointly with the main task you care about (such as doc-\n",
    "ument classification or sentiment prediction). In this setup, you start with ran-\n",
    "dom word vectors and then learn word vectors in the same way you learn the\n",
    "weights of a neural network.\n",
    "* Load into your model word embeddings that were precomputed using a differ-\n",
    "ent machine-learning task than the one you’re trying to solve. These are called\n",
    "pretrained word embeddings.\n",
    "\n",
    "## Embedding layer\n",
    "`from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000, 64) # looking at the first 64 words in every review. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layer on top leads to a model that\n",
    "treats each word in the input sequence separately, without considering inter-word\n",
    "relationships and sentence structure (for example, this model would likely treat both\n",
    "“this movie is a bomb” and “this movie is the bomb” as being negative reviews). It’s\n",
    "much better to add recurrent layers or 1D convolutional layers on top of the embed-\n",
    "ded sequences to learn features that take into account each sequence as a whole.\n",
    "That’s what we’ll focus on in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING PRETRAINED WORD EMBEDDINGS\n",
    "Sometimes, you have so little training data available that you can’t use your data\n",
    "alone to learn an appropriate task-specific embedding of your vocabulary. What do\n",
    "you do then?\n",
    "\n",
    "There are various precomputed databases of word embeddings that you can down-\n",
    "load and use in a Keras Embedding layer. Word2vec is one of them. Another popular\n",
    "one is called Global Vectors for Word Representation (GloVe, https://nlp.stanford\n",
    ".edu/projects/glove), which was developed by Stanford researchers in 2014.\n",
    "\n",
    "## Understanding recurrent neural networks\n",
    "\n",
    "A major characteristic of all neural networks you’ve seen so far, such as densely con-\n",
    "nected networks and convnets, is that they have no memory. Each input shown to\n",
    "them is processed independently, with no state kept in between inputs. With such net-\n",
    "works, in order to process a sequence or a temporal series of data points, you have to\n",
    "show the entire sequence to the network at once: turn it into a single data point. For\n",
    "instance, this is what you did in the IMDB example: an entire movie review was trans-\n",
    "formed into a single large vector and processed in one go. Such networks are called\n",
    "feedforward networks.\n",
    "\n",
    "\n",
    "## Understanding the LSTM and GRU layers\n",
    "SimpleRNN isn’t the only recurrent layer available in Keras. There are two others: LSTM\n",
    "and GRU . In practice, you’ll always use one of these, because SimpleRNN is generally too\n",
    "simplistic to be of real use. SimpleRNN has a major issue: although it should theoretically\n",
    "be able to retain at time t information about inputs seen many timesteps before, in\n",
    "practice, such long-term dependencies are impossible to learn. This is due to the van-\n",
    "ishing gradient problem, an effect that is similar to what is observed with non-recurrent\n",
    "networks (feedforward networks) that are many layers deep: as you keep adding layers\n",
    "to a network, the network eventually becomes untrainable. The theoretical reasons for\n",
    "this effect were studied by Hochreiter, Schmidhuber, and Bengio in the early 1990s. 2\n",
    "\n",
    "The LSTM and GRU layers are designed to solve this problem.\n",
    "\n",
    "Let’s consider the LSTM layer. The underlying Long Short-Term Memory ( LSTM )\n",
    "algorithm was developed by Hochreiter and Schmidhuber in 1997; 3 it was the culmi-\n",
    "nation of their research on the vanishing gradient problem.\n",
    "\n",
    "This layer is a variant of the SimpleRNN layer you already know about; it adds a way\n",
    "to carry information across many timesteps. Imagine a conveyor belt running parallel\n",
    "to the sequence you’re processing. Information from the sequence can jump onto the\n",
    "conveyor belt at any point, be transported to a later timestep, and jump off, intact,\n",
    "when you need it. This is essentially what LSTM does: it saves information for later,\n",
    "thus preventing older signals from gradually vanishing during processing.\n",
    "\n",
    "**Just keep in mind what the LSTM cell is meant to do: allow past informa-\n",
    "tion to be reinjected at a later time, thus fighting the vanishing-gradient problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A temperature-forecasting problem (NO FUNCIONA --> IR A TIME SERIES)\n",
    "\n",
    "You have access to a timeseries of data points coming from sensors installed on\n",
    "the roof of a building, such as temperature, air pressure, and humidity, which you use\n",
    "to predict what the temperature will be 24 hours after the last data point. This is a\n",
    "fairly challenging problem that exemplifies many common difficulties encountered\n",
    "when working with timeseries.\n",
    "\n",
    "We’ll cover the following techniques:\n",
    "\n",
    "* Recurrent dropout—This is a specific, built-in way to use dropout to fight overfit-\n",
    "ting in recurrent layers.\n",
    "* **Stacking recurrent layers**—This increases the representational power of the net-\n",
    "work (at the cost of higher computational loads).\n",
    "* Bidirectional recurrent layers—These present the same information to a recurrent\n",
    "network in different ways, increasing accuracy and mitigating forgetting issues.\n",
    "\n",
    "In this dataset, 14 different quantities (such air temperature, atmospheric pres-\n",
    "sure, humidity, wind direction, and so on) were recorded every 10 minutes, over sev-\n",
    "eral years. The original data goes back to 2003, but this example is limited to data\n",
    "from 2009–2016. This dataset is perfect for learning to work with numerical\n",
    "timeseries. You’ll use it to build a model that takes as input some data from the recent\n",
    "past (a few days’ worth of data points) and predicts the air temperature 24 hours in\n",
    "the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cd ~/Downloads\n",
    "mkdir jena_climate\n",
    "cd jena_climate\n",
    "wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
    "unzip jena_climate_2009_2016.csv.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
      "420551\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir = '/home/erikapat/Downloads/jena_climate'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01.01.2009 00:10:00,996.52,-8.02,265.40,-8.90,93.30,3.33,3.11,0.22,1.94,3.12,1307.75,1.03,1.75,152.30',\n",
       " '01.01.2009 00:20:00,996.57,-8.41,265.01,-9.28,93.40,3.23,3.02,0.21,1.89,3.03,1309.80,0.72,1.50,136.10',\n",
       " '01.01.2009 00:30:00,996.53,-8.51,264.91,-9.31,93.90,3.21,3.01,0.20,1.88,3.02,1310.24,0.19,0.63,171.60',\n",
       " '01.01.2009 00:40:00,996.51,-8.31,265.12,-9.07,94.20,3.26,3.07,0.19,1.92,3.08,1309.19,0.34,0.50,198.00',\n",
       " '01.01.2009 00:50:00,996.51,-8.27,265.15,-9.04,94.10,3.27,3.08,0.19,1.92,3.09,1309.00,0.32,0.63,214.30',\n",
       " '01.01.2009 01:00:00,996.50,-8.05,265.38,-8.78,94.40,3.33,3.14,0.19,1.96,3.15,1307.86,0.21,0.63,192.70',\n",
       " '01.01.2009 01:10:00,996.50,-7.62,265.81,-8.30,94.80,3.44,3.26,0.18,2.04,3.27,1305.68,0.18,0.63,166.50',\n",
       " '01.01.2009 01:20:00,996.50,-7.62,265.81,-8.36,94.40,3.44,3.25,0.19,2.03,3.26,1305.69,0.19,0.50,118.60',\n",
       " '01.01.2009 01:30:00,996.50,-7.91,265.52,-8.73,93.80,3.36,3.15,0.21,1.97,3.16,1307.17,0.28,0.75,188.50',\n",
       " '01.01.2009 01:40:00,996.53,-8.43,264.99,-9.34,93.10,3.23,3.00,0.22,1.88,3.02,1309.85,0.59,0.88,185.00',\n",
       " '01.01.2009 01:50:00,996.62,-8.76,264.66,-9.66,93.10,3.14,2.93,0.22,1.83,2.94,1311.64,0.45,0.88,183.20',\n",
       " '01.01.2009 02:00:00,996.62,-8.88,264.54,-9.77,93.20,3.12,2.90,0.21,1.81,2.91,1312.25,0.25,0.63,190.30',\n",
       " '01.01.2009 02:10:00,996.63,-8.85,264.57,-9.70,93.50,3.12,2.92,0.20,1.82,2.93,1312.11,0.16,0.50,158.30',\n",
       " '01.01.2009 02:20:00,996.74,-8.83,264.58,-9.68,93.50,3.13,2.92,0.20,1.83,2.93,1312.15,0.36,0.63,184.80',\n",
       " '01.01.2009 02:30:00,996.81,-8.66,264.74,-9.46,93.90,3.17,2.98,0.19,1.86,2.99,1311.37,0.33,0.75,155.90',\n",
       " '01.01.2009 02:40:00,996.81,-8.66,264.74,-9.50,93.60,3.17,2.97,0.20,1.85,2.98,1311.38,0.07,0.50,272.40',\n",
       " '01.01.2009 02:50:00,996.86,-8.70,264.70,-9.55,93.50,3.16,2.95,0.21,1.85,2.96,1311.64,0.32,0.63,219.20',\n",
       " '01.01.2009 03:00:00,996.84,-8.81,264.59,-9.66,93.50,3.13,2.93,0.20,1.83,2.94,1312.18,0.18,0.63,167.20',\n",
       " '01.01.2009 03:10:00,996.87,-8.84,264.56,-9.69,93.50,3.13,2.92,0.20,1.83,2.93,1312.37,0.07,0.25,129.30',\n",
       " '01.01.2009 03:20:00,996.97,-8.94,264.45,-9.82,93.30,3.10,2.89,0.21,1.81,2.90,1313.01,0.10,0.63,115.30',\n",
       " '01.01.2009 03:30:00,997.08,-8.94,264.44,-9.80,93.40,3.10,2.90,0.20,1.81,2.90,1313.15,0.30,0.75,149.30',\n",
       " '01.01.2009 03:40:00,997.10,-8.86,264.52,-9.76,93.10,3.12,2.90,0.22,1.81,2.91,1312.78,0.29,0.75,149.70',\n",
       " '01.01.2009 03:50:00,997.06,-8.99,264.39,-9.99,92.40,3.09,2.85,0.23,1.78,2.86,1313.39,0.12,0.63,231.70',\n",
       " '01.01.2009 04:00:00,996.99,-9.05,264.34,-10.02,92.60,3.07,2.85,0.23,1.78,2.85,1313.61,0.10,0.38,240.00',\n",
       " '01.01.2009 04:10:00,997.05,-9.23,264.15,-10.25,92.20,3.03,2.79,0.24,1.74,2.80,1314.62,0.10,0.38,203.90',\n",
       " '01.01.2009 04:20:00,997.11,-9.49,263.89,-10.54,92.00,2.97,2.73,0.24,1.71,2.74,1316.02,0.34,0.75,159.70',\n",
       " '01.01.2009 04:30:00,997.19,-9.50,263.87,-10.51,92.30,2.97,2.74,0.23,1.71,2.75,1316.16,0.43,0.88,66.16',\n",
       " '01.01.2009 04:40:00,997.24,-9.35,264.02,-10.29,92.80,3.00,2.79,0.22,1.74,2.79,1315.47,0.40,0.88,105.00',\n",
       " '01.01.2009 04:50:00,997.37,-9.47,263.89,-10.46,92.40,2.97,2.75,0.23,1.72,2.75,1316.25,0.37,0.75,125.80',\n",
       " '01.01.2009 05:00:00,997.46,-9.63,263.72,-10.65,92.20,2.94,2.71,0.23,1.69,2.71,1317.19,0.40,0.88,157.00',\n",
       " '01.01.2009 05:10:00,997.43,-9.67,263.68,-10.63,92.60,2.93,2.71,0.22,1.69,2.72,1317.35,0.36,0.75,132.50',\n",
       " '01.01.2009 05:20:00,997.42,-9.68,263.67,-10.73,92.00,2.92,2.69,0.23,1.68,2.70,1317.40,0.09,0.50,143.20',\n",
       " '01.01.2009 05:30:00,997.53,-9.90,263.45,-10.98,91.70,2.87,2.64,0.24,1.64,2.64,1318.68,0.29,1.00,72.50',\n",
       " '01.01.2009 05:40:00,997.60,-9.91,263.43,-10.90,92.40,2.87,2.65,0.22,1.66,2.66,1318.81,0.50,1.00,60.72',\n",
       " '01.01.2009 05:50:00,997.62,-9.51,263.83,-10.37,93.40,2.96,2.77,0.20,1.73,2.77,1316.77,0.31,0.75,147.40',\n",
       " '01.01.2009 06:00:00,997.71,-9.67,263.66,-10.62,92.70,2.93,2.71,0.21,1.69,2.72,1317.71,0.05,0.50,146.00',\n",
       " '01.01.2009 06:10:00,997.81,-9.59,263.74,-10.47,93.20,2.95,2.74,0.20,1.71,2.75,1317.44,0.24,0.75,161.60',\n",
       " '01.01.2009 06:20:00,997.86,-9.15,264.17,-10.02,93.30,3.05,2.85,0.20,1.78,2.85,1315.26,0.18,0.50,125.70',\n",
       " '01.01.2009 06:30:00,998.00,-8.91,264.40,-9.89,92.50,3.11,2.87,0.23,1.79,2.88,1314.23,0.45,0.88,67.11',\n",
       " '01.01.2009 06:40:00,998.14,-9.04,264.26,-10.11,91.90,3.08,2.83,0.25,1.76,2.83,1315.09,1.48,2.75,14.00',\n",
       " '01.01.2009 06:50:00,998.21,-9.43,263.86,-10.57,91.30,2.98,2.72,0.26,1.70,2.73,1317.18,2.51,3.63,357.00',\n",
       " '01.01.2009 07:00:00,998.33,-9.17,264.12,-10.10,92.90,3.04,2.83,0.22,1.76,2.83,1315.98,2.08,2.88,348.80',\n",
       " '01.01.2009 07:10:00,998.50,-8.71,264.56,-9.63,93.00,3.16,2.94,0.22,1.83,2.94,1313.87,2.06,2.75,338.10',\n",
       " '01.01.2009 07:20:00,998.59,-8.55,264.72,-9.47,93.00,3.20,2.97,0.22,1.85,2.98,1313.17,1.95,2.38,357.90',\n",
       " '01.01.2009 07:30:00,998.79,-8.40,264.85,-9.31,93.10,3.24,3.01,0.22,1.88,3.02,1312.67,0.92,1.38,340.10',\n",
       " '01.01.2009 07:40:00,998.86,-8.30,264.95,-9.21,93.10,3.26,3.04,0.22,1.89,3.04,1312.25,0.81,1.13,321.10',\n",
       " '01.01.2009 07:50:00,999.04,-8.13,265.10,-9.03,93.20,3.30,3.08,0.22,1.92,3.08,1311.63,0.61,1.13,254.20',\n",
       " '01.01.2009 08:00:00,999.17,-8.10,265.12,-9.05,92.80,3.31,3.07,0.24,1.92,3.08,1311.65,0.72,1.25,213.90',\n",
       " '01.01.2009 08:10:00,999.27,-8.14,265.08,-9.12,92.60,3.30,3.06,0.24,1.91,3.06,1311.98,0.71,1.00,241.40',\n",
       " '01.01.2009 08:20:00,999.33,-8.06,265.15,-9.02,92.70,3.32,3.08,0.24,1.92,3.08,1311.66,0.53,0.88,242.50',\n",
       " '01.01.2009 08:30:00,999.44,-7.95,265.25,-8.93,92.60,3.35,3.10,0.25,1.93,3.10,1311.26,0.53,0.88,352.70',\n",
       " '01.01.2009 08:40:00,999.46,-7.74,265.46,-8.67,93.00,3.41,3.17,0.24,1.97,3.17,1310.21,0.38,0.75,100.20',\n",
       " '01.01.2009 08:50:00,999.59,-7.57,265.62,-8.64,92.00,3.45,3.18,0.28,1.98,3.18,1309.54,0.17,0.50,166.70',\n",
       " '01.01.2009 09:00:00,999.69,-7.66,265.52,-8.84,91.20,3.43,3.13,0.30,1.95,3.13,1310.14,0.34,0.63,202.20',\n",
       " '01.01.2009 09:10:00,999.79,-7.71,265.47,-8.87,91.30,3.41,3.12,0.30,1.94,3.12,1310.51,0.14,0.50,235.00',\n",
       " '01.01.2009 09:20:00,999.81,-7.56,265.61,-8.67,91.70,3.45,3.17,0.29,1.97,3.17,1309.78,0.11,0.38,310.50',\n",
       " '01.01.2009 09:30:00,999.83,-7.29,265.88,-8.33,92.20,3.53,3.25,0.28,2.03,3.25,1308.43,0.75,2.50,17.30',\n",
       " '01.01.2009 09:40:00,999.96,-7.15,266.01,-8.21,92.10,3.57,3.28,0.28,2.05,3.28,1307.90,1.62,2.75,4.63',\n",
       " '01.01.2009 09:50:00,1000.13,-7.02,266.13,-8.06,92.20,3.60,3.32,0.28,2.07,3.32,1307.46,1.24,3.13,314.30',\n",
       " '01.01.2009 10:00:00,1000.27,-7.04,266.10,-8.17,91.60,3.60,3.30,0.30,2.05,3.29,1307.76,1.45,3.00,292.60',\n",
       " '01.01.2009 10:10:00,1000.43,-7.03,266.10,-8.16,91.60,3.60,3.30,0.30,2.05,3.30,1307.91,1.12,2.75,266.30',\n",
       " '01.01.2009 10:20:00,1000.54,-7.15,265.97,-8.34,91.10,3.57,3.25,0.32,2.02,3.25,1308.67,2.12,3.25,267.20',\n",
       " '01.01.2009 10:30:00,1000.68,-7.26,265.85,-8.47,91.00,3.54,3.22,0.32,2.00,3.22,1309.41,1.54,3.25,258.80',\n",
       " '01.01.2009 10:40:00,1000.78,-7.34,265.76,-8.57,90.80,3.51,3.19,0.32,1.99,3.19,1309.96,1.81,3.00,257.00',\n",
       " '01.01.2009 10:50:00,1000.83,-7.35,265.75,-8.57,90.90,3.51,3.19,0.32,1.99,3.19,1310.07,1.77,3.25,260.50',\n",
       " '01.01.2009 11:00:00,1000.87,-7.41,265.68,-8.66,90.70,3.50,3.17,0.33,1.97,3.17,1310.43,1.91,3.38,269.60',\n",
       " '01.01.2009 11:10:00,1000.81,-7.48,265.62,-8.76,90.50,3.48,3.15,0.33,1.96,3.14,1310.71,1.73,3.25,247.10',\n",
       " '01.01.2009 11:20:00,1000.74,-7.38,265.72,-8.64,90.60,3.50,3.17,0.33,1.98,3.17,1310.10,1.25,2.13,230.60',\n",
       " '01.01.2009 11:30:00,1000.61,-7.21,265.90,-8.53,90.20,3.55,3.20,0.35,1.99,3.20,1309.08,1.41,2.25,208.20',\n",
       " '01.01.2009 11:40:00,1000.50,-7.16,265.96,-8.54,89.80,3.56,3.20,0.36,1.99,3.20,1308.69,1.28,1.88,198.90',\n",
       " '01.01.2009 11:50:00,1000.36,-7.03,266.10,-8.44,89.60,3.60,3.23,0.37,2.01,3.22,1307.86,1.75,3.00,207.50',\n",
       " '01.01.2009 12:00:00,1000.30,-6.87,266.27,-8.28,89.60,3.64,3.27,0.38,2.03,3.26,1306.98,1.84,2.63,184.40',\n",
       " '01.01.2009 12:10:00,1000.21,-6.77,266.38,-8.19,89.50,3.67,3.29,0.39,2.05,3.29,1306.33,2.15,2.88,202.80',\n",
       " '01.01.2009 12:20:00,1000.18,-6.70,266.44,-8.09,89.80,3.69,3.32,0.38,2.06,3.32,1305.97,1.89,3.13,222.70',\n",
       " '01.01.2009 12:30:00,1000.14,-6.61,266.54,-8.00,89.70,3.72,3.34,0.38,2.08,3.34,1305.44,2.25,3.38,237.60',\n",
       " '01.01.2009 12:40:00,1000.02,-6.51,266.65,-7.93,89.50,3.75,3.36,0.39,2.09,3.36,1304.79,1.28,2.63,197.90',\n",
       " '01.01.2009 12:50:00,1000.02,-6.21,266.95,-7.65,89.40,3.84,3.43,0.41,2.14,3.43,1303.28,0.33,1.00,202.60',\n",
       " '01.01.2009 13:00:00,1000.03,-5.89,267.27,-7.46,88.60,3.93,3.48,0.45,2.17,3.48,1301.73,0.57,1.25,187.30',\n",
       " '01.01.2009 13:10:00,999.97,-5.83,267.33,-7.52,87.80,3.95,3.47,0.48,2.16,3.47,1301.38,0.86,1.38,160.90',\n",
       " '01.01.2009 13:20:00,999.97,-5.76,267.41,-7.46,87.70,3.97,3.48,0.49,2.17,3.48,1301.00,0.91,1.63,180.10',\n",
       " '01.01.2009 13:30:00,1000.02,-5.90,267.26,-7.62,87.50,3.93,3.44,0.49,2.14,3.44,1301.76,1.45,2.00,138.10',\n",
       " '01.01.2009 13:40:00,999.89,-5.97,267.20,-7.55,88.50,3.91,3.46,0.45,2.15,3.46,1301.94,1.22,1.88,161.40',\n",
       " '01.01.2009 13:50:00,999.81,-5.88,267.30,-7.44,88.60,3.94,3.49,0.45,2.17,3.49,1301.37,1.43,2.00,146.00',\n",
       " '01.01.2009 14:00:00,999.81,-5.94,267.24,-7.43,89.10,3.92,3.49,0.43,2.17,3.49,1301.67,1.25,2.00,144.00',\n",
       " '01.01.2009 14:10:00,999.81,-5.84,267.34,-7.26,89.60,3.95,3.54,0.41,2.20,3.54,1301.17,1.01,2.13,151.00',\n",
       " '01.01.2009 14:20:00,999.80,-5.76,267.41,-7.16,89.80,3.97,3.56,0.40,2.22,3.56,1300.77,1.19,1.88,157.00',\n",
       " '01.01.2009 14:30:00,999.81,-5.75,267.43,-7.14,89.80,3.97,3.57,0.41,2.22,3.57,1300.69,1.25,2.00,145.10',\n",
       " '01.01.2009 14:40:00,999.82,-5.76,267.41,-7.10,90.20,3.97,3.58,0.39,2.23,3.58,1300.77,1.35,2.25,137.70',\n",
       " '01.01.2009 14:50:00,999.83,-5.73,267.45,-7.05,90.30,3.98,3.59,0.39,2.24,3.59,1300.61,1.74,2.38,142.40',\n",
       " '01.01.2009 15:00:00,999.88,-5.69,267.48,-7.00,90.40,3.99,3.61,0.38,2.25,3.61,1300.51,1.17,1.88,134.90',\n",
       " '01.01.2009 15:10:00,999.98,-5.53,267.63,-6.87,90.20,4.04,3.64,0.40,2.27,3.64,1299.83,1.33,1.75,148.60',\n",
       " '01.01.2009 15:20:00,1000.06,-5.57,267.59,-6.97,89.80,4.03,3.62,0.41,2.25,3.62,1300.14,1.24,2.00,146.10',\n",
       " '01.01.2009 15:30:00,1000.04,-5.43,267.73,-6.80,90.00,4.07,3.66,0.41,2.28,3.66,1299.40,0.90,1.25,155.10',\n",
       " '01.01.2009 15:40:00,1000.00,-5.32,267.84,-6.76,89.50,4.11,3.68,0.43,2.29,3.68,1298.82,0.98,1.50,137.50',\n",
       " '01.01.2009 15:50:00,999.95,-5.36,267.81,-6.84,89.20,4.09,3.65,0.44,2.28,3.65,1298.95,1.48,2.25,151.90',\n",
       " '01.01.2009 16:00:00,999.94,-5.40,267.76,-6.86,89.40,4.08,3.65,0.43,2.27,3.65,1299.17,1.40,2.13,145.50',\n",
       " '01.01.2009 16:10:00,1000.05,-5.31,267.85,-6.69,89.90,4.11,3.69,0.42,2.30,3.69,1298.81,1.03,1.50,127.00',\n",
       " '01.01.2009 16:20:00,1000.05,-5.28,267.88,-6.68,89.80,4.12,3.70,0.42,2.30,3.70,1298.69,1.11,1.88,134.70',\n",
       " '01.01.2009 16:30:00,1000.10,-5.32,267.83,-6.77,89.50,4.11,3.67,0.43,2.29,3.67,1298.96,1.35,2.13,132.30',\n",
       " '01.01.2009 16:40:00,1000.17,-5.29,267.86,-6.70,89.70,4.12,3.69,0.42,2.30,3.69,1298.87,1.03,1.75,140.80',\n",
       " '01.01.2009 16:50:00,1000.13,-5.33,267.82,-6.81,89.20,4.10,3.66,0.44,2.28,3.66,1299.04,1.08,1.50,160.00',\n",
       " '01.01.2009 17:00:00,1000.17,-5.37,267.78,-6.82,89.40,4.09,3.66,0.43,2.28,3.66,1299.27,0.85,1.63,142.20',\n",
       " '01.01.2009 17:10:00,1000.17,-5.43,267.72,-6.90,89.30,4.07,3.64,0.44,2.26,3.64,1299.60,1.27,1.88,144.00',\n",
       " '01.01.2009 17:20:00,1000.18,-5.28,267.86,-6.68,89.80,4.12,3.70,0.42,2.30,3.70,1298.87,0.55,1.13,162.60',\n",
       " '01.01.2009 17:30:00,1000.18,-5.21,267.94,-6.70,89.20,4.14,3.69,0.45,2.30,3.69,1298.51,0.53,1.38,147.20',\n",
       " '01.01.2009 17:40:00,1000.17,-5.21,267.93,-6.75,88.90,4.14,3.68,0.46,2.29,3.68,1298.52,0.75,1.25,182.20',\n",
       " '01.01.2009 17:50:00,1000.16,-5.24,267.91,-6.77,88.90,4.13,3.67,0.46,2.29,3.67,1298.65,0.72,1.25,150.40',\n",
       " '01.01.2009 18:00:00,1000.16,-5.25,267.90,-6.75,89.10,4.13,3.68,0.45,2.29,3.68,1298.68,0.55,1.00,183.70',\n",
       " '01.01.2009 18:10:00,1000.13,-5.16,267.99,-6.67,89.10,4.16,3.70,0.45,2.31,3.70,1298.21,0.60,1.00,186.90',\n",
       " '01.01.2009 18:20:00,1000.07,-5.12,268.04,-6.62,89.10,4.17,3.72,0.45,2.31,3.72,1297.91,0.50,0.88,179.30',\n",
       " '01.01.2009 18:30:00,1000.11,-5.04,268.12,-6.57,88.90,4.20,3.73,0.47,2.32,3.73,1297.56,0.41,0.88,152.30',\n",
       " '01.01.2009 18:40:00,1000.18,-5.01,268.13,-6.58,88.70,4.20,3.73,0.47,2.32,3.73,1297.55,0.47,1.00,153.20',\n",
       " '01.01.2009 18:50:00,1000.23,-5.12,268.02,-6.68,88.70,4.17,3.70,0.47,2.30,3.70,1298.15,0.93,1.38,148.80',\n",
       " '01.01.2009 19:00:00,1000.22,-5.11,268.03,-6.57,89.40,4.17,3.73,0.44,2.32,3.73,1298.07,0.41,0.75,176.70',\n",
       " '01.01.2009 19:10:00,1000.30,-4.90,268.24,-6.36,89.40,4.24,3.79,0.45,2.36,3.79,1297.10,0.35,0.88,158.10',\n",
       " '01.01.2009 19:20:00,1000.19,-4.86,268.28,-6.40,88.90,4.25,3.78,0.47,2.35,3.78,1296.82,0.27,0.63,131.40',\n",
       " '01.01.2009 19:30:00,1000.18,-4.90,268.25,-6.46,88.70,4.24,3.76,0.48,2.34,3.76,1296.97,0.50,0.88,149.70',\n",
       " '01.01.2009 19:40:00,1000.14,-4.97,268.18,-6.53,88.70,4.22,3.74,0.48,2.33,3.74,1297.26,0.60,1.00,158.70',\n",
       " '01.01.2009 19:50:00,1000.18,-4.99,268.16,-6.51,89.00,4.21,3.75,0.46,2.33,3.75,1297.41,0.55,0.88,158.40',\n",
       " '01.01.2009 20:00:00,1000.22,-4.90,268.24,-6.38,89.30,4.24,3.79,0.45,2.36,3.78,1297.05,0.68,1.13,195.20',\n",
       " '01.01.2009 20:10:00,1000.27,-4.93,268.21,-6.38,89.50,4.23,3.78,0.44,2.36,3.78,1297.25,0.63,1.13,206.50',\n",
       " '01.01.2009 20:20:00,1000.21,-4.89,268.26,-6.29,89.80,4.24,3.81,0.43,2.37,3.81,1296.93,0.58,1.00,169.90',\n",
       " '01.01.2009 20:30:00,1000.19,-4.93,268.21,-6.32,89.90,4.23,3.80,0.43,2.37,3.80,1297.13,1.04,1.50,156.20',\n",
       " '01.01.2009 20:40:00,1000.24,-5.00,268.14,-6.34,90.20,4.21,3.80,0.41,2.36,3.80,1297.52,0.85,1.63,144.80',\n",
       " '01.01.2009 20:50:00,1000.30,-4.90,268.24,-6.19,90.60,4.24,3.84,0.40,2.39,3.84,1297.08,0.28,0.75,146.60',\n",
       " '01.01.2009 21:00:00,1000.19,-4.80,268.35,-6.14,90.20,4.27,3.85,0.42,2.40,3.85,1296.45,0.44,0.75,206.30',\n",
       " '01.01.2009 21:10:00,1000.19,-4.84,268.31,-6.21,90.00,4.26,3.83,0.43,2.39,3.83,1296.64,0.83,1.13,168.50',\n",
       " '01.01.2009 21:20:00,1000.15,-4.90,268.25,-6.25,90.20,4.24,3.82,0.42,2.38,3.82,1296.91,0.58,1.00,195.80',\n",
       " '01.01.2009 21:30:00,1000.09,-4.86,268.29,-6.18,90.40,4.25,3.84,0.41,2.39,3.84,1296.63,0.34,0.75,212.10',\n",
       " '01.01.2009 21:40:00,1000.06,-4.76,268.40,-6.06,90.50,4.29,3.88,0.41,2.42,3.88,1296.07,0.35,0.75,269.10',\n",
       " '01.01.2009 21:50:00,1000.03,-4.61,268.55,-5.90,90.60,4.34,3.93,0.41,2.45,3.93,1295.29,0.33,0.75,219.70',\n",
       " '01.01.2009 22:00:00,1000.00,-4.50,268.66,-5.82,90.40,4.37,3.95,0.42,2.46,3.95,1294.72,0.33,0.63,175.70',\n",
       " '01.01.2009 22:10:00,999.92,-4.41,268.76,-5.73,90.40,4.40,3.98,0.42,2.48,3.98,1294.16,0.18,0.50,170.10',\n",
       " '01.01.2009 22:20:00,999.93,-4.40,268.77,-5.77,90.10,4.40,3.97,0.44,2.47,3.97,1294.14,0.39,0.75,146.90',\n",
       " '01.01.2009 22:30:00,999.87,-4.32,268.85,-5.70,90.00,4.43,3.99,0.44,2.48,3.99,1293.65,0.20,0.50,172.80',\n",
       " '01.01.2009 22:40:00,999.80,-4.17,269.00,-5.60,89.70,4.48,4.02,0.46,2.50,4.02,1292.87,0.34,0.63,198.40',\n",
       " '01.01.2009 22:50:00,999.77,-4.30,268.88,-5.79,89.30,4.44,3.96,0.47,2.47,3.96,1293.46,0.47,0.75,210.80',\n",
       " '01.01.2009 23:00:00,999.77,-4.47,268.71,-5.84,90.10,4.38,3.95,0.43,2.46,3.95,1294.29,0.65,1.00,219.30',\n",
       " '01.01.2009 23:10:00,999.80,-4.50,268.68,-5.71,91.20,4.37,3.99,0.38,2.48,3.99,1294.45,0.37,0.88,149.60',\n",
       " '01.01.2009 23:20:00,999.81,-4.51,268.66,-5.65,91.70,4.37,4.00,0.36,2.50,4.01,1294.50,0.44,0.88,198.60',\n",
       " '01.01.2009 23:30:00,999.80,-4.33,268.84,-5.36,92.50,4.43,4.09,0.33,2.55,4.09,1293.60,0.24,0.63,132.70',\n",
       " '01.01.2009 23:40:00,999.71,-4.33,268.85,-5.43,92.00,4.43,4.07,0.35,2.54,4.07,1293.47,0.77,1.13,206.40',\n",
       " '01.01.2009 23:50:00,999.67,-4.58,268.61,-5.62,92.40,4.34,4.01,0.33,2.50,4.02,1294.65,0.69,1.25,213.90',\n",
       " '02.01.2009 00:00:00,999.59,-4.54,268.65,-5.46,93.20,4.36,4.06,0.30,2.53,4.06,1294.33,0.41,0.88,155.00',\n",
       " '02.01.2009 00:10:00,999.55,-4.60,268.60,-5.52,93.20,4.34,4.04,0.29,2.52,4.04,1294.58,0.85,1.38,151.20',\n",
       " '02.01.2009 00:20:00,999.56,-4.67,268.53,-5.52,93.70,4.32,4.04,0.27,2.52,4.05,1294.91,0.83,1.50,148.80',\n",
       " '02.01.2009 00:30:00,999.56,-4.66,268.53,-5.46,94.10,4.32,4.06,0.25,2.53,4.06,1294.89,0.53,1.38,154.30',\n",
       " '02.01.2009 00:40:00,999.48,-4.55,268.65,-5.31,94.40,4.35,4.11,0.24,2.56,4.11,1294.22,0.41,1.25,159.30',\n",
       " '02.01.2009 00:50:00,999.44,-4.43,268.77,-5.19,94.40,4.39,4.15,0.25,2.59,4.15,1293.56,0.63,1.13,192.10',\n",
       " '02.01.2009 01:00:00,999.34,-4.44,268.77,-5.19,94.40,4.39,4.15,0.25,2.58,4.15,1293.47,0.28,1.00,148.10',\n",
       " '02.01.2009 01:10:00,999.31,-4.39,268.82,-5.14,94.50,4.41,4.16,0.24,2.60,4.17,1293.22,0.42,1.25,153.50',\n",
       " '02.01.2009 01:20:00,999.21,-4.40,268.82,-5.16,94.40,4.40,4.16,0.25,2.59,4.16,1293.11,0.51,0.88,146.20',\n",
       " '02.01.2009 01:30:00,999.18,-4.40,268.82,-5.13,94.60,4.40,4.17,0.24,2.60,4.17,1293.08,0.27,0.63,165.10',\n",
       " '02.01.2009 01:40:00,999.08,-4.31,268.92,-5.03,94.70,4.43,4.20,0.24,2.62,4.20,1292.49,0.22,0.75,185.00',\n",
       " '02.01.2009 01:50:00,999.00,-4.26,268.98,-4.99,94.60,4.45,4.21,0.24,2.63,4.21,1292.14,0.37,0.75,223.10',\n",
       " '02.01.2009 02:00:00,998.93,-4.29,268.95,-5.04,94.50,4.44,4.20,0.24,2.62,4.20,1292.22,0.49,0.88,209.80',\n",
       " '02.01.2009 02:10:00,998.93,-4.26,268.99,-4.99,94.60,4.45,4.21,0.24,2.63,4.22,1292.03,0.29,0.75,182.30',\n",
       " '02.01.2009 02:20:00,998.82,-4.27,268.99,-5.02,94.40,4.45,4.20,0.25,2.62,4.20,1291.94,0.30,0.75,148.50',\n",
       " '02.01.2009 02:30:00,998.82,-4.30,268.95,-5.04,94.50,4.44,4.19,0.24,2.62,4.20,1292.11,0.39,0.75,223.60',\n",
       " '02.01.2009 02:40:00,998.78,-4.32,268.94,-5.05,94.60,4.43,4.19,0.24,2.61,4.20,1292.15,0.61,1.13,176.30',\n",
       " '02.01.2009 02:50:00,998.73,-4.39,268.87,-5.12,94.60,4.41,4.17,0.24,2.60,4.18,1292.43,0.45,0.75,194.00',\n",
       " '02.01.2009 03:00:00,998.69,-4.45,268.81,-5.15,94.80,4.39,4.16,0.23,2.59,4.16,1292.69,0.65,1.00,203.30',\n",
       " '02.01.2009 03:10:00,998.67,-4.50,268.76,-5.17,95.00,4.37,4.15,0.22,2.59,4.16,1292.90,0.72,1.00,177.50',\n",
       " '02.01.2009 03:20:00,998.58,-4.48,268.79,-5.12,95.20,4.38,4.17,0.21,2.60,4.17,1292.66,0.56,0.88,179.30',\n",
       " '02.01.2009 03:30:00,998.49,-4.43,268.84,-5.07,95.30,4.39,4.19,0.21,2.61,4.19,1292.34,0.57,1.00,212.20',\n",
       " '02.01.2009 03:40:00,998.44,-4.33,268.95,-4.94,95.50,4.43,4.23,0.20,2.64,4.23,1291.75,0.42,0.88,160.70',\n",
       " '02.01.2009 03:50:00,998.39,-4.41,268.87,-5.07,95.10,4.40,4.18,0.22,2.61,4.19,1292.10,0.92,1.50,167.60',\n",
       " '02.01.2009 04:00:00,998.32,-4.58,268.71,-5.22,95.20,4.34,4.14,0.21,2.58,4.14,1292.83,0.77,1.38,190.20',\n",
       " '02.01.2009 04:10:00,998.29,-4.63,268.66,-5.25,95.40,4.33,4.13,0.20,2.58,4.14,1293.05,0.83,1.25,190.80',\n",
       " '02.01.2009 04:20:00,998.12,-4.63,268.68,-5.23,95.50,4.33,4.13,0.19,2.58,4.14,1292.80,0.80,1.25,222.00',\n",
       " '02.01.2009 04:30:00,997.95,-4.74,268.58,-5.33,95.60,4.29,4.10,0.19,2.56,4.11,1293.15,1.05,1.50,223.20',\n",
       " '02.01.2009 04:40:00,998.03,-4.86,268.45,-5.44,95.70,4.25,4.07,0.18,2.54,4.08,1293.84,0.50,1.00,113.50',\n",
       " '02.01.2009 04:50:00,998.08,-4.97,268.34,-5.51,95.90,4.22,4.05,0.17,2.53,4.05,1294.44,0.49,1.13,107.10',\n",
       " '02.01.2009 05:00:00,998.10,-4.96,268.34,-5.50,96.00,4.22,4.05,0.17,2.53,4.06,1294.44,0.79,1.25,22.22',\n",
       " '02.01.2009 05:10:00,998.21,-4.94,268.36,-5.48,96.00,4.23,4.06,0.17,2.53,4.06,1294.48,1.03,1.63,344.20',\n",
       " '02.01.2009 05:20:00,998.21,-4.78,268.52,-5.27,96.30,4.28,4.12,0.16,2.57,4.13,1293.66,0.90,1.50,358.80',\n",
       " '02.01.2009 05:30:00,998.13,-4.63,268.68,-5.14,96.20,4.33,4.16,0.16,2.60,4.17,1292.80,0.44,1.00,30.37',\n",
       " '02.01.2009 05:40:00,998.02,-4.64,268.67,-5.19,95.90,4.32,4.15,0.18,2.59,4.16,1292.73,0.49,0.88,166.50',\n",
       " '02.01.2009 05:50:00,997.93,-4.67,268.65,-5.21,96.00,4.31,4.14,0.17,2.59,4.15,1292.78,0.44,1.13,312.70',\n",
       " '02.01.2009 06:00:00,998.00,-4.43,268.88,-4.94,96.20,4.39,4.23,0.17,2.64,4.23,1291.66,0.81,2.13,40.88',\n",
       " '02.01.2009 06:10:00,998.11,-4.38,268.93,-4.93,95.90,4.41,4.23,0.18,2.64,4.24,1291.55,0.75,1.63,52.73',\n",
       " '02.01.2009 06:20:00,998.15,-4.32,268.98,-4.87,95.90,4.43,4.25,0.18,2.65,4.26,1291.31,0.68,1.38,38.00',\n",
       " '02.01.2009 06:30:00,998.21,-4.25,269.05,-4.81,95.80,4.46,4.27,0.19,2.66,4.28,1291.03,0.25,0.75,59.41',\n",
       " '02.01.2009 06:40:00,998.22,-4.16,269.14,-4.75,95.60,4.48,4.29,0.20,2.68,4.30,1290.61,0.93,2.00,52.70',\n",
       " '02.01.2009 06:50:00,998.19,-4.26,269.04,-4.88,95.40,4.45,4.25,0.20,2.65,4.25,1291.06,0.97,1.75,26.64',\n",
       " '02.01.2009 07:00:00,998.17,-4.28,269.02,-4.89,95.50,4.44,4.24,0.20,2.65,4.25,1291.15,0.53,1.13,156.40',\n",
       " '02.01.2009 07:10:00,998.19,-4.34,268.96,-4.98,95.30,4.42,4.21,0.21,2.63,4.22,1291.50,0.33,0.63,202.60',\n",
       " '02.01.2009 07:20:00,998.20,-4.37,268.93,-5.01,95.20,4.42,4.20,0.21,2.62,4.21,1291.62,0.38,0.63,216.90',\n",
       " '02.01.2009 07:30:00,998.22,-4.33,268.97,-4.97,95.20,4.43,4.22,0.21,2.63,4.22,1291.44,1.04,1.75,197.70',\n",
       " '02.01.2009 07:40:00,998.17,-4.43,268.87,-5.09,95.10,4.39,4.18,0.22,2.61,4.19,1291.91,1.11,2.38,190.70',\n",
       " '02.01.2009 07:50:00,998.18,-4.39,268.91,-5.05,95.10,4.41,4.19,0.22,2.62,4.20,1291.73,1.14,2.38,168.00',\n",
       " '02.01.2009 08:00:00,998.18,-4.33,268.97,-5.04,94.80,4.43,4.20,0.23,2.62,4.20,1291.44,1.64,2.38,178.70',\n",
       " '02.01.2009 08:10:00,998.11,-4.37,268.94,-5.08,94.70,4.41,4.18,0.23,2.61,4.19,1291.52,1.62,2.38,176.80',\n",
       " '02.01.2009 08:20:00,998.10,-4.31,268.99,-5.03,94.70,4.43,4.20,0.23,2.62,4.21,1291.24,1.42,2.25,174.70',\n",
       " '02.01.2009 08:30:00,998.15,-4.30,269.00,-5.06,94.40,4.44,4.19,0.25,2.61,4.20,1291.24,1.97,2.63,174.80',\n",
       " '02.01.2009 08:40:00,998.29,-4.24,269.05,-5.00,94.40,4.46,4.21,0.25,2.63,4.22,1291.14,1.39,2.00,173.90',\n",
       " '02.01.2009 08:50:00,998.34,-4.17,269.12,-4.97,94.10,4.48,4.22,0.26,2.63,4.23,1290.83,1.68,2.50,176.20',\n",
       " '02.01.2009 09:00:00,998.35,-4.13,269.16,-4.97,93.80,4.49,4.22,0.28,2.63,4.22,1290.68,1.32,1.88,184.50',\n",
       " '02.01.2009 09:10:00,998.39,-4.07,269.22,-4.93,93.70,4.52,4.23,0.28,2.64,4.24,1290.42,1.15,1.63,176.00',\n",
       " '02.01.2009 09:20:00,998.46,-3.95,269.33,-4.87,93.30,4.56,4.25,0.31,2.65,4.26,1289.96,1.31,2.00,169.90',\n",
       " '02.01.2009 09:30:00,998.53,-3.90,269.38,-4.84,93.10,4.57,4.26,0.32,2.66,4.27,1289.77,1.42,1.88,180.80',\n",
       " '02.01.2009 09:40:00,998.59,-3.90,269.37,-4.90,92.70,4.57,4.24,0.33,2.65,4.25,1289.86,1.44,1.88,188.80',\n",
       " '02.01.2009 09:50:00,998.58,-3.92,269.35,-4.94,92.60,4.57,4.23,0.34,2.64,4.23,1289.99,1.24,1.75,167.90',\n",
       " '02.01.2009 10:00:00,998.65,-3.93,269.34,-4.99,92.30,4.56,4.21,0.35,2.63,4.22,1290.10,1.83,2.25,141.70',\n",
       " '02.01.2009 10:10:00,998.78,-3.97,269.29,-5.00,92.50,4.55,4.21,0.34,2.63,4.21,1290.45,1.78,2.25,140.50',\n",
       " '02.01.2009 10:20:00,998.83,-3.93,269.32,-4.93,92.70,4.56,4.23,0.33,2.64,4.23,1290.35,1.46,2.00,143.10',\n",
       " '02.01.2009 10:30:00,998.84,-3.90,269.35,-4.91,92.60,4.57,4.24,0.34,2.64,4.24,1290.20,1.88,2.38,139.70',\n",
       " '02.01.2009 10:40:00,998.87,-3.87,269.38,-4.88,92.60,4.59,4.25,0.34,2.65,4.25,1290.06,1.69,2.25,140.60',\n",
       " '02.01.2009 10:50:00,998.92,-3.74,269.51,-4.77,92.50,4.63,4.28,0.35,2.67,4.29,1289.50,1.42,1.88,137.70',\n",
       " '02.01.2009 11:00:00,998.98,-3.62,269.62,-4.68,92.30,4.67,4.31,0.36,2.69,4.32,1288.99,1.21,1.63,141.50',\n",
       " '02.01.2009 11:10:00,998.88,-3.49,269.76,-4.61,91.90,4.72,4.33,0.38,2.70,4.34,1288.25,1.95,2.50,152.90',\n",
       " '02.01.2009 11:20:00,998.79,-3.36,269.89,-4.51,91.70,4.76,4.37,0.40,2.72,4.37,1287.49,2.00,2.75,182.50',\n",
       " '02.01.2009 11:30:00,998.88,-3.21,270.04,-4.43,91.20,4.82,4.39,0.42,2.74,4.40,1286.88,1.44,1.88,170.60',\n",
       " '02.01.2009 11:40:00,998.88,-3.17,270.08,-4.45,90.80,4.83,4.39,0.44,2.74,4.39,1286.66,1.52,2.38,151.30',\n",
       " '02.01.2009 11:50:00,998.94,-3.02,270.22,-4.33,90.60,4.89,4.43,0.46,2.76,4.43,1286.02,1.33,2.00,142.40',\n",
       " '02.01.2009 12:00:00,998.91,-3.12,270.13,-4.50,90.10,4.85,4.37,0.48,2.73,4.38,1286.47,1.54,2.00,127.00',\n",
       " '02.01.2009 12:10:00,998.85,-3.07,270.18,-4.40,90.50,4.87,4.41,0.46,2.75,4.41,1286.16,1.20,2.13,126.40',\n",
       " '02.01.2009 12:20:00,998.82,-2.90,270.35,-4.33,89.80,4.93,4.43,0.50,2.76,4.43,1285.32,1.00,1.50,160.70',\n",
       " '02.01.2009 12:30:00,998.78,-2.81,270.45,-4.36,89.00,4.96,4.42,0.55,2.76,4.42,1284.82,1.02,1.50,144.30',\n",
       " '02.01.2009 12:40:00,998.64,-2.80,270.47,-4.39,88.70,4.97,4.41,0.56,2.75,4.41,1284.58,1.06,1.63,181.40',\n",
       " '02.01.2009 12:50:00,998.61,-2.64,270.63,-4.27,88.50,5.03,4.45,0.58,2.78,4.46,1283.79,0.74,1.38,141.60',\n",
       " '02.01.2009 13:00:00,998.59,-2.53,270.74,-4.23,88.00,5.07,4.46,0.61,2.78,4.47,1283.22,0.92,1.50,129.40',\n",
       " '02.01.2009 13:10:00,998.58,-2.54,270.73,-4.30,87.60,5.07,4.44,0.63,2.77,4.44,1283.27,1.13,1.75,141.10',\n",
       " '02.01.2009 13:20:00,998.62,-2.49,270.77,-4.23,87.80,5.08,4.46,0.62,2.78,4.47,1283.09,0.88,1.38,138.70',\n",
       " '02.01.2009 13:30:00,998.60,-2.26,271.01,-4.08,87.20,5.17,4.51,0.66,2.81,4.52,1281.93,0.66,1.50,158.10',\n",
       " '02.01.2009 13:40:00,998.67,-2.21,271.05,-4.13,86.60,5.19,4.49,0.70,2.80,4.50,1281.82,0.59,1.00,138.80',\n",
       " '02.01.2009 13:50:00,998.70,-2.41,270.85,-4.33,86.60,5.11,4.43,0.69,2.76,4.43,1282.82,0.65,1.25,76.20',\n",
       " '02.01.2009 14:00:00,998.73,-2.56,270.70,-4.29,87.80,5.06,4.44,0.62,2.77,4.45,1283.56,0.73,1.25,42.40',\n",
       " '02.01.2009 14:10:00,998.73,-2.62,270.63,-4.24,88.60,5.03,4.46,0.57,2.78,4.46,1283.86,0.92,1.50,32.27',\n",
       " '02.01.2009 14:20:00,998.75,-2.54,270.72,-4.09,89.00,5.07,4.51,0.56,2.81,4.51,1283.45,0.44,1.00,85.80',\n",
       " '02.01.2009 14:30:00,998.70,-2.14,271.12,-3.88,87.80,5.22,4.58,0.64,2.86,4.59,1281.45,0.84,1.38,145.30',\n",
       " '02.01.2009 14:40:00,998.75,-2.22,271.04,-4.08,87.00,5.19,4.51,0.67,2.82,4.52,1281.94,0.76,1.13,173.00',\n",
       " '02.01.2009 14:50:00,998.71,-2.18,271.08,-3.97,87.50,5.20,4.55,0.65,2.84,4.56,1281.71,0.28,0.63,217.40',\n",
       " '02.01.2009 15:00:00,998.87,-2.12,271.12,-4.00,86.90,5.22,4.54,0.68,2.83,4.54,1281.64,0.47,1.00,336.30',\n",
       " '02.01.2009 15:10:00,999.05,-2.31,270.92,-4.12,87.30,5.15,4.50,0.65,2.81,4.50,1282.76,0.94,1.25,21.58',\n",
       " '02.01.2009 15:20:00,999.14,-2.42,270.80,-4.05,88.50,5.11,4.52,0.59,2.82,4.53,1283.40,1.35,2.00,15.64',\n",
       " '02.01.2009 15:30:00,999.25,-2.47,270.75,-4.01,89.10,5.09,4.54,0.55,2.83,4.54,1283.77,2.18,2.75,20.08',\n",
       " '02.01.2009 15:40:00,999.32,-2.55,270.67,-4.04,89.40,5.06,4.53,0.54,2.82,4.53,1284.21,2.39,3.63,26.15',\n",
       " '02.01.2009 15:50:00,999.46,-2.65,270.56,-4.12,89.50,5.03,4.50,0.53,2.80,4.50,1284.88,2.94,4.00,15.71',\n",
       " '02.01.2009 16:00:00,999.59,-2.76,270.43,-4.22,89.60,4.98,4.46,0.52,2.78,4.46,1285.63,2.91,4.13,11.99',\n",
       " '02.01.2009 16:10:00,999.62,-2.81,270.38,-4.26,89.70,4.96,4.45,0.51,2.77,4.45,1285.90,2.16,3.13,8.34',\n",
       " '02.01.2009 16:20:00,999.80,-2.83,270.34,-4.32,89.40,4.95,4.43,0.53,2.76,4.43,1286.25,2.03,3.25,7.34',\n",
       " '02.01.2009 16:30:00,999.92,-2.84,270.33,-4.47,88.40,4.95,4.38,0.57,2.73,4.38,1286.45,1.30,2.50,9.22',\n",
       " '02.01.2009 16:40:00,1000.13,-2.83,270.32,-4.49,88.30,4.95,4.37,0.58,2.73,4.37,1286.71,1.70,2.75,8.57',\n",
       " '02.01.2009 16:50:00,1000.16,-2.84,270.31,-4.45,88.60,4.95,4.39,0.56,2.73,4.39,1286.77,1.11,2.38,347.00',\n",
       " '02.01.2009 17:00:00,1000.21,-2.88,270.26,-4.43,89.00,4.94,4.39,0.54,2.74,4.39,1287.04,1.52,2.88,7.05',\n",
       " '02.01.2009 17:10:00,1000.23,-2.96,270.18,-4.44,89.50,4.91,4.39,0.52,2.74,4.39,1287.44,1.22,2.13,355.10',\n",
       " '02.01.2009 17:20:00,1000.35,-2.96,270.17,-4.36,90.00,4.91,4.42,0.49,2.75,4.41,1287.59,1.24,2.13,357.00',\n",
       " '02.01.2009 17:30:00,1000.43,-2.97,270.16,-4.32,90.30,4.91,4.43,0.48,2.76,4.43,1287.70,1.50,2.88,8.76',\n",
       " '02.01.2009 17:40:00,1000.54,-3.03,270.09,-4.28,91.00,4.88,4.44,0.44,2.77,4.44,1288.14,1.76,2.50,9.15',\n",
       " '02.01.2009 17:50:00,1000.66,-3.05,270.06,-4.18,91.80,4.88,4.48,0.40,2.79,4.47,1288.35,1.62,2.75,13.10',\n",
       " '02.01.2009 18:00:00,1000.74,-3.07,270.03,-4.14,92.30,4.87,4.49,0.37,2.80,4.49,1288.58,2.97,4.63,12.56',\n",
       " '02.01.2009 18:10:00,1000.78,-3.16,269.94,-4.23,92.30,4.83,4.46,0.37,2.78,4.46,1289.07,3.44,6.13,24.44',\n",
       " '02.01.2009 18:20:00,1000.92,-3.20,269.89,-4.32,91.90,4.82,4.43,0.39,2.76,4.43,1289.44,3.26,4.75,15.55',\n",
       " '02.01.2009 18:30:00,1000.98,-3.23,269.85,-4.41,91.50,4.81,4.40,0.41,2.74,4.40,1289.69,3.48,4.63,13.29',\n",
       " '02.01.2009 18:40:00,1001.13,-3.29,269.78,-4.56,90.90,4.79,4.35,0.44,2.71,4.35,1290.18,3.51,5.38,15.81',\n",
       " '02.01.2009 18:50:00,1001.19,-3.30,269.77,-4.62,90.50,4.79,4.33,0.45,2.69,4.33,1290.30,3.30,4.63,15.74',\n",
       " '02.01.2009 19:00:00,1001.27,-3.34,269.72,-4.64,90.70,4.77,4.33,0.44,2.69,4.32,1290.63,3.07,4.50,14.34',\n",
       " '02.01.2009 19:10:00,1001.34,-3.36,269.69,-4.56,91.40,4.76,4.35,0.41,2.71,4.35,1290.81,2.81,4.13,10.57',\n",
       " '02.01.2009 19:20:00,1001.40,-3.36,269.69,-4.47,92.00,4.76,4.38,0.38,2.73,4.37,1290.86,1.99,3.50,17.52',\n",
       " '02.01.2009 19:30:00,1001.46,-3.36,269.68,-4.44,92.20,4.76,4.39,0.37,2.73,4.38,1290.94,2.19,3.88,13.96',\n",
       " '02.01.2009 19:40:00,1001.57,-3.36,269.68,-4.40,92.50,4.76,4.40,0.36,2.74,4.40,1291.08,2.55,3.88,18.38',\n",
       " '02.01.2009 19:50:00,1001.71,-3.31,269.72,-4.30,92.80,4.78,4.44,0.34,2.76,4.43,1290.98,2.11,3.13,23.41',\n",
       " '02.01.2009 20:00:00,1001.83,-3.30,269.72,-4.29,92.80,4.79,4.44,0.34,2.76,4.43,1291.08,2.89,4.00,24.28',\n",
       " '02.01.2009 20:10:00,1001.90,-3.28,269.74,-4.30,92.60,4.79,4.44,0.35,2.76,4.43,1291.08,2.09,3.13,21.20',\n",
       " '02.01.2009 20:20:00,1002.01,-3.36,269.65,-4.41,92.40,4.76,4.40,0.36,2.74,4.39,1291.62,3.01,4.88,13.15',\n",
       " '02.01.2009 20:30:00,1002.03,-3.36,269.64,-4.41,92.40,4.76,4.40,0.36,2.74,4.39,1291.67,2.89,4.13,21.52',\n",
       " '02.01.2009 20:40:00,1002.13,-3.36,269.64,-4.44,92.20,4.76,4.39,0.37,2.73,4.38,1291.79,2.99,4.25,20.42',\n",
       " '02.01.2009 20:50:00,1002.15,-3.43,269.56,-4.51,92.20,4.74,4.37,0.37,2.72,4.36,1292.16,2.98,4.75,20.56',\n",
       " '02.01.2009 21:00:00,1002.23,-3.49,269.50,-4.61,91.90,4.72,4.34,0.38,2.69,4.33,1292.57,2.48,3.88,14.31',\n",
       " '02.01.2009 21:10:00,1002.32,-3.50,269.48,-4.64,91.70,4.71,4.32,0.39,2.69,4.31,1292.73,2.69,3.75,22.07',\n",
       " '02.01.2009 21:20:00,1002.36,-3.52,269.46,-4.72,91.30,4.71,4.30,0.41,2.67,4.29,1292.90,3.26,5.13,23.45',\n",
       " '02.01.2009 21:30:00,1002.37,-3.66,269.32,-4.99,90.40,4.66,4.21,0.45,2.62,4.20,1293.61,3.87,5.00,18.31',\n",
       " '02.01.2009 21:40:00,1002.35,-3.80,269.18,-5.22,89.80,4.61,4.14,0.47,2.57,4.13,1294.31,3.42,4.75,20.89',\n",
       " '02.01.2009 21:50:00,1002.39,-3.94,269.04,-5.43,89.30,4.56,4.07,0.49,2.53,4.06,1295.07,3.01,4.13,29.22',\n",
       " '02.01.2009 22:00:00,1002.39,-4.02,268.96,-5.58,88.80,4.53,4.03,0.51,2.50,4.02,1295.46,3.06,4.13,23.48',\n",
       " '02.01.2009 22:10:00,1002.39,-4.06,268.91,-5.68,88.40,4.52,3.99,0.52,2.48,3.98,1295.70,3.06,4.00,15.50',\n",
       " '02.01.2009 22:20:00,1002.44,-4.20,268.77,-5.78,88.70,4.47,3.97,0.51,2.46,3.96,1296.43,2.39,3.63,24.21',\n",
       " '02.01.2009 22:30:00,1002.54,-4.34,268.63,-5.88,88.90,4.42,3.93,0.49,2.44,3.92,1297.24,1.74,2.88,12.55',\n",
       " '02.01.2009 22:40:00,1002.52,-4.30,268.66,-5.80,89.20,4.44,3.96,0.48,2.46,3.95,1297.05,1.81,3.00,13.50',\n",
       " '02.01.2009 22:50:00,1002.57,-4.35,268.61,-5.79,89.60,4.42,3.96,0.46,2.46,3.95,1297.33,1.42,2.88,354.10',\n",
       " '02.01.2009 23:00:00,1002.52,-4.38,268.59,-5.80,89.70,4.41,3.96,0.45,2.46,3.95,1297.39,0.74,1.38,354.00',\n",
       " '02.01.2009 23:10:00,1002.64,-4.45,268.51,-5.89,89.60,4.39,3.93,0.46,2.44,3.92,1297.89,0.82,1.63,313.20',\n",
       " '02.01.2009 23:20:00,1002.65,-4.53,268.43,-5.95,89.70,4.36,3.91,0.45,2.43,3.90,1298.32,1.26,2.13,323.80',\n",
       " '02.01.2009 23:30:00,1002.64,-4.57,268.39,-5.97,89.90,4.35,3.91,0.44,2.43,3.90,1298.51,1.38,2.75,349.20',\n",
       " '02.01.2009 23:40:00,1002.53,-4.56,268.41,-5.97,89.80,4.35,3.91,0.44,2.43,3.90,1298.31,2.13,2.75,17.64',\n",
       " '02.01.2009 23:50:00,1002.35,-4.59,268.39,-6.03,89.60,4.34,3.89,0.45,2.42,3.88,1298.26,1.98,2.38,17.74',\n",
       " '03.01.2009 00:00:00,1002.32,-4.71,268.27,-6.09,90.00,4.30,3.87,0.43,2.41,3.86,1298.79,1.42,1.88,16.85',\n",
       " '03.01.2009 00:10:00,1002.39,-4.89,268.09,-6.16,90.70,4.24,3.85,0.39,2.39,3.84,1299.75,0.80,1.38,13.01',\n",
       " '03.01.2009 00:20:00,1002.39,-5.20,267.78,-6.49,90.60,4.14,3.75,0.39,2.33,3.75,1301.31,0.47,0.88,189.40',\n",
       " '03.01.2009 00:30:00,1002.49,-5.48,267.49,-6.70,91.10,4.06,3.69,0.36,2.30,3.69,1302.85,0.38,0.88,240.20',\n",
       " '03.01.2009 00:40:00,1002.42,-5.50,267.48,-6.61,91.80,4.05,3.72,0.33,2.31,3.71,1302.82,0.12,0.38,200.20',\n",
       " '03.01.2009 00:50:00,1002.39,-5.43,267.55,-6.49,92.20,4.07,3.75,0.32,2.33,3.75,1302.43,0.96,1.75,244.90',\n",
       " '03.01.2009 01:00:00,1002.39,-5.28,267.70,-6.35,92.10,4.12,3.79,0.33,2.36,3.78,1301.68,0.27,1.00,292.80',\n",
       " '03.01.2009 01:10:00,1002.39,-5.39,267.58,-6.62,91.00,4.08,3.72,0.37,2.31,3.71,1302.27,0.63,1.13,140.20',\n",
       " '03.01.2009 01:20:00,1002.31,-5.66,267.32,-6.93,90.70,4.00,3.63,0.37,2.25,3.62,1303.51,0.64,1.38,325.20',\n",
       " '03.01.2009 01:30:00,1002.34,-5.70,267.28,-6.90,91.20,3.99,3.64,0.35,2.26,3.63,1303.75,0.96,1.63,49.94',\n",
       " '03.01.2009 01:40:00,1002.36,-5.89,267.09,-7.04,91.50,3.93,3.60,0.33,2.23,3.59,1304.71,0.99,1.38,2.56',\n",
       " '03.01.2009 01:50:00,1002.27,-6.08,266.90,-7.18,91.90,3.87,3.56,0.31,2.21,3.55,1305.55,0.74,1.75,20.66',\n",
       " '03.01.2009 02:00:00,1002.27,-6.23,266.76,-7.26,92.30,3.83,3.54,0.29,2.20,3.53,1306.26,0.39,1.00,279.20',\n",
       " '03.01.2009 02:10:00,1002.28,-6.38,266.60,-7.42,92.30,3.78,3.49,0.29,2.17,3.49,1307.06,0.43,0.75,178.70',\n",
       " '03.01.2009 02:20:00,1002.29,-6.55,266.44,-7.52,92.70,3.74,3.46,0.27,2.15,3.46,1307.89,0.67,1.00,239.30',\n",
       " '03.01.2009 02:30:00,1002.37,-6.41,266.57,-7.31,93.30,3.78,3.52,0.25,2.19,3.51,1307.32,0.47,1.13,196.40',\n",
       " '03.01.2009 02:40:00,1002.27,-6.12,266.87,-7.00,93.40,3.86,3.61,0.25,2.24,3.60,1305.69,0.38,0.88,127.10',\n",
       " '03.01.2009 02:50:00,1002.20,-6.01,266.98,-6.99,92.70,3.89,3.61,0.28,2.24,3.60,1305.09,0.34,1.13,181.20',\n",
       " '03.01.2009 03:00:00,1002.15,-6.13,266.87,-7.22,91.90,3.86,3.55,0.31,2.20,3.54,1305.61,0.43,0.75,76.80',\n",
       " '03.01.2009 03:10:00,1002.14,-6.36,266.63,-7.44,92.00,3.79,3.49,0.30,2.17,3.48,1306.79,0.19,0.63,73.10',\n",
       " '03.01.2009 03:20:00,1002.14,-6.48,266.52,-7.51,92.30,3.76,3.47,0.29,2.16,3.46,1307.35,0.31,0.88,296.30',\n",
       " '03.01.2009 03:30:00,1002.06,-6.46,266.54,-7.44,92.70,3.76,3.49,0.27,2.17,3.48,1307.15,0.57,1.00,231.80',\n",
       " '03.01.2009 03:40:00,1002.03,-6.33,266.68,-7.32,92.60,3.80,3.52,0.28,2.19,3.51,1306.44,0.25,0.63,145.40',\n",
       " '03.01.2009 03:50:00,1002.01,-6.39,266.62,-7.42,92.30,3.78,3.49,0.29,2.17,3.48,1306.74,0.82,1.75,240.80',\n",
       " '03.01.2009 04:00:00,1002.02,-6.21,266.79,-7.32,91.80,3.83,3.52,0.31,2.19,3.51,1305.89,0.77,1.50,273.20',\n",
       " '03.01.2009 04:10:00,1001.99,-6.17,266.84,-7.40,90.90,3.85,3.50,0.35,2.17,3.49,1305.62,0.27,0.75,162.30',\n",
       " '03.01.2009 04:20:00,1001.97,-6.25,266.76,-7.47,91.00,3.82,3.48,0.34,2.16,3.47,1306.00,0.31,1.00,96.20',\n",
       " '03.01.2009 04:30:00,1001.93,-6.30,266.72,-7.47,91.30,3.81,3.48,0.33,2.16,3.47,1306.20,0.37,0.88,192.00',\n",
       " '03.01.2009 04:40:00,1001.90,-6.49,266.53,-7.65,91.40,3.75,3.43,0.32,2.13,3.42,1307.11,0.24,0.63,180.70',\n",
       " '03.01.2009 04:50:00,1001.79,-6.64,266.38,-7.79,91.50,3.71,3.39,0.32,2.11,3.39,1307.75,0.51,0.88,191.20',\n",
       " '03.01.2009 05:00:00,1001.65,-7.02,266.01,-8.10,91.90,3.60,3.31,0.29,2.06,3.31,1309.46,0.48,0.88,193.90',\n",
       " '03.01.2009 05:10:00,1001.52,-7.19,265.85,-8.20,92.40,3.56,3.29,0.27,2.04,3.28,1310.14,0.39,0.88,158.20',\n",
       " '03.01.2009 05:20:00,1001.40,-7.35,265.70,-8.36,92.40,3.51,3.24,0.27,2.02,3.24,1310.79,0.55,1.00,65.23',\n",
       " '03.01.2009 05:30:00,1001.29,-7.54,265.52,-8.50,92.80,3.46,3.21,0.25,2.00,3.21,1311.60,0.45,0.88,58.63',\n",
       " '03.01.2009 05:40:00,1001.29,-7.80,265.26,-8.71,93.10,3.39,3.16,0.23,1.96,3.15,1312.91,0.28,0.75,17.82',\n",
       " '03.01.2009 05:50:00,1001.31,-7.96,265.10,-8.80,93.60,3.35,3.13,0.21,1.95,3.13,1313.75,0.20,0.75,76.70',\n",
       " '03.01.2009 06:00:00,1001.29,-8.20,264.86,-9.05,93.50,3.29,3.07,0.21,1.91,3.07,1314.93,0.47,1.75,221.40',\n",
       " '03.01.2009 06:10:00,1001.20,-8.14,264.93,-8.94,93.90,3.30,3.10,0.20,1.93,3.10,1314.51,2.00,2.63,249.70',\n",
       " '03.01.2009 06:20:00,1001.15,-7.92,265.15,-8.91,92.50,3.36,3.11,0.25,1.93,3.10,1313.34,2.58,3.13,240.60',\n",
       " '03.01.2009 06:30:00,1001.13,-7.89,265.18,-9.13,90.70,3.37,3.05,0.31,1.90,3.05,1313.19,2.34,2.88,241.00',\n",
       " '03.01.2009 06:40:00,1001.12,-7.97,265.11,-9.32,89.90,3.35,3.01,0.34,1.87,3.00,1313.61,1.95,2.63,243.20',\n",
       " '03.01.2009 06:50:00,1001.07,-8.10,264.98,-9.48,89.70,3.31,2.97,0.34,1.85,2.97,1314.20,1.04,2.13,244.70',\n",
       " '03.01.2009 07:00:00,1001.17,-8.48,264.59,-9.91,89.30,3.21,2.87,0.34,1.79,2.87,1316.27,1.32,2.25,229.70',\n",
       " '03.01.2009 07:10:00,1001.22,-8.63,264.44,-9.88,90.60,3.18,2.88,0.30,1.79,2.88,1317.08,1.31,2.13,228.30',\n",
       " '03.01.2009 07:20:00,1001.16,-8.61,264.46,-9.73,91.50,3.18,2.91,0.27,1.81,2.91,1316.88,1.41,1.88,233.60',\n",
       " '03.01.2009 07:30:00,1001.16,-8.61,264.46,-9.73,91.50,3.18,2.91,0.27,1.81,2.91,1316.88,1.13,1.75,223.90',\n",
       " '03.01.2009 07:40:00,1001.11,-8.85,264.23,-10.04,91.00,3.12,2.84,0.28,1.77,2.84,1318.04,1.08,2.25,207.90',\n",
       " '03.01.2009 07:50:00,1001.03,-9.18,263.90,-10.41,90.70,3.04,2.76,0.28,1.72,2.76,1319.63,1.90,2.75,237.30',\n",
       " '03.01.2009 08:00:00,1001.04,-9.28,263.80,-10.44,91.20,3.02,2.75,0.27,1.71,2.75,1320.15,0.85,1.75,219.70',\n",
       " '03.01.2009 08:10:00,1000.99,-9.45,263.64,-10.62,91.10,2.98,2.71,0.27,1.69,2.71,1320.96,0.73,1.38,176.50',\n",
       " '03.01.2009 08:20:00,1000.95,-9.81,263.28,-10.99,91.00,2.89,2.63,0.26,1.64,2.63,1322.74,0.90,1.75,215.20',\n",
       " '03.01.2009 08:30:00,1000.91,-9.85,263.24,-10.95,91.60,2.89,2.64,0.24,1.64,2.64,1322.89,0.72,1.13,213.00',\n",
       " '03.01.2009 08:40:00,1000.92,-9.71,263.38,-10.77,91.90,2.92,2.68,0.24,1.67,2.68,1322.18,0.93,1.25,207.70',\n",
       " '03.01.2009 08:50:00,1000.92,-9.40,263.69,-10.45,92.00,2.99,2.75,0.24,1.71,2.75,1320.59,0.43,0.88,149.20',\n",
       " '03.01.2009 09:00:00,1000.92,-9.46,263.63,-10.69,90.70,2.98,2.70,0.28,1.68,2.70,1320.91,0.79,1.50,177.80',\n",
       " '03.01.2009 09:10:00,1000.92,-10.00,263.09,-11.36,89.70,2.85,2.56,0.29,1.59,2.55,1323.70,0.82,1.38,194.30',\n",
       " '03.01.2009 09:20:00,1000.93,-9.93,263.16,-11.18,90.50,2.87,2.59,0.27,1.61,2.59,1323.34,0.68,1.13,201.50',\n",
       " '03.01.2009 09:30:00,1000.85,-9.30,263.80,-10.55,90.50,3.01,2.73,0.29,1.70,2.73,1320.01,0.65,1.00,176.20',\n",
       " '03.01.2009 09:40:00,1000.80,-8.83,264.27,-10.47,87.80,3.13,2.75,0.38,1.71,2.74,1317.59,0.72,1.00,163.70',\n",
       " '03.01.2009 09:50:00,1000.71,-8.48,264.63,-10.56,84.80,3.21,2.73,0.49,1.70,2.72,1315.74,0.80,1.63,178.60',\n",
       " '03.01.2009 10:00:00,1000.67,-8.53,264.58,-10.83,83.30,3.20,2.67,0.53,1.66,2.67,1315.96,1.08,1.88,167.50',\n",
       " '03.01.2009 10:10:00,1000.64,-8.56,264.55,-10.68,84.50,3.19,2.70,0.50,1.68,2.70,1316.05,1.19,1.88,176.90',\n",
       " '03.01.2009 10:20:00,1000.51,-8.47,264.65,-10.48,85.30,3.22,2.74,0.47,1.71,2.74,1315.41,1.28,2.13,172.10',\n",
       " '03.01.2009 10:30:00,1000.37,-8.51,264.62,-10.53,85.20,3.21,2.73,0.47,1.70,2.73,1315.44,1.77,2.38,177.10',\n",
       " '03.01.2009 10:40:00,1000.28,-8.41,264.73,-10.26,86.40,3.23,2.79,0.44,1.74,2.79,1314.79,1.49,2.25,177.80',\n",
       " '03.01.2009 10:50:00,1000.15,-8.16,264.99,-10.01,86.40,3.30,2.85,0.45,1.77,2.85,1313.34,1.39,2.38,188.60',\n",
       " '03.01.2009 11:00:00,1000.04,-7.87,265.29,-9.84,85.60,3.37,2.89,0.49,1.80,2.89,1311.75,1.34,2.25,172.10',\n",
       " '03.01.2009 11:10:00,999.88,-7.52,265.65,-9.63,84.70,3.47,2.94,0.53,1.83,2.94,1309.78,1.09,1.63,191.40',\n",
       " '03.01.2009 11:20:00,999.70,-7.29,265.89,-9.56,83.70,3.53,2.95,0.58,1.84,2.95,1308.41,1.38,2.25,178.90',\n",
       " '03.01.2009 11:30:00,999.56,-6.89,266.31,-9.24,83.20,3.64,3.03,0.61,1.89,3.03,1306.20,1.28,1.88,182.50',\n",
       " '03.01.2009 11:40:00,999.41,-6.56,266.65,-9.10,82.00,3.73,3.06,0.67,1.91,3.06,1304.38,1.42,2.13,189.70',\n",
       " '03.01.2009 11:50:00,999.26,-6.32,266.90,-8.91,81.70,3.80,3.11,0.70,1.94,3.11,1302.99,1.27,1.88,204.80',\n",
       " '03.01.2009 12:00:00,999.02,-5.96,267.28,-8.72,80.70,3.91,3.16,0.75,1.97,3.16,1300.90,1.31,1.88,213.90',\n",
       " '03.01.2009 12:10:00,998.83,-5.78,267.47,-8.58,80.50,3.96,3.19,0.77,1.99,3.19,1299.79,1.30,2.00,198.50',\n",
       " '03.01.2009 12:20:00,998.78,-5.15,268.10,-8.14,79.40,4.16,3.30,0.86,2.06,3.31,1296.61,1.15,1.75,188.50',\n",
       " '03.01.2009 12:30:00,998.69,-4.34,268.92,-7.81,76.60,4.42,3.39,1.04,2.11,3.39,1292.53,1.54,2.13,182.10',\n",
       " '03.01.2009 12:40:00,998.60,-3.77,269.50,-7.63,74.40,4.62,3.44,1.18,2.14,3.44,1289.65,1.46,2.13,184.80',\n",
       " '03.01.2009 12:50:00,998.47,-3.27,270.01,-7.57,72.00,4.79,3.45,1.34,2.15,3.46,1287.11,1.62,2.38,163.70',\n",
       " '03.01.2009 13:00:00,998.30,-2.82,270.47,-7.40,70.50,4.96,3.50,1.46,2.18,3.50,1284.70,1.52,2.25,180.50',\n",
       " '03.01.2009 13:10:00,998.17,-2.23,271.07,-7.31,67.98,5.18,3.52,1.66,2.20,3.53,1281.75,1.53,2.25,166.30',\n",
       " '03.01.2009 13:20:00,998.00,-2.37,270.95,-7.58,67.26,5.13,3.45,1.68,2.15,3.46,1282.21,1.69,2.38,155.50',\n",
       " '03.01.2009 13:30:00,997.84,-2.11,271.22,-7.47,66.55,5.23,3.48,1.75,2.17,3.49,1280.76,1.62,2.25,157.60',\n",
       " '03.01.2009 13:40:00,997.74,-1.66,271.68,-7.44,64.48,5.41,3.49,1.92,2.18,3.49,1278.50,1.59,2.38,153.90',\n",
       " '03.01.2009 13:50:00,997.65,-1.43,271.91,-7.34,63.90,5.50,3.51,1.99,2.19,3.52,1277.29,0.97,2.13,357.20',\n",
       " '03.01.2009 14:00:00,997.53,-2.15,271.20,-7.68,65.61,5.21,3.42,1.79,2.14,3.43,1280.57,0.88,1.50,60.25',\n",
       " '03.01.2009 14:10:00,997.31,-2.74,270.63,-7.97,67.06,4.99,3.35,1.64,2.09,3.36,1283.13,1.63,2.63,117.20',\n",
       " '03.01.2009 14:20:00,997.23,-2.29,271.09,-7.88,65.31,5.16,3.37,1.79,2.11,3.38,1280.86,1.18,3.13,187.80',\n",
       " '03.01.2009 14:30:00,997.13,-1.66,271.72,-9.39,55.33,5.41,2.99,2.42,1.87,3.00,1277.96,2.82,4.25,229.10',\n",
       " '03.01.2009 14:40:00,997.09,-1.52,271.87,-9.30,55.18,5.46,3.01,2.45,1.88,3.02,1277.23,1.73,2.25,193.20',\n",
       " '03.01.2009 14:50:00,997.02,-0.90,272.49,-9.39,52.35,5.72,2.99,2.72,1.87,3.00,1274.27,1.52,2.25,194.90',\n",
       " '03.01.2009 15:00:00,996.91,-0.82,272.59,-9.82,50.25,5.75,2.89,2.86,1.81,2.90,1273.76,1.34,2.25,209.10',\n",
       " '03.01.2009 15:10:00,996.85,-1.37,272.04,-10.43,49.89,5.52,2.76,2.77,1.72,2.76,1276.35,1.42,2.13,219.50',\n",
       " '03.01.2009 15:20:00,996.73,-1.91,271.50,-10.73,50.70,5.31,2.69,2.62,1.68,2.70,1278.79,1.70,2.50,221.00',\n",
       " '03.01.2009 15:30:00,996.58,-2.28,271.14,-10.63,52.52,5.16,2.71,2.45,1.69,2.72,1280.32,1.67,2.63,224.20',\n",
       " '03.01.2009 15:40:00,996.39,-2.24,271.20,-10.85,51.42,5.18,2.66,2.52,1.66,2.67,1279.91,1.97,2.75,228.30',\n",
       " '03.01.2009 15:50:00,996.35,-2.32,271.12,-11.51,49.06,5.15,2.52,2.62,1.58,2.53,1280.33,1.60,2.38,236.20',\n",
       " '03.01.2009 16:00:00,996.25,-2.80,270.65,-12.13,48.39,4.97,2.40,2.56,1.50,2.41,1282.51,1.82,2.50,240.60',\n",
       " '03.01.2009 16:10:00,996.12,-3.64,269.82,-12.35,50.57,4.66,2.36,2.31,1.47,2.37,1286.34,2.08,3.00,244.90',\n",
       " '03.01.2009 16:20:00,996.04,-3.98,269.49,-12.62,50.80,4.55,2.31,2.24,1.44,2.32,1287.91,2.60,3.25,238.70',\n",
       " '03.01.2009 16:30:00,995.92,-4.18,269.30,-12.72,51.12,4.48,2.29,2.19,1.43,2.30,1288.71,2.45,3.13,242.60',\n",
       " '03.01.2009 16:40:00,995.87,-4.35,269.13,-12.77,51.59,4.42,2.28,2.14,1.43,2.29,1289.47,2.53,3.25,241.40',\n",
       " '03.01.2009 16:50:00,995.85,-4.52,268.96,-12.72,52.47,4.36,2.29,2.07,1.43,2.30,1290.25,2.37,3.25,242.30',\n",
       " '03.01.2009 17:00:00,995.76,-4.69,268.80,-12.56,53.86,4.31,2.32,1.99,1.45,2.33,1290.94,2.63,3.50,248.80',\n",
       " '03.01.2009 17:10:00,995.70,-4.63,268.86,-12.38,54.42,4.33,2.35,1.97,1.47,2.36,1290.57,3.01,3.88,249.60',\n",
       " '03.01.2009 17:20:00,995.62,-4.60,268.90,-12.20,55.10,4.34,2.39,1.95,1.49,2.40,1290.30,3.64,4.88,255.70',\n",
       " '03.01.2009 17:30:00,995.60,-4.31,269.19,-11.92,55.11,4.43,2.44,1.99,1.53,2.45,1288.86,3.26,4.38,252.60',\n",
       " '03.01.2009 17:40:00,995.59,-4.18,269.32,-11.70,55.57,4.48,2.49,1.99,1.56,2.50,1288.21,3.15,4.63,250.80',\n",
       " '03.01.2009 17:50:00,995.51,-4.16,269.34,-11.57,56.06,4.48,2.51,1.97,1.57,2.53,1287.99,3.44,4.75,253.20',\n",
       " '03.01.2009 18:00:00,995.48,-4.08,269.43,-11.44,56.32,4.51,2.54,1.97,1.59,2.55,1287.53,3.12,4.63,250.50',\n",
       " '03.01.2009 18:10:00,995.47,-3.98,269.53,-11.35,56.28,4.54,2.56,1.99,1.60,2.57,1287.06,2.49,3.88,244.50',\n",
       " '03.01.2009 18:20:00,995.42,-3.91,269.60,-11.23,56.56,4.57,2.58,1.99,1.62,2.60,1286.64,2.29,3.25,247.50',\n",
       " '03.01.2009 18:30:00,995.41,-3.90,269.62,-11.06,57.28,4.57,2.62,1.95,1.64,2.63,1286.55,1.83,2.75,240.90',\n",
       " '03.01.2009 18:40:00,995.34,-3.90,269.62,-10.89,58.04,4.57,2.65,1.92,1.66,2.67,1286.44,1.82,2.50,246.70',\n",
       " '03.01.2009 18:50:00,995.37,-3.87,269.65,-10.69,58.83,4.59,2.70,1.89,1.69,2.71,1286.30,1.73,2.38,243.60',\n",
       " '03.01.2009 19:00:00,995.44,-3.78,269.73,-10.50,59.35,4.62,2.74,1.88,1.71,2.75,1285.95,0.96,2.13,236.30',\n",
       " '03.01.2009 19:10:00,995.28,-3.77,269.75,-10.47,59.45,4.62,2.74,1.87,1.72,2.76,1285.72,1.15,1.88,173.70',\n",
       " '03.01.2009 19:20:00,995.26,-3.95,269.58,-10.37,60.72,4.56,2.77,1.79,1.73,2.78,1286.51,1.15,2.13,185.10',\n",
       " '03.01.2009 19:30:00,995.19,-3.90,269.63,-10.02,62.26,4.57,2.85,1.73,1.78,2.86,1286.16,0.78,1.75,185.90',\n",
       " '03.01.2009 19:40:00,995.15,-3.75,269.79,-9.72,63.02,4.63,2.92,1.71,1.82,2.93,1285.34,0.90,2.00,145.90',\n",
       " '03.01.2009 19:50:00,995.07,-3.81,269.74,-9.58,63.99,4.61,2.95,1.66,1.84,2.96,1285.50,1.30,2.25,124.30',\n",
       " '03.01.2009 20:00:00,994.94,-3.84,269.72,-9.56,64.22,4.60,2.95,1.64,1.85,2.97,1285.48,0.91,2.13,143.40',\n",
       " '03.01.2009 20:10:00,994.76,-3.88,269.69,-9.47,64.90,4.58,2.97,1.61,1.86,2.99,1285.44,0.52,1.13,111.00',\n",
       " '03.01.2009 20:20:00,994.65,-3.67,269.90,-9.00,66.32,4.65,3.09,1.57,1.93,3.10,1284.27,1.16,3.25,2.64',\n",
       " '03.01.2009 20:30:00,994.64,-3.30,270.27,-8.75,65.81,4.78,3.15,1.64,1.97,3.17,1282.45,0.78,1.88,176.30',\n",
       " '03.01.2009 20:40:00,994.57,-3.34,270.24,-8.67,66.42,4.77,3.17,1.60,1.98,3.19,1282.54,0.69,1.63,151.90',\n",
       " '03.01.2009 20:50:00,994.45,-3.24,270.35,-8.58,66.37,4.81,3.19,1.62,2.00,3.21,1281.89,0.93,1.88,231.80',\n",
       " '03.01.2009 21:00:00,994.30,-3.15,270.45,-8.48,66.47,4.84,3.22,1.62,2.01,3.23,1281.27,0.97,2.00,220.00',\n",
       " '03.01.2009 21:10:00,994.23,-3.07,270.54,-8.36,66.65,4.87,3.25,1.62,2.03,3.26,1280.75,1.42,2.50,226.10',\n",
       " '03.01.2009 21:20:00,994.20,-2.93,270.68,-8.24,66.56,4.92,3.28,1.65,2.05,3.29,1280.04,1.40,2.88,237.00',\n",
       " '03.01.2009 21:30:00,994.04,-2.78,270.84,-8.14,66.40,4.97,3.30,1.67,2.07,3.32,1279.14,1.37,2.50,228.00',\n",
       " '03.01.2009 21:40:00,994.08,-2.76,270.86,-8.08,66.63,4.98,3.32,1.66,2.08,3.34,1279.09,1.37,2.50,226.30',\n",
       " '03.01.2009 21:50:00,994.06,-2.76,270.86,-7.99,67.09,4.98,3.34,1.64,2.09,3.36,1279.05,1.20,2.25,206.40',\n",
       " '03.01.2009 22:00:00,994.06,-2.76,270.86,-7.88,67.66,4.98,3.37,1.61,2.11,3.39,1279.04,1.03,2.00,214.80',\n",
       " '03.01.2009 22:10:00,994.01,-2.71,270.92,-7.74,68.08,5.00,3.41,1.60,2.13,3.43,1278.68,1.08,2.25,233.90',\n",
       " '03.01.2009 22:20:00,993.77,-2.67,270.98,-7.61,68.59,5.02,3.44,1.58,2.16,3.46,1278.18,1.73,2.50,217.60',\n",
       " '03.01.2009 22:30:00,993.61,-2.69,270.97,-7.53,69.12,5.01,3.46,1.55,2.17,3.48,1278.06,1.31,2.13,217.00',\n",
       " '03.01.2009 22:40:00,993.54,-2.70,270.97,-7.40,69.86,5.01,3.50,1.51,2.19,3.52,1277.99,1.23,2.50,219.80',\n",
       " '03.01.2009 22:50:00,993.43,-2.64,271.04,-7.15,70.90,5.03,3.57,1.46,2.24,3.59,1277.52,1.44,2.88,224.40',\n",
       " '03.01.2009 23:00:00,993.28,-2.58,271.10,-7.05,71.20,5.05,3.59,1.45,2.25,3.62,1277.07,1.60,4.25,237.70',\n",
       " '03.01.2009 23:10:00,993.14,-2.46,271.23,-6.93,71.20,5.09,3.63,1.47,2.27,3.65,1276.32,1.69,2.75,233.60',\n",
       " '03.01.2009 23:20:00,993.13,-2.33,271.37,-6.79,71.30,5.15,3.67,1.48,2.30,3.69,1275.64,1.05,2.75,199.20',\n",
       " '03.01.2009 23:30:00,993.14,-2.12,271.58,-6.59,71.30,5.23,3.73,1.50,2.34,3.75,1274.64,1.19,2.25,81.30',\n",
       " '03.01.2009 23:40:00,993.13,-1.97,271.73,-6.49,71.00,5.28,3.75,1.53,2.35,3.78,1273.90,1.26,2.75,23.71',\n",
       " '03.01.2009 23:50:00,993.05,-1.90,271.80,-6.46,70.80,5.31,3.76,1.55,2.36,3.79,1273.47,1.56,2.75,119.80',\n",
       " '04.01.2009 00:00:00,992.93,-1.90,271.81,-6.39,71.20,5.31,3.78,1.53,2.37,3.81,1273.31,1.47,2.50,147.60',\n",
       " '04.01.2009 00:10:00,992.79,-1.83,271.89,-6.29,71.40,5.34,3.81,1.53,2.39,3.84,1272.79,1.44,2.63,214.10',\n",
       " '04.01.2009 00:20:00,992.67,-1.70,272.03,-6.09,71.80,5.39,3.87,1.52,2.43,3.90,1272.00,1.04,2.63,188.80',\n",
       " '04.01.2009 00:30:00,992.57,-1.60,272.14,-5.92,72.20,5.43,3.92,1.51,2.46,3.95,1271.37,1.60,3.50,176.60',\n",
       " '04.01.2009 00:40:00,992.54,-1.55,272.19,-5.72,73.10,5.45,3.98,1.47,2.50,4.01,1271.10,1.06,2.38,215.70',\n",
       " '04.01.2009 00:50:00,992.41,-1.46,272.29,-5.61,73.20,5.49,4.02,1.47,2.52,4.05,1270.49,1.90,4.75,243.60',\n",
       " '04.01.2009 01:00:00,992.19,-1.42,272.35,-5.61,73.00,5.50,4.02,1.49,2.52,4.05,1270.01,2.77,4.75,232.10',\n",
       " '04.01.2009 01:10:00,992.09,-1.49,272.28,-5.25,75.40,5.47,4.13,1.35,2.59,4.16,1270.17,3.25,4.63,246.70',\n",
       " '04.01.2009 01:20:00,991.98,-2.01,271.77,-4.42,83.50,5.27,4.40,0.87,2.76,4.43,1272.33,2.63,5.50,238.00',\n",
       " '04.01.2009 01:30:00,991.97,-2.32,271.47,-4.08,87.60,5.15,4.51,0.64,2.83,4.55,1273.68,1.70,3.50,229.40',\n",
       " '04.01.2009 01:40:00,991.85,-2.37,271.42,-3.88,89.30,5.13,4.58,0.55,2.88,4.62,1273.77,2.18,4.13,231.20',\n",
       " '04.01.2009 01:50:00,991.84,-2.32,271.48,-3.72,90.00,5.15,4.63,0.51,2.91,4.67,1273.46,2.13,4.00,228.40',\n",
       " '04.01.2009 02:00:00,991.76,-2.22,271.58,-3.61,90.10,5.19,4.67,0.51,2.94,4.71,1272.89,1.82,4.75,201.90',\n",
       " '04.01.2009 02:10:00,991.73,-2.09,271.72,-3.53,89.80,5.24,4.70,0.53,2.96,4.74,1272.20,1.29,3.25,138.00',\n",
       " '04.01.2009 02:20:00,991.65,-2.04,271.77,-3.51,89.60,5.26,4.71,0.55,2.96,4.75,1271.87,1.36,2.25,124.60',\n",
       " '04.01.2009 02:30:00,991.58,-2.25,271.56,-3.73,89.50,5.17,4.63,0.54,2.91,4.67,1272.82,1.24,2.75,118.60',\n",
       " '04.01.2009 02:40:00,991.49,-2.54,271.29,-3.87,90.50,5.07,4.58,0.48,2.88,4.62,1274.06,1.35,2.88,62.46',\n",
       " '04.01.2009 02:50:00,991.36,-2.26,271.58,-3.50,91.10,5.17,4.71,0.46,2.96,4.75,1272.52,0.99,2.00,114.50',\n",
       " '04.01.2009 03:00:00,991.26,-2.20,271.64,-3.52,90.60,5.20,4.71,0.49,2.96,4.75,1272.12,0.87,1.88,94.40',\n",
       " '04.01.2009 03:10:00,991.24,-2.16,271.68,-3.48,90.60,5.21,4.72,0.49,2.97,4.76,1271.91,1.04,3.00,112.50',\n",
       " '04.01.2009 03:20:00,991.15,-2.05,271.80,-3.36,90.70,5.25,4.76,0.49,3.00,4.81,1271.25,0.87,2.88,153.80',\n",
       " '04.01.2009 03:30:00,991.09,-1.90,271.96,-3.31,90.00,5.31,4.78,0.53,3.01,4.82,1270.47,1.01,2.13,167.60',\n",
       " '04.01.2009 03:40:00,991.00,-1.79,272.08,-3.26,89.60,5.36,4.80,0.56,3.02,4.84,1269.82,1.18,3.13,230.60',\n",
       " '04.01.2009 03:50:00,990.89,-1.79,272.08,-3.31,89.30,5.35,4.78,0.57,3.01,4.82,1269.71,1.46,3.75,190.30',\n",
       " '04.01.2009 04:00:00,990.83,-1.88,272.00,-3.29,90.00,5.32,4.79,0.53,3.01,4.83,1270.02,0.62,1.75,167.30',\n",
       " '04.01.2009 04:10:00,990.74,-1.87,272.01,-3.24,90.30,5.32,4.81,0.52,3.02,4.85,1269.87,0.83,2.00,83.60',\n",
       " '04.01.2009 04:20:00,990.69,-1.78,272.10,-3.09,90.70,5.36,4.86,0.50,3.06,4.90,1269.37,1.69,3.50,217.90',\n",
       " '04.01.2009 04:30:00,990.63,-1.66,272.24,-2.98,90.60,5.41,4.90,0.51,3.08,4.95,1268.68,0.83,1.63,174.10',\n",
       " '04.01.2009 04:40:00,990.50,-1.64,272.27,-2.96,90.60,5.42,4.91,0.51,3.09,4.95,1268.42,0.88,1.88,203.30',\n",
       " '04.01.2009 04:50:00,990.45,-1.64,272.27,-2.92,90.90,5.42,4.92,0.49,3.10,4.97,1268.35,1.07,1.88,200.30',\n",
       " '04.01.2009 05:00:00,990.37,-1.64,272.28,-2.87,91.20,5.42,4.94,0.48,3.11,4.99,1268.23,1.31,3.13,215.40',\n",
       " '04.01.2009 05:10:00,990.33,-1.50,272.41,-2.77,91.00,5.47,4.98,0.49,3.13,5.03,1267.55,1.24,3.50,201.60',\n",
       " '04.01.2009 05:20:00,990.25,-1.44,272.48,-2.72,90.90,5.50,5.00,0.50,3.14,5.05,1267.13,1.48,3.13,236.50',\n",
       " '04.01.2009 05:30:00,990.22,-1.47,272.46,-2.74,91.00,5.48,4.99,0.49,3.14,5.04,1267.23,1.15,2.25,174.20',\n",
       " '04.01.2009 05:40:00,990.20,-1.49,272.44,-2.70,91.40,5.48,5.01,0.47,3.15,5.06,1267.28,1.17,2.25,158.40',\n",
       " '04.01.2009 05:50:00,990.09,-1.44,272.50,-2.64,91.50,5.50,5.03,0.47,3.17,5.08,1266.91,1.24,2.63,205.50',\n",
       " '04.01.2009 06:00:00,989.99,-1.44,272.50,-2.66,91.30,5.50,5.02,0.48,3.16,5.07,1266.78,1.00,2.38,200.90',\n",
       " '04.01.2009 06:10:00,989.96,-1.44,272.51,-2.68,91.20,5.50,5.01,0.48,3.16,5.06,1266.74,1.47,3.75,221.30',\n",
       " '04.01.2009 06:20:00,989.95,-1.39,272.55,-2.66,91.00,5.51,5.02,0.50,3.16,5.07,1266.52,1.48,3.00,231.00',\n",
       " '04.01.2009 06:30:00,989.92,-1.33,272.62,-2.63,90.80,5.54,5.03,0.51,3.17,5.08,1266.18,1.34,2.63,239.80',\n",
       " '04.01.2009 06:40:00,989.94,-1.25,272.69,-2.66,90.10,5.57,5.02,0.55,3.16,5.07,1265.86,1.36,3.13,226.30',\n",
       " '04.01.2009 06:50:00,989.88,-1.14,272.81,-2.65,89.40,5.62,5.02,0.60,3.16,5.07,1265.24,2.48,7.13,254.40',\n",
       " '04.01.2009 07:00:00,989.90,-1.09,272.86,-2.65,89.10,5.64,5.02,0.61,3.16,5.08,1265.04,2.75,5.75,246.80',\n",
       " '04.01.2009 07:10:00,989.93,-1.00,272.95,-2.62,88.70,5.68,5.04,0.64,3.17,5.09,1264.65,3.95,6.88,255.80',\n",
       " '04.01.2009 07:20:00,989.96,-0.92,273.03,-2.58,88.40,5.71,5.05,0.66,3.18,5.10,1264.30,2.68,5.63,257.00',\n",
       " '04.01.2009 07:30:00,990.00,-0.90,273.04,-2.55,88.50,5.72,5.06,0.66,3.18,5.11,1264.29,3.25,5.50,251.20',\n",
       " '04.01.2009 07:40:00,990.03,-0.86,273.08,-2.51,88.50,5.73,5.08,0.66,3.19,5.13,1264.12,2.58,5.50,249.90',\n",
       " '04.01.2009 07:50:00,989.97,-0.90,273.05,-2.50,88.80,5.72,5.08,0.64,3.20,5.13,1264.22,2.30,4.75,256.70',\n",
       " '04.01.2009 08:00:00,989.92,-0.89,273.06,-2.45,89.10,5.72,5.10,0.62,3.21,5.15,1264.12,2.93,5.00,242.60',\n",
       " '04.01.2009 08:10:00,989.96,-0.81,273.14,-2.45,88.60,5.76,5.10,0.66,3.21,5.15,1263.78,2.32,5.00,251.20',\n",
       " '04.01.2009 08:20:00,989.95,-0.76,273.19,-2.49,88.00,5.78,5.08,0.69,3.20,5.14,1263.54,1.98,4.13,231.50',\n",
       " '04.01.2009 08:30:00,989.97,-0.70,273.24,-2.51,87.50,5.80,5.08,0.73,3.20,5.13,1263.32,2.12,4.13,254.90',\n",
       " '04.01.2009 08:40:00,989.97,-0.70,273.24,-2.57,87.10,5.80,5.05,0.75,3.18,5.10,1263.32,1.80,3.88,243.40',\n",
       " '04.01.2009 08:50:00,989.90,-0.76,273.19,-2.69,86.70,5.78,5.01,0.77,3.15,5.06,1263.53,1.53,3.13,232.40',\n",
       " '04.01.2009 09:00:00,989.95,-0.72,273.23,-2.73,86.20,5.79,4.99,0.80,3.14,5.04,1263.42,1.70,3.88,254.00',\n",
       " '04.01.2009 09:10:00,989.95,-0.90,273.05,-2.84,86.60,5.72,4.95,0.77,3.12,5.00,1264.26,2.71,5.13,220.70',\n",
       " '04.01.2009 09:20:00,989.89,-1.21,272.74,-2.79,88.90,5.59,4.97,0.62,3.13,5.02,1265.61,1.77,4.00,225.50',\n",
       " '04.01.2009 09:30:00,989.87,-1.31,272.64,-2.74,89.90,5.55,4.99,0.56,3.14,5.04,1266.05,1.67,2.50,212.10',\n",
       " '04.01.2009 09:40:00,989.87,-1.37,272.58,-2.79,90.00,5.52,4.97,0.55,3.13,5.02,1266.33,2.87,5.13,231.40',\n",
       " '04.01.2009 09:50:00,989.86,-1.37,272.58,-2.79,90.00,5.52,4.97,0.55,3.13,5.02,1266.33,2.90,5.00,225.40',\n",
       " '04.01.2009 10:00:00,989.86,-1.37,272.59,-2.71,90.50,5.53,5.00,0.52,3.15,5.05,1266.28,2.59,4.50,232.90',\n",
       " '04.01.2009 10:10:00,989.87,-1.30,272.66,-2.64,90.50,5.55,5.03,0.53,3.16,5.08,1265.96,2.44,4.50,221.70',\n",
       " '04.01.2009 10:20:00,989.86,-1.20,272.75,-2.61,90.10,5.59,5.04,0.55,3.17,5.09,1265.51,2.65,4.13,224.80',\n",
       " '04.01.2009 10:30:00,989.83,-1.14,272.81,-2.54,90.20,5.62,5.07,0.55,3.19,5.12,1265.19,2.44,4.13,236.40',\n",
       " '04.01.2009 10:40:00,989.76,-1.10,272.86,-2.48,90.30,5.63,5.09,0.55,3.20,5.14,1264.90,2.44,4.13,235.60',\n",
       " '04.01.2009 10:50:00,989.76,-1.10,272.86,-2.51,90.10,5.63,5.08,0.56,3.20,5.13,1264.89,2.58,4.25,237.30',\n",
       " '04.01.2009 11:00:00,989.64,-1.10,272.87,-2.53,90.00,5.63,5.07,0.56,3.19,5.12,1264.75,2.38,4.50,234.60',\n",
       " '04.01.2009 11:10:00,989.49,-1.12,272.86,-2.53,90.10,5.63,5.07,0.56,3.19,5.12,1264.63,2.14,5.13,223.90',\n",
       " '04.01.2009 11:20:00,989.38,-1.11,272.88,-2.47,90.40,5.63,5.09,0.54,3.21,5.15,1264.43,2.58,4.50,229.30',\n",
       " '04.01.2009 11:30:00,989.29,-1.10,272.90,-2.44,90.60,5.63,5.10,0.53,3.22,5.16,1264.29,2.19,5.13,224.20',\n",
       " '04.01.2009 11:40:00,989.14,-1.16,272.85,-2.42,91.10,5.61,5.11,0.50,3.22,5.17,1264.36,1.92,4.63,225.00',\n",
       " '04.01.2009 11:50:00,989.03,-1.17,272.85,-2.36,91.60,5.60,5.13,0.47,3.23,5.19,1264.26,1.97,4.13,222.50',\n",
       " '04.01.2009 12:00:00,988.94,-1.21,272.82,-2.36,91.80,5.59,5.13,0.46,3.23,5.19,1264.30,1.99,3.88,225.80',\n",
       " '04.01.2009 12:10:00,988.78,-1.12,272.92,-2.29,91.70,5.62,5.16,0.47,3.25,5.22,1263.70,2.07,4.50,232.30',\n",
       " '04.01.2009 12:20:00,988.62,-0.99,273.06,-2.27,91.00,5.68,5.17,0.51,3.26,5.23,1262.88,2.43,5.13,231.40',\n",
       " '04.01.2009 12:30:00,988.49,-0.94,273.12,-2.39,89.80,5.70,5.12,0.58,3.23,5.18,1262.50,2.58,4.50,233.20',\n",
       " '04.01.2009 12:40:00,988.34,-0.86,273.22,-2.43,89.00,5.74,5.10,0.63,3.22,5.17,1261.93,2.40,4.38,225.10',\n",
       " '04.01.2009 12:50:00,988.20,-0.92,273.16,-2.53,88.80,5.71,5.07,0.64,3.20,5.13,1262.07,3.04,6.38,226.90',\n",
       " '04.01.2009 13:00:00,988.07,-1.02,273.08,-2.53,89.40,5.67,5.07,0.60,3.20,5.13,1262.35,3.08,6.50,227.60',\n",
       " '04.01.2009 13:10:00,987.96,-0.99,273.11,-2.46,89.70,5.68,5.09,0.59,3.21,5.16,1262.07,2.22,3.63,225.20',\n",
       " '04.01.2009 13:20:00,987.88,-1.00,273.11,-2.50,89.50,5.68,5.08,0.60,3.21,5.14,1262.01,3.13,5.75,236.70',\n",
       " '04.01.2009 13:30:00,987.88,-1.04,273.07,-2.51,89.70,5.66,5.08,0.58,3.20,5.14,1262.19,2.30,4.50,228.50',\n",
       " '04.01.2009 13:40:00,987.81,-0.99,273.13,-2.43,89.90,5.68,5.11,0.57,3.22,5.17,1261.86,1.38,3.50,235.40',\n",
       " '04.01.2009 13:50:00,987.69,-0.83,273.29,-2.32,89.60,5.75,5.15,0.60,3.25,5.21,1260.96,1.12,2.50,209.10',\n",
       " '04.01.2009 14:00:00,987.57,-0.77,273.37,-2.33,89.10,5.77,5.14,0.63,3.25,5.21,1260.53,1.50,2.88,229.60',\n",
       " '04.01.2009 14:10:00,987.51,-0.79,273.35,-2.44,88.50,5.77,5.10,0.66,3.22,5.17,1260.54,2.31,5.00,214.50',\n",
       " '04.01.2009 14:20:00,987.38,-0.87,273.28,-2.57,88.20,5.73,5.05,0.68,3.19,5.12,1260.80,2.81,4.63,209.90',\n",
       " '04.01.2009 14:30:00,987.23,-0.90,273.26,-2.67,87.70,5.72,5.01,0.70,3.17,5.08,1260.76,3.71,5.75,215.20',\n",
       " '04.01.2009 14:40:00,987.15,-0.90,273.27,-2.71,87.40,5.72,5.00,0.72,3.16,5.06,1260.64,2.99,6.00,211.80',\n",
       " '04.01.2009 14:50:00,987.13,-0.79,273.38,-2.63,87.30,5.76,5.03,0.73,3.18,5.10,1260.12,1.60,3.00,220.20',\n",
       " '04.01.2009 15:00:00,987.03,-0.70,273.48,-2.52,87.40,5.80,5.07,0.73,3.20,5.14,1259.56,0.61,1.88,249.10',\n",
       " '04.01.2009 15:10:00,987.00,-0.63,273.55,-2.49,87.20,5.83,5.09,0.75,3.21,5.15,1259.19,1.17,3.00,189.80',\n",
       " '04.01.2009 15:20:00,986.95,-0.75,273.44,-2.61,87.10,5.78,5.04,0.75,3.18,5.10,1259.67,1.23,2.63,204.70',\n",
       " '04.01.2009 15:30:00,986.90,-0.84,273.35,-2.63,87.60,5.74,5.03,0.71,3.18,5.10,1260.04,1.01,2.13,213.70',\n",
       " '04.01.2009 15:40:00,986.79,-0.96,273.24,-2.70,87.90,5.69,5.00,0.69,3.16,5.07,1260.47,2.04,3.25,222.10',\n",
       " '04.01.2009 15:50:00,986.78,-1.02,273.18,-2.72,88.20,5.67,5.00,0.67,3.16,5.07,1260.74,1.14,3.13,229.90',\n",
       " '04.01.2009 16:00:00,986.69,-1.05,273.15,-2.70,88.50,5.65,5.00,0.65,3.16,5.07,1260.77,1.03,2.13,192.80',\n",
       " '04.01.2009 16:10:00,986.65,-1.13,273.07,-2.80,88.40,5.62,4.97,0.65,3.14,5.04,1261.11,2.18,3.63,230.80',\n",
       " '04.01.2009 16:20:00,986.63,-1.22,272.99,-2.82,88.80,5.59,4.96,0.63,3.13,5.03,1261.49,1.19,3.25,231.80',\n",
       " '04.01.2009 16:30:00,986.63,-1.29,272.92,-2.83,89.20,5.56,4.96,0.60,3.13,5.02,1261.81,1.59,4.25,228.20',\n",
       " '04.01.2009 16:40:00,986.65,-1.40,272.81,-2.83,89.90,5.51,4.96,0.56,3.13,5.02,1262.35,1.67,3.50,231.30',\n",
       " '04.01.2009 16:50:00,986.59,-1.44,272.77,-2.80,90.40,5.50,4.97,0.53,3.14,5.04,1262.45,1.40,2.63,211.40',\n",
       " '04.01.2009 17:00:00,986.54,-1.46,272.76,-2.77,90.70,5.49,4.98,0.51,3.14,5.05,1262.46,1.10,2.25,220.70',\n",
       " '04.01.2009 17:10:00,986.45,-1.41,272.81,-2.70,90.90,5.51,5.00,0.50,3.16,5.07,1262.14,1.17,3.50,245.10',\n",
       " '04.01.2009 17:20:00,986.48,-1.44,272.78,-2.69,91.10,5.50,5.01,0.49,3.16,5.08,1262.29,0.86,2.75,190.70',\n",
       " '04.01.2009 17:30:00,986.39,-1.42,272.81,-2.65,91.30,5.50,5.02,0.48,3.17,5.09,1262.09,0.54,1.38,225.50',\n",
       " '04.01.2009 17:40:00,986.24,-1.45,272.79,-2.66,91.40,5.49,5.02,0.47,3.17,5.09,1262.01,1.15,2.50,211.60',\n",
       " '04.01.2009 17:50:00,986.17,-1.49,272.75,-2.70,91.40,5.47,5.00,0.47,3.16,5.07,1262.13,1.60,2.88,237.20',\n",
       " '04.01.2009 18:00:00,986.15,-1.51,272.74,-2.73,91.30,5.47,4.99,0.48,3.16,5.06,1262.18,1.39,2.50,217.00',\n",
       " '04.01.2009 18:10:00,986.07,-1.51,272.74,-2.73,91.30,5.47,4.99,0.48,3.15,5.06,1262.10,1.09,2.25,189.60',\n",
       " '04.01.2009 18:20:00,986.05,-1.51,272.75,-2.67,91.70,5.47,5.01,0.45,3.17,5.09,1262.04,0.95,2.63,180.40',\n",
       " '04.01.2009 18:30:00,986.04,-1.56,272.70,-2.71,91.80,5.45,5.00,0.45,3.16,5.07,1262.29,1.36,3.25,206.00',\n",
       " '04.01.2009 18:40:00,985.98,-1.57,272.69,-2.69,92.00,5.44,5.01,0.44,3.16,5.08,1262.27,0.79,1.88,220.90',\n",
       " '04.01.2009 18:50:00,985.93,-1.58,272.69,-2.70,92.00,5.44,5.01,0.44,3.16,5.08,1262.23,1.72,4.25,218.70',\n",
       " '04.01.2009 19:00:00,985.92,-1.62,272.64,-2.73,92.10,5.42,4.99,0.43,3.16,5.07,1262.42,0.97,2.25,191.30',\n",
       " '04.01.2009 19:10:00,985.83,-1.56,272.71,-2.65,92.20,5.45,5.02,0.42,3.17,5.09,1262.01,1.56,3.00,210.90',\n",
       " '04.01.2009 19:20:00,985.81,-1.63,272.64,-2.73,92.20,5.42,4.99,0.42,3.16,5.07,1262.35,1.47,3.88,219.30',\n",
       " '04.01.2009 19:30:00,985.83,-1.60,272.67,-2.68,92.30,5.43,5.01,0.42,3.17,5.08,1262.21,1.15,3.50,227.40',\n",
       " '04.01.2009 19:40:00,985.77,-1.58,272.69,-2.68,92.20,5.44,5.01,0.42,3.17,5.09,1262.04,1.44,2.38,218.10',\n",
       " '04.01.2009 19:50:00,985.69,-1.61,272.67,-2.70,92.20,5.43,5.00,0.42,3.16,5.08,1262.09,2.06,3.38,227.50',\n",
       " '04.01.2009 20:00:00,985.68,-1.56,272.72,-2.66,92.20,5.45,5.02,0.42,3.17,5.09,1261.83,1.76,3.25,223.00',\n",
       " '04.01.2009 20:10:00,985.67,-1.51,272.78,-2.63,92.00,5.47,5.03,0.44,3.18,5.10,1261.55,1.29,2.88,221.50',\n",
       " '04.01.2009 20:20:00,985.67,-1.45,272.83,-2.56,92.10,5.49,5.06,0.43,3.20,5.13,1261.29,0.79,1.88,249.00',\n",
       " '04.01.2009 20:30:00,985.65,-1.51,272.78,-2.58,92.30,5.47,5.05,0.42,3.19,5.12,1261.52,1.36,3.13,229.20',\n",
       " '04.01.2009 20:40:00,985.56,-1.51,272.79,-2.56,92.50,5.47,5.06,0.41,3.20,5.13,1261.40,1.47,3.75,213.90',\n",
       " '04.01.2009 20:50:00,985.50,-1.51,272.79,-2.54,92.60,5.47,5.06,0.40,3.20,5.14,1261.32,1.97,4.25,236.80',\n",
       " '04.01.2009 21:00:00,985.45,-1.51,272.80,-2.51,92.80,5.47,5.08,0.39,3.21,5.15,1261.25,1.89,4.13,220.50',\n",
       " '04.01.2009 21:10:00,985.46,-1.51,272.80,-2.48,93.00,5.47,5.09,0.38,3.22,5.16,1261.25,2.20,3.50,223.10',\n",
       " '04.01.2009 21:20:00,985.43,-1.51,272.80,-2.47,93.10,5.47,5.09,0.38,3.22,5.17,1261.21,1.37,3.25,200.70',\n",
       " '04.01.2009 21:30:00,985.43,-1.49,272.82,-2.44,93.20,5.48,5.10,0.37,3.23,5.18,1261.12,1.05,3.38,211.20',\n",
       " '04.01.2009 21:40:00,985.41,-1.44,272.87,-2.37,93.30,5.50,5.13,0.37,3.24,5.20,1260.86,0.82,2.25,177.00',\n",
       " '04.01.2009 21:50:00,985.43,-1.44,272.87,-2.34,93.50,5.50,5.14,0.36,3.25,5.21,1260.88,0.81,2.75,146.50',\n",
       " '04.01.2009 22:00:00,985.43,-1.43,272.88,-2.31,93.70,5.50,5.15,0.35,3.26,5.23,1260.82,0.88,3.38,144.80',\n",
       " '04.01.2009 22:10:00,985.38,-1.38,272.92,-2.26,93.70,5.52,5.17,0.35,3.27,5.25,1260.54,0.75,2.25,227.00',\n",
       " '04.01.2009 22:20:00,985.32,-1.45,272.86,-2.33,93.70,5.49,5.15,0.35,3.25,5.22,1260.78,1.45,2.88,226.50',\n",
       " '04.01.2009 22:30:00,985.31,-1.51,272.81,-2.37,93.80,5.47,5.13,0.34,3.24,5.21,1261.04,1.79,3.25,205.60',\n",
       " '04.01.2009 22:40:00,985.31,-1.46,272.86,-2.31,93.90,5.49,5.15,0.33,3.26,5.23,1260.81,1.86,4.50,204.60',\n",
       " '04.01.2009 22:50:00,985.31,-1.44,272.87,-2.32,93.70,5.50,5.15,0.35,3.26,5.23,1260.72,2.06,3.63,224.30',\n",
       " '04.01.2009 23:00:00,985.27,-1.44,272.87,-2.35,93.50,5.49,5.14,0.36,3.25,5.21,1260.69,1.47,3.13,212.50',\n",
       " '04.01.2009 23:10:00,985.20,-1.44,272.88,-2.36,93.40,5.50,5.13,0.36,3.25,5.21,1260.59,1.59,3.00,229.00',\n",
       " '04.01.2009 23:20:00,985.19,-1.42,272.90,-2.36,93.30,5.50,5.13,0.37,3.25,5.21,1260.50,1.12,2.63,217.00',\n",
       " '04.01.2009 23:30:00,985.19,-1.44,272.88,-2.42,93.00,5.50,5.11,0.38,3.23,5.19,1260.58,1.50,3.25,234.90',\n",
       " '04.01.2009 23:40:00,985.18,-1.46,272.87,-2.45,92.90,5.49,5.10,0.39,3.23,5.18,1260.66,1.50,2.63,236.20',\n",
       " '04.01.2009 23:50:00,985.12,-1.50,272.83,-2.48,93.00,5.47,5.09,0.38,3.22,5.16,1260.79,1.28,2.50,231.40',\n",
       " '05.01.2009 00:00:00,985.07,-1.47,272.86,-2.45,93.00,5.48,5.10,0.38,3.23,5.18,1260.58,1.42,3.00,239.80',\n",
       " '05.01.2009 00:10:00,985.06,-1.50,272.83,-2.51,92.80,5.47,5.08,0.39,3.21,5.15,1260.74,1.59,3.50,231.00',\n",
       " '05.01.2009 00:20:00,985.04,-1.52,272.81,-2.51,92.90,5.46,5.07,0.39,3.21,5.15,1260.79,1.48,2.88,248.80',\n",
       " '05.01.2009 00:30:00,984.96,-1.56,272.78,-2.52,93.10,5.45,5.07,0.38,3.21,5.15,1260.85,1.55,3.25,235.00',\n",
       " '05.01.2009 00:40:00,984.94,-1.56,272.78,-2.51,93.20,5.45,5.08,0.37,3.21,5.15,1260.86,1.77,4.25,251.10',\n",
       " '05.01.2009 00:50:00,984.83,-1.57,272.78,-2.55,93.00,5.44,5.06,0.38,3.20,5.14,1260.77,2.12,4.25,256.90',\n",
       " '05.01.2009 01:00:00,984.74,-1.57,272.79,-2.58,92.80,5.44,5.05,0.39,3.20,5.13,1260.66,2.55,4.75,245.30',\n",
       " '05.01.2009 01:10:00,984.65,-1.57,272.79,-2.61,92.60,5.44,5.04,0.40,3.19,5.12,1260.55,2.84,4.63,250.10',\n",
       " '05.01.2009 01:20:00,984.60,-1.58,272.79,-2.65,92.40,5.44,5.02,0.41,3.18,5.10,1260.53,2.98,5.00,254.10',\n",
       " '05.01.2009 01:30:00,984.61,-1.57,272.80,-2.65,92.30,5.44,5.02,0.42,3.18,5.10,1260.51,2.77,5.00,257.80',\n",
       " '05.01.2009 01:40:00,984.65,-1.60,272.77,-2.69,92.20,5.43,5.01,0.42,3.17,5.09,1260.68,3.00,4.38,258.30',\n",
       " '05.01.2009 01:50:00,984.74,-1.63,272.73,-2.71,92.30,5.42,5.00,0.42,3.17,5.08,1260.94,2.95,5.63,253.40',\n",
       " '05.01.2009 02:00:00,984.82,-1.60,272.75,-2.71,92.10,5.43,5.00,0.43,3.17,5.08,1260.91,2.52,4.38,262.40',\n",
       " '05.01.2009 02:10:00,984.90,-1.57,272.77,-2.74,91.70,5.44,4.99,0.45,3.16,5.07,1260.89,2.71,4.75,263.90',\n",
       " '05.01.2009 02:20:00,984.99,-1.54,272.80,-2.78,91.20,5.45,4.97,0.48,3.15,5.05,1260.87,2.53,4.38,264.80',\n",
       " '05.01.2009 02:30:00,985.06,-1.50,272.83,-2.87,90.30,5.47,4.94,0.53,3.12,5.01,1260.80,2.19,4.25,259.60',\n",
       " '05.01.2009 02:40:00,985.11,-1.46,272.87,-3.02,89.00,5.49,4.88,0.60,3.09,4.96,1260.67,2.47,4.75,270.30',\n",
       " '05.01.2009 02:50:00,985.09,-1.44,272.89,-3.22,87.60,5.49,4.81,0.68,3.04,4.89,1260.62,2.89,4.63,255.20',\n",
       " '05.01.2009 03:00:00,985.10,-1.50,272.83,-3.34,87.20,5.47,4.77,0.70,3.02,4.84,1260.92,3.06,5.38,258.30',\n",
       " '05.01.2009 03:10:00,985.17,-1.53,272.79,-3.42,86.90,5.46,4.74,0.72,3.00,4.81,1261.17,3.25,4.63,260.10',\n",
       " '05.01.2009 03:20:00,985.28,-1.57,272.75,-3.45,86.90,5.44,4.73,0.71,2.99,4.80,1261.49,2.89,4.75,260.20',\n",
       " '05.01.2009 03:30:00,985.35,-1.61,272.70,-3.48,87.00,5.43,4.72,0.71,2.98,4.79,1261.79,2.79,4.63,261.60',\n",
       " '05.01.2009 03:40:00,985.47,-1.64,272.66,-3.45,87.40,5.41,4.73,0.68,2.99,4.80,1262.07,3.12,4.50,255.90',\n",
       " '05.01.2009 03:50:00,985.50,-1.70,272.60,-3.38,88.20,5.39,4.75,0.64,3.01,4.82,1262.37,3.23,5.13,261.70',\n",
       " '05.01.2009 04:00:00,985.47,-1.76,272.54,-3.32,89.00,5.37,4.78,0.59,3.02,4.85,1262.61,2.74,6.25,258.90',\n",
       " '05.01.2009 04:10:00,985.44,-1.77,272.53,-3.22,89.70,5.36,4.81,0.55,3.04,4.88,1262.57,3.11,5.63,260.90',\n",
       " '05.01.2009 04:20:00,985.51,-1.80,272.49,-3.25,89.80,5.35,4.80,0.55,3.04,4.87,1262.84,2.86,4.25,257.00',\n",
       " '05.01.2009 04:30:00,985.57,-1.81,272.48,-3.28,89.60,5.35,4.79,0.56,3.03,4.86,1262.96,2.86,4.63,259.60',\n",
       " '05.01.2009 04:40:00,985.65,-1.77,272.52,-3.32,89.10,5.36,4.78,0.58,3.02,4.85,1262.87,2.58,5.00,253.00',\n",
       " '05.01.2009 04:50:00,985.64,-1.77,272.52,-3.39,88.60,5.36,4.75,0.61,3.01,4.82,1262.85,2.85,5.50,256.00',\n",
       " '05.01.2009 05:00:00,985.57,-1.77,272.53,-3.45,88.20,5.36,4.73,0.63,2.99,4.80,1262.78,3.35,5.50,259.50',\n",
       " '05.01.2009 05:10:00,985.64,-1.77,272.52,-3.46,88.10,5.36,4.73,0.64,2.99,4.80,1262.87,3.35,5.00,258.40',\n",
       " '05.01.2009 05:20:00,985.59,-1.76,272.53,-3.48,88.00,5.37,4.72,0.64,2.99,4.79,1262.80,3.53,5.13,259.90',\n",
       " '05.01.2009 05:30:00,985.59,-1.76,272.53,-3.45,88.20,5.37,4.73,0.63,2.99,4.80,1262.79,3.15,4.25,257.40',\n",
       " '05.01.2009 05:40:00,985.75,-1.76,272.51,-3.45,88.20,5.37,4.73,0.63,2.99,4.80,1263.00,2.62,4.50,256.40',\n",
       " '05.01.2009 05:50:00,985.89,-1.76,272.50,-3.48,88.00,5.37,4.72,0.64,2.98,4.79,1263.18,2.16,3.63,261.10',\n",
       " '05.01.2009 06:00:00,985.97,-1.76,272.50,-3.48,87.90,5.37,4.72,0.65,2.98,4.79,1263.25,1.84,3.63,266.30',\n",
       " '05.01.2009 06:10:00,986.10,-1.70,272.55,-3.44,87.80,5.39,4.73,0.66,2.99,4.80,1263.13,1.42,4.13,284.40',\n",
       " '05.01.2009 06:20:00,986.26,-1.70,272.54,-3.47,87.60,5.39,4.72,0.67,2.98,4.79,1263.34,1.00,2.75,233.50',\n",
       " '05.01.2009 06:30:00,986.42,-1.71,272.51,-3.55,87.20,5.39,4.70,0.69,2.97,4.76,1263.64,1.42,4.13,336.10',\n",
       " '05.01.2009 06:40:00,986.52,-1.79,272.42,-3.63,87.20,5.35,4.67,0.69,2.95,4.73,1264.16,1.13,2.50,267.70',\n",
       " '05.01.2009 06:50:00,986.67,-1.92,272.29,-3.69,87.60,5.31,4.65,0.66,2.93,4.71,1264.93,1.29,3.00,8.36',\n",
       " '05.01.2009 07:00:00,986.79,-2.08,272.12,-3.82,87.80,5.24,4.60,0.64,2.91,4.66,1265.86,1.28,2.88,356.40',\n",
       " '05.01.2009 07:10:00,986.90,-2.18,272.01,-3.94,87.60,5.20,4.56,0.65,2.88,4.62,1266.48,1.16,3.50,354.40',\n",
       " '05.01.2009 07:20:00,986.99,-2.26,271.91,-4.03,87.60,5.17,4.53,0.64,2.86,4.59,1267.02,1.16,2.13,283.80',\n",
       " '05.01.2009 07:30:00,987.15,-2.31,271.85,-4.06,87.70,5.15,4.52,0.63,2.85,4.58,1267.46,1.30,2.63,183.20',\n",
       " '05.01.2009 07:40:00,987.33,-2.36,271.79,-4.13,87.60,5.13,4.50,0.64,2.84,4.55,1267.94,1.48,2.88,252.80',\n",
       " '05.01.2009 07:50:00,987.52,-2.59,271.54,-4.32,87.80,5.05,4.43,0.62,2.80,4.49,1269.28,1.55,3.63,5.86',\n",
       " '05.01.2009 08:00:00,987.67,-2.72,271.40,-4.37,88.30,5.00,4.41,0.58,2.78,4.47,1270.09,1.59,3.50,0.12',\n",
       " '05.01.2009 08:10:00,987.83,-2.83,271.28,-4.42,88.70,4.96,4.40,0.56,2.77,4.45,1270.83,1.65,4.13,30.92',\n",
       " '05.01.2009 08:20:00,987.94,-2.87,271.23,-4.48,88.60,4.94,4.38,0.56,2.76,4.43,1271.17,1.44,2.50,223.00',\n",
       " '05.01.2009 08:30:00,988.04,-2.90,271.19,-4.53,88.40,4.93,4.36,0.57,2.75,4.41,1271.43,1.08,2.13,218.50',\n",
       " '05.01.2009 08:40:00,988.19,-2.89,271.19,-4.46,88.80,4.94,4.38,0.55,2.76,4.44,1271.56,1.79,4.38,306.00',\n",
       " '05.01.2009 08:50:00,988.29,-3.03,271.05,-4.42,90.00,4.88,4.40,0.49,2.77,4.45,1272.34,1.44,2.88,275.30',\n",
       " '05.01.2009 09:00:00,988.42,-3.11,270.95,-4.35,91.10,4.85,4.42,0.43,2.79,4.47,1272.89,1.61,4.63,187.70',\n",
       " '05.01.2009 09:10:00,988.59,-3.12,270.93,-4.26,91.80,4.85,4.45,0.40,2.81,4.50,1273.15,1.89,4.25,357.00',\n",
       " '05.01.2009 09:20:00,988.71,-3.11,270.93,-4.25,91.80,4.85,4.45,0.40,2.81,4.51,1273.26,1.77,4.25,12.32',\n",
       " '05.01.2009 09:30:00,988.83,-3.18,270.85,-4.31,91.80,4.83,4.43,0.40,2.79,4.48,1273.73,1.82,4.13,351.50',\n",
       " '05.01.2009 09:40:00,988.97,-3.23,270.79,-4.42,91.40,4.81,4.40,0.41,2.77,4.45,1274.18,2.32,4.25,5.16',\n",
       " '05.01.2009 09:50:00,989.06,-3.24,270.77,-4.45,91.30,4.81,4.39,0.42,2.76,4.44,1274.36,1.93,4.63,7.63',\n",
       " '05.01.2009 10:00:00,989.23,-3.29,270.71,-4.55,90.90,4.79,4.35,0.44,2.74,4.40,1274.82,2.53,4.63,6.79',\n",
       " '05.01.2009 10:10:00,989.38,-3.30,270.69,-4.65,90.30,4.79,4.32,0.46,2.72,4.37,1275.07,2.93,5.38,358.90',\n",
       " '05.01.2009 10:20:00,989.53,-3.28,270.70,-4.69,89.90,4.79,4.31,0.48,2.71,4.35,1275.17,2.21,4.13,1.02',\n",
       " '05.01.2009 10:30:00,989.68,-3.31,270.65,-4.81,89.30,4.78,4.27,0.51,2.69,4.31,1275.53,2.66,5.75,4.46',\n",
       " '05.01.2009 10:40:00,989.76,-3.36,270.59,-4.83,89.50,4.76,4.26,0.50,2.68,4.31,1275.90,2.69,5.13,6.77',\n",
       " '05.01.2009 10:50:00,989.87,-3.46,270.48,-4.89,89.80,4.73,4.24,0.48,2.67,4.29,1276.52,3.85,6.13,8.64',\n",
       " '05.01.2009 11:00:00,989.97,-3.53,270.41,-5.11,88.70,4.70,4.17,0.53,2.63,4.21,1276.99,5.06,7.88,13.74',\n",
       " '05.01.2009 11:10:00,990.08,-3.57,270.36,-5.43,86.80,4.69,4.07,0.62,2.56,4.11,1277.36,5.25,8.00,20.67',\n",
       " '05.01.2009 11:20:00,990.20,-3.58,270.34,-5.41,87.00,4.69,4.08,0.61,2.57,4.12,1277.56,3.97,5.75,22.11',\n",
       " '05.01.2009 11:30:00,990.30,-3.62,270.30,-5.35,87.70,4.67,4.10,0.57,2.58,4.14,1277.87,3.64,6.00,17.47',\n",
       " '05.01.2009 11:40:00,990.35,-3.63,270.28,-5.38,87.60,4.67,4.09,0.58,2.57,4.13,1278.01,5.20,8.38,15.59',\n",
       " '05.01.2009 11:50:00,990.38,-3.63,270.27,-5.47,87.00,4.67,4.06,0.61,2.55,4.10,1278.07,5.89,8.00,18.85',\n",
       " '05.01.2009 12:00:00,990.42,-3.65,270.26,-5.54,86.60,4.66,4.04,0.62,2.54,4.08,1278.21,5.49,8.38,17.44',\n",
       " '05.01.2009 12:10:00,990.55,-3.73,270.16,-5.60,86.80,4.63,4.02,0.61,2.53,4.06,1278.79,4.74,7.38,20.51',\n",
       " '05.01.2009 12:20:00,990.68,-3.86,270.02,-5.59,87.70,4.59,4.02,0.56,2.53,4.06,1279.58,4.14,6.25,14.12',\n",
       " '05.01.2009 12:30:00,990.73,-4.06,269.82,-5.62,88.80,4.52,4.01,0.51,2.52,4.05,1280.57,4.45,6.25,10.26',\n",
       " '05.01.2009 12:40:00,990.78,-4.16,269.72,-5.64,89.30,4.49,4.01,0.48,2.52,4.04,1281.10,4.58,7.50,13.37',\n",
       " '05.01.2009 12:50:00,990.78,-4.17,269.71,-5.71,88.90,4.48,3.98,0.50,2.51,4.02,1281.16,5.89,8.13,18.80',\n",
       " '05.01.2009 13:00:00,990.82,-4.15,269.72,-5.84,87.90,4.49,3.94,0.54,2.48,3.98,1281.16,5.91,9.75,28.54',\n",
       " '05.01.2009 13:10:00,990.85,-4.23,269.64,-6.13,86.50,4.46,3.86,0.60,2.43,3.89,1281.61,6.48,9.13,23.81',\n",
       " '05.01.2009 13:20:00,990.94,-4.33,269.53,-6.21,86.60,4.43,3.84,0.59,2.41,3.87,1282.20,6.30,9.25,19.91',\n",
       " '05.01.2009 13:30:00,990.99,-4.37,269.49,-6.19,87.00,4.42,3.84,0.57,2.41,3.88,1282.46,5.96,8.13,24.19',\n",
       " '05.01.2009 13:40:00,991.03,-4.45,269.40,-6.20,87.50,4.39,3.84,0.55,2.41,3.87,1282.92,5.97,8.25,19.51',\n",
       " '05.01.2009 13:50:00,991.07,-4.50,269.35,-6.10,88.50,4.37,3.87,0.50,2.43,3.90,1283.18,5.49,7.25,19.58',\n",
       " '05.01.2009 14:00:00,991.08,-4.57,269.28,-6.05,89.30,4.35,3.88,0.47,2.44,3.92,1283.50,6.11,7.75,19.12',\n",
       " '05.01.2009 14:10:00,991.17,-4.68,269.17,-6.06,90.00,4.31,3.88,0.43,2.44,3.92,1284.15,6.20,8.25,19.39',\n",
       " '05.01.2009 14:20:00,991.27,-4.77,269.07,-6.20,89.60,4.28,3.84,0.45,2.41,3.87,1284.73,5.96,7.88,21.96',\n",
       " '05.01.2009 14:30:00,991.40,-4.81,269.02,-6.25,89.50,4.27,3.82,0.45,2.40,3.86,1285.09,5.24,7.38,25.22',\n",
       " '05.01.2009 14:40:00,991.47,-4.95,268.87,-6.32,90.00,4.22,3.80,0.42,2.39,3.83,1285.88,6.57,9.25,27.77',\n",
       " '05.01.2009 14:50:00,991.64,-5.08,268.72,-6.48,89.80,4.18,3.76,0.43,2.36,3.79,1286.76,5.05,6.88,28.77',\n",
       " '05.01.2009 15:00:00,991.80,-5.14,268.66,-6.54,89.80,4.16,3.74,0.42,2.35,3.77,1287.24,5.67,8.25,26.38',\n",
       " '05.01.2009 15:10:00,991.91,-5.26,268.52,-6.67,89.70,4.13,3.70,0.42,2.32,3.73,1288.00,6.75,9.75,28.06',\n",
       " '05.01.2009 15:20:00,991.99,-5.41,268.37,-6.89,89.20,4.08,3.64,0.44,2.28,3.67,1288.85,6.06,9.63,26.26',\n",
       " '05.01.2009 15:30:00,992.09,-5.49,268.28,-7.03,88.80,4.05,3.60,0.45,2.26,3.63,1289.38,6.29,8.88,30.09',\n",
       " '05.01.2009 15:40:00,992.28,-5.54,268.21,-7.05,89.00,4.04,3.59,0.44,2.26,3.62,1289.88,5.70,7.63,31.42',\n",
       " '05.01.2009 15:50:00,992.46,-5.59,268.15,-7.11,88.90,4.02,3.58,0.45,2.24,3.60,1290.35,6.67,10.13,22.48',\n",
       " '05.01.2009 16:00:00,992.67,-5.74,267.99,-7.36,88.20,3.98,3.51,0.47,2.20,3.53,1291.37,7.11,9.50,31.28',\n",
       " '05.01.2009 16:10:00,992.87,-5.77,267.93,-7.34,88.60,3.97,3.51,0.45,2.20,3.54,1291.82,5.96,9.00,29.18',\n",
       " '05.01.2009 16:20:00,992.91,-5.94,267.77,-7.52,88.50,3.92,3.47,0.45,2.17,3.49,1292.67,6.52,10.38,21.52',\n",
       " '05.01.2009 16:30:00,993.02,-6.05,267.65,-7.63,88.50,3.88,3.44,0.45,2.16,3.46,1293.38,6.45,8.63,25.72',\n",
       " '05.01.2009 16:40:00,993.19,-6.10,267.58,-7.66,88.60,3.87,3.43,0.44,2.15,3.45,1293.86,6.12,9.38,35.86',\n",
       " '05.01.2009 16:50:00,993.30,-6.21,267.46,-7.76,88.70,3.83,3.40,0.43,2.13,3.42,1294.56,6.49,9.38,24.54',\n",
       " '05.01.2009 17:00:00,993.40,-6.38,267.29,-7.95,88.50,3.79,3.35,0.44,2.10,3.37,1295.53,7.23,9.50,20.77',\n",
       " '05.01.2009 17:10:00,993.60,-6.55,267.10,-8.18,88.10,3.74,3.29,0.44,2.06,3.31,1296.67,6.53,9.00,22.97',\n",
       " '05.01.2009 17:20:00,993.78,-6.63,267.01,-8.34,87.50,3.71,3.25,0.46,2.04,3.27,1297.28,6.14,9.38,26.80',\n",
       " '05.01.2009 17:30:00,993.93,-6.70,266.92,-8.55,86.60,3.69,3.20,0.49,2.00,3.22,1297.86,6.09,8.13,32.02',\n",
       " '05.01.2009 17:40:00,994.08,-6.76,266.85,-8.56,86.90,3.68,3.19,0.48,2.00,3.21,1298.35,5.33,8.25,32.75',\n",
       " '05.01.2009 17:50:00,994.17,-6.74,266.87,-8.39,87.90,3.68,3.24,0.45,2.03,3.26,1298.32,5.12,7.25,20.06',\n",
       " '05.01.2009 18:00:00,994.25,-6.85,266.75,-8.55,87.60,3.65,3.20,0.45,2.00,3.22,1299.00,6.34,9.25,20.57',\n",
       " '05.01.2009 18:10:00,994.27,-7.08,266.52,-9.08,85.50,3.59,3.07,0.52,1.92,3.08,1300.21,6.73,10.38,21.48',\n",
       " '05.01.2009 18:20:00,994.29,-7.45,266.15,-10.08,81.30,3.48,2.83,0.65,1.77,2.85,1302.17,7.81,10.88,18.93',\n",
       " '05.01.2009 18:30:00,994.42,-7.62,265.97,-10.51,79.60,3.44,2.74,0.70,1.71,2.75,1303.22,7.38,9.63,21.41',\n",
       " '05.01.2009 18:40:00,994.49,-7.73,265.85,-10.74,78.80,3.41,2.69,0.72,1.68,2.70,1303.87,7.62,9.88,25.07',\n",
       " '05.01.2009 18:50:00,994.54,-7.80,265.78,-11.13,76.80,3.39,2.60,0.79,1.63,2.62,1304.33,7.16,9.25,23.63',\n",
       " '05.01.2009 19:00:00,994.69,-7.90,265.66,-11.46,75.40,3.36,2.54,0.83,1.59,2.55,1305.05,7.29,10.38,20.94',\n",
       " '05.01.2009 19:10:00,994.93,-8.01,265.54,-11.63,75.00,3.34,2.50,0.83,1.57,2.51,1305.92,6.91,9.63,23.47',\n",
       " '05.01.2009 19:20:00,995.06,-8.13,265.41,-11.73,75.10,3.30,2.48,0.82,1.55,2.49,1306.69,6.77,9.13,22.11',\n",
       " '05.01.2009 19:30:00,995.19,-8.31,265.22,-11.99,74.60,3.26,2.43,0.83,1.52,2.44,1307.78,6.25,9.13,22.52',\n",
       " '05.01.2009 19:40:00,995.38,-8.55,264.96,-12.19,74.80,3.20,2.39,0.81,1.50,2.40,1309.24,5.97,8.25,18.50',\n",
       " '05.01.2009 19:50:00,995.52,-8.87,264.63,-12.51,74.70,3.12,2.33,0.79,1.46,2.34,1311.03,6.47,8.75,20.94',\n",
       " '05.01.2009 20:00:00,995.62,-9.13,264.36,-12.71,75.00,3.05,2.29,0.76,1.43,2.30,1312.47,5.45,7.88,16.84',\n",
       " '05.01.2009 20:10:00,995.84,-9.35,264.12,-12.89,75.20,3.00,2.26,0.74,1.41,2.27,1313.88,5.33,7.13,16.27',\n",
       " '05.01.2009 20:20:00,995.93,-9.55,263.92,-13.06,75.40,2.95,2.23,0.73,1.39,2.24,1315.01,5.03,7.13,12.09',\n",
       " '05.01.2009 20:30:00,995.93,-9.73,263.74,-13.18,75.70,2.91,2.20,0.71,1.38,2.21,1315.92,4.87,6.50,14.07',\n",
       " '05.01.2009 20:40:00,996.10,-9.90,263.55,-13.30,76.00,2.87,2.18,0.69,1.36,2.19,1317.00,3.74,5.25,14.93',\n",
       " '05.01.2009 20:50:00,996.23,-10.07,263.37,-13.40,76.40,2.84,2.17,0.67,1.35,2.17,1318.03,4.19,6.25,8.33',\n",
       " '05.01.2009 21:00:00,996.43,-10.18,263.25,-13.51,76.40,2.81,2.15,0.66,1.34,2.15,1318.87,3.77,5.75,10.63',\n",
       " '05.01.2009 21:10:00,996.58,-10.32,263.10,-13.63,76.50,2.78,2.13,0.65,1.33,2.13,1319.77,2.79,4.38,14.44',\n",
       " '05.01.2009 21:20:00,996.71,-10.41,263.00,-13.67,76.80,2.76,2.12,0.64,1.32,2.13,1320.40,2.40,4.38,16.53',\n",
       " '05.01.2009 21:30:00,996.78,-10.44,262.96,-13.74,76.50,2.75,2.11,0.65,1.32,2.11,1320.66,2.97,4.88,26.59',\n",
       " '05.01.2009 21:40:00,996.87,-10.46,262.94,-13.78,76.40,2.75,2.10,0.65,1.31,2.11,1320.87,2.35,4.00,31.94',\n",
       " '05.01.2009 21:50:00,996.95,-10.50,262.89,-13.88,76.00,2.74,2.08,0.66,1.30,2.09,1321.19,2.01,3.50,35.38',\n",
       " '05.01.2009 22:00:00,996.99,-10.60,262.79,-13.95,76.20,2.72,2.07,0.65,1.29,2.08,1321.75,2.61,3.75,25.33',\n",
       " '05.01.2009 22:10:00,997.04,-10.74,262.64,-14.02,76.60,2.69,2.06,0.63,1.29,2.06,1322.53,2.74,3.75,22.86',\n",
       " '05.01.2009 22:20:00,997.11,-10.88,262.50,-14.08,77.10,2.66,2.05,0.61,1.28,2.05,1323.34,2.54,4.00,18.33',\n",
       " '05.01.2009 22:30:00,997.13,-11.09,262.29,-14.20,77.60,2.61,2.03,0.59,1.27,2.03,1324.43,3.20,4.38,14.47',\n",
       " '05.01.2009 22:40:00,997.11,-11.32,262.06,-14.33,78.20,2.56,2.01,0.56,1.25,2.01,1325.59,3.48,5.13,17.36',\n",
       " '05.01.2009 22:50:00,997.22,-11.50,261.87,-14.43,78.70,2.53,1.99,0.54,1.24,2.00,1326.65,2.70,4.25,12.10',\n",
       " '05.01.2009 23:00:00,997.29,-11.72,261.64,-14.57,79.20,2.48,1.97,0.52,1.23,1.97,1327.86,2.96,5.25,8.06',\n",
       " '05.01.2009 23:10:00,997.42,-11.96,261.39,-14.73,79.70,2.44,1.94,0.49,1.21,1.95,1329.27,2.12,3.38,2.77',\n",
       " '05.01.2009 23:20:00,997.51,-12.16,261.19,-14.83,80.30,2.40,1.92,0.47,1.20,1.93,1330.42,2.01,3.00,5.68',\n",
       " '05.01.2009 23:30:00,997.54,-12.30,261.04,-14.89,80.80,2.37,1.91,0.45,1.19,1.92,1331.17,1.56,2.75,351.00',\n",
       " '05.01.2009 23:40:00,997.50,-12.43,260.92,-14.96,81.20,2.34,1.90,0.44,1.19,1.91,1331.79,1.22,2.25,350.40',\n",
       " '05.01.2009 23:50:00,997.51,-12.54,260.81,-15.01,81.60,2.32,1.90,0.43,1.18,1.90,1332.38,1.67,2.25,350.30',\n",
       " '06.01.2009 00:00:00,997.60,-12.63,260.71,-15.07,81.80,2.31,1.89,0.42,1.18,1.89,1332.96,1.60,2.63,355.50',\n",
       " '06.01.2009 00:10:00,997.62,-12.78,260.56,-15.17,82.10,2.28,1.87,0.41,1.17,1.88,1333.76,1.37,2.63,7.26',\n",
       " '06.01.2009 00:20:00,997.61,-12.93,260.41,-15.24,82.60,2.25,1.86,0.39,1.16,1.86,1334.52,1.64,3.00,351.90',\n",
       " '06.01.2009 00:30:00,997.62,-13.06,260.28,-15.33,82.90,2.23,1.85,0.38,1.15,1.85,1335.21,1.08,2.00,342.80',\n",
       " '06.01.2009 00:40:00,997.60,-13.14,260.20,-15.35,83.30,2.21,1.84,0.37,1.15,1.85,1335.60,1.43,2.50,352.10',\n",
       " '06.01.2009 00:50:00,997.62,-13.24,260.10,-15.43,83.40,2.19,1.83,0.36,1.14,1.83,1336.14,1.29,2.38,358.20',\n",
       " '06.01.2009 01:00:00,997.71,-13.34,259.99,-15.46,83.90,2.18,1.83,0.35,1.14,1.83,1336.78,0.91,2.25,8.35',\n",
       " '06.01.2009 01:10:00,997.72,-13.39,259.94,-15.49,84.00,2.17,1.82,0.35,1.14,1.83,1337.06,1.26,3.00,350.20',\n",
       " '06.01.2009 01:20:00,997.72,-13.43,259.90,-15.51,84.20,2.16,1.82,0.34,1.13,1.82,1337.27,1.57,2.88,3.27',\n",
       " '06.01.2009 01:30:00,997.72,-13.49,259.84,-15.54,84.40,2.15,1.81,0.34,1.13,1.82,1337.57,1.38,3.38,339.10',\n",
       " '06.01.2009 01:40:00,997.76,-13.59,259.74,-15.62,84.50,2.13,1.80,0.33,1.12,1.81,1338.15,0.95,2.50,342.20',\n",
       " '06.01.2009 01:50:00,997.85,-13.62,259.70,-15.61,84.80,2.13,1.80,0.32,1.13,1.81,1338.42,2.19,4.25,356.90',\n",
       " '06.01.2009 02:00:00,997.96,-13.64,259.67,-15.63,84.80,2.12,1.80,0.32,1.12,1.80,1338.68,1.09,3.13,340.20',\n",
       " '06.01.2009 02:10:00,998.09,-13.66,259.64,-15.63,84.90,2.12,1.80,0.32,1.12,1.80,1338.96,1.30,2.50,1.11',\n",
       " '06.01.2009 02:20:00,998.10,-13.75,259.55,-15.72,84.90,2.10,1.79,0.32,1.11,1.79,1339.44,1.10,2.13,335.80',\n",
       " '06.01.2009 02:30:00,998.09,-13.80,259.50,-15.74,85.10,2.10,1.78,0.31,1.11,1.79,1339.69,1.45,2.63,352.10',\n",
       " '06.01.2009 02:40:00,998.09,-13.85,259.45,-15.82,84.90,2.09,1.77,0.32,1.11,1.78,1339.95,1.38,2.75,358.10',\n",
       " '06.01.2009 02:50:00,998.16,-13.87,259.43,-15.80,85.20,2.08,1.78,0.31,1.11,1.78,1340.14,1.93,4.25,7.37',\n",
       " '06.01.2009 03:00:00,998.22,-13.89,259.40,-15.80,85.30,2.08,1.77,0.31,1.11,1.78,1340.33,2.16,4.13,353.80',\n",
       " '06.01.2009 03:10:00,998.26,-13.98,259.31,-15.89,85.30,2.06,1.76,0.30,1.10,1.76,1340.85,1.67,3.63,358.70',\n",
       " '06.01.2009 03:20:00,998.25,-14.08,259.21,-15.96,85.50,2.05,1.75,0.30,1.09,1.75,1341.36,1.78,2.50,348.70',\n",
       " '06.01.2009 03:30:00,998.35,-14.18,259.10,-16.03,85.70,2.03,1.74,0.29,1.09,1.74,1342.02,1.41,2.88,354.00',\n",
       " '06.01.2009 03:40:00,998.34,-14.30,258.98,-16.08,86.20,2.01,1.73,0.28,1.08,1.74,1342.63,1.42,2.88,346.80',\n",
       " '06.01.2009 03:50:00,998.34,-14.26,259.02,-15.99,86.60,2.02,1.75,0.27,1.09,1.75,1342.42,1.85,2.38,350.90',\n",
       " '06.01.2009 04:00:00,998.24,-14.29,259.00,-16.03,86.50,2.01,1.74,0.27,1.09,1.74,1342.44,1.83,2.75,355.00',\n",
       " '06.01.2009 04:10:00,998.25,-14.39,258.90,-16.11,86.60,2.00,1.73,0.27,1.08,1.73,1342.99,1.70,2.50,351.50',\n",
       " '06.01.2009 04:20:00,998.34,-14.47,258.81,-16.17,86.80,1.98,1.72,0.26,1.07,1.72,1343.52,1.40,2.38,355.40',\n",
       " '06.01.2009 04:30:00,998.17,-14.60,258.70,-16.28,86.90,1.96,1.70,0.26,1.06,1.71,1343.98,1.25,2.25,342.30',\n",
       " '06.01.2009 04:40:00,998.10,-14.73,258.57,-16.41,86.90,1.94,1.69,0.25,1.05,1.69,1344.57,1.24,2.50,11.40',\n",
       " '06.01.2009 04:50:00,998.19,-14.90,258.39,-16.53,87.20,1.91,1.67,0.24,1.04,1.67,1345.58,0.91,2.13,14.31',\n",
       " '06.01.2009 05:00:00,998.14,-15.13,258.17,-16.75,87.30,1.88,1.64,0.24,1.02,1.64,1346.73,1.08,2.50,13.16',\n",
       " '06.01.2009 05:10:00,998.22,-15.20,258.09,-16.69,88.20,1.87,1.65,0.22,1.03,1.65,1347.19,2.40,3.50,4.70',\n",
       " '06.01.2009 05:20:00,998.24,-15.16,258.13,-16.62,88.50,1.87,1.66,0.22,1.03,1.66,1347.01,2.16,3.13,1.04',\n",
       " '06.01.2009 05:30:00,998.25,-15.25,258.04,-16.66,88.80,1.86,1.65,0.21,1.03,1.65,1347.50,1.74,3.00,2.45',\n",
       " '06.01.2009 05:40:00,998.22,-15.31,257.98,-16.70,89.00,1.85,1.65,0.20,1.03,1.65,1347.78,1.59,2.63,6.51',\n",
       " '06.01.2009 05:50:00,998.21,-15.29,258.00,-16.65,89.20,1.85,1.65,0.20,1.03,1.66,1347.66,1.61,2.88,16.12',\n",
       " '06.01.2009 06:00:00,998.22,-15.37,257.92,-16.73,89.20,1.84,1.64,0.20,1.02,1.64,1348.09,0.76,2.00,47.73',\n",
       " '06.01.2009 06:10:00,998.22,-15.49,257.80,-16.87,89.00,1.82,1.62,0.20,1.01,1.62,1348.74,0.86,2.38,24.59',\n",
       " '06.01.2009 06:20:00,998.26,-15.73,257.56,-17.18,88.50,1.79,1.58,0.21,0.99,1.58,1350.06,0.57,1.25,56.56',\n",
       " '06.01.2009 06:30:00,998.33,-16.31,256.97,-17.83,87.90,1.70,1.49,0.21,0.93,1.50,1353.26,0.49,1.38,48.85',\n",
       " '06.01.2009 06:40:00,998.37,-16.69,256.59,-18.23,87.70,1.65,1.44,0.20,0.90,1.45,1355.33,0.37,1.00,58.44',\n",
       " '06.01.2009 06:50:00,998.37,-16.71,256.57,-18.18,88.20,1.64,1.45,0.19,0.90,1.45,1355.44,0.39,1.50,271.20',\n",
       " '06.01.2009 07:00:00,998.34,-16.86,256.42,-18.39,87.80,1.62,1.43,0.20,0.89,1.43,1356.20,0.33,0.75,173.90',\n",
       " '06.01.2009 07:10:00,998.34,-16.92,256.36,-18.47,87.60,1.61,1.41,0.20,0.88,1.42,1356.53,0.34,1.00,172.70',\n",
       " '06.01.2009 07:20:00,998.34,-16.90,256.38,-18.44,87.70,1.62,1.42,0.20,0.88,1.42,1356.42,0.25,0.75,151.80',\n",
       " '06.01.2009 07:30:00,998.34,-16.89,256.39,-18.45,87.50,1.62,1.42,0.20,0.88,1.42,1356.37,0.42,0.88,202.10',\n",
       " '06.01.2009 07:40:00,998.38,-16.97,256.31,-18.57,87.20,1.61,1.40,0.21,0.87,1.40,1356.85,0.71,1.00,219.30',\n",
       " '06.01.2009 07:50:00,998.43,-17.02,256.26,-18.61,87.30,1.60,1.40,0.20,0.87,1.40,1357.19,0.62,1.00,223.30',\n",
       " '06.01.2009 08:00:00,998.34,-16.92,256.36,-18.40,88.10,1.61,1.42,0.19,0.89,1.43,1356.53,1.01,1.75,3.41',\n",
       " '06.01.2009 08:10:00,998.35,-16.26,257.02,-17.60,89.30,1.71,1.52,0.18,0.95,1.53,1353.00,1.27,1.75,350.60',\n",
       " '06.01.2009 08:20:00,998.37,-16.09,257.19,-17.52,88.60,1.73,1.53,0.20,0.96,1.54,1352.13,1.15,1.75,350.70',\n",
       " '06.01.2009 08:30:00,998.46,-16.02,257.25,-17.49,88.30,1.74,1.54,0.20,0.96,1.54,1351.88,0.79,1.63,16.38',\n",
       " '06.01.2009 08:40:00,998.46,-15.98,257.29,-17.49,88.00,1.75,1.54,0.21,0.96,1.54,1351.67,0.45,0.88,9.12',\n",
       " '06.01.2009 08:50:00,998.47,-15.90,257.37,-17.44,87.80,1.76,1.55,0.21,0.96,1.55,1351.25,0.50,0.88,18.19',\n",
       " '06.01.2009 09:00:00,998.35,-15.71,257.57,-17.25,87.80,1.79,1.57,0.22,0.98,1.57,1350.08,0.44,1.00,192.00',\n",
       " '06.01.2009 09:10:00,998.44,-15.69,257.59,-17.37,86.80,1.79,1.55,0.24,0.97,1.56,1350.11,0.58,1.00,224.10',\n",
       " '06.01.2009 09:20:00,998.46,-15.53,257.74,-17.20,86.90,1.82,1.58,0.24,0.98,1.58,1349.29,0.51,1.00,192.10',\n",
       " '06.01.2009 09:30:00,998.37,-15.07,258.21,-16.81,86.40,1.89,1.63,0.26,1.02,1.63,1346.73,0.77,1.88,14.04',\n",
       " '06.01.2009 09:40:00,998.34,-15.25,258.03,-17.06,85.90,1.86,1.60,0.26,1.00,1.60,1347.65,2.01,3.13,18.19',\n",
       " '06.01.2009 09:50:00,998.42,-15.43,257.85,-17.07,87.10,1.83,1.59,0.24,0.99,1.60,1348.70,1.57,2.50,27.66',\n",
       " '06.01.2009 10:00:00,998.46,-15.23,258.04,-16.85,87.30,1.86,1.63,0.24,1.01,1.63,1347.70,1.63,2.38,24.18',\n",
       " '06.01.2009 10:10:00,998.49,-15.11,258.16,-16.78,86.90,1.88,1.63,0.25,1.02,1.64,1347.10,1.16,2.13,26.08',\n",
       " '06.01.2009 10:20:00,998.46,-15.12,258.15,-16.85,86.50,1.88,1.63,0.25,1.01,1.63,1347.12,1.73,2.88,7.15',\n",
       " '06.01.2009 10:30:00,998.46,-15.06,258.21,-16.87,85.90,1.89,1.62,0.27,1.01,1.62,1346.81,1.71,3.00,19.62',\n",
       " '06.01.2009 10:40:00,998.49,-15.04,258.23,-16.96,85.10,1.89,1.61,0.28,1.00,1.61,1346.75,1.72,2.63,355.10',\n",
       " '06.01.2009 10:50:00,998.44,-15.14,258.14,-17.11,84.70,1.88,1.59,0.29,0.99,1.59,1347.22,1.84,2.88,13.03',\n",
       " '06.01.2009 11:00:00,998.29,-15.05,258.24,-17.04,84.60,1.89,1.60,0.29,1.00,1.60,1346.54,1.67,2.38,19.66',\n",
       " '06.01.2009 11:10:00,998.17,-14.71,258.59,-16.83,83.70,1.94,1.63,0.32,1.01,1.63,1344.59,2.09,3.00,20.25',\n",
       " '06.01.2009 11:20:00,998.21,-14.75,258.54,-16.98,82.90,1.94,1.61,0.33,1.00,1.61,1344.86,2.57,3.50,24.40',\n",
       " '06.01.2009 11:30:00,998.11,-14.72,258.58,-17.03,82.40,1.94,1.60,0.34,1.00,1.60,1344.57,3.04,3.75,27.37',\n",
       " '06.01.2009 11:40:00,998.03,-14.62,258.69,-16.91,82.50,1.96,1.62,0.34,1.01,1.62,1343.94,2.44,3.25,25.76',\n",
       " '06.01.2009 11:50:00,997.92,-14.46,258.85,-16.87,81.70,1.98,1.62,0.36,1.01,1.62,1342.96,2.81,4.00,30.30',\n",
       " '06.01.2009 12:00:00,997.79,-14.52,258.80,-17.02,81.10,1.97,1.60,0.37,1.00,1.61,1343.10,3.42,4.50,23.73',\n",
       " '06.01.2009 12:10:00,997.73,-14.46,258.87,-16.97,81.00,1.98,1.61,0.38,1.00,1.61,1342.70,2.15,3.13,15.21',\n",
       " '06.01.2009 12:20:00,997.60,-14.08,259.26,-16.72,80.20,2.05,1.64,0.41,1.02,1.65,1340.54,1.06,1.63,31.38',\n",
       " '06.01.2009 12:30:00,997.46,-13.56,259.79,-16.47,78.50,2.14,1.68,0.46,1.05,1.68,1337.65,1.43,2.00,13.46',\n",
       " '06.01.2009 12:40:00,997.24,-13.18,260.19,-16.36,76.80,2.21,1.69,0.51,1.06,1.70,1335.39,1.13,1.63,30.07',\n",
       " '06.01.2009 12:50:00,997.14,-12.42,260.95,-15.84,75.40,2.35,1.77,0.58,1.10,1.77,1331.33,0.58,1.13,320.10',\n",
       " '06.01.2009 13:00:00,997.02,-10.97,262.41,-15.17,70.90,2.64,1.87,0.77,1.17,1.88,1323.75,0.99,1.75,148.90',\n",
       " '06.01.2009 13:10:00,996.89,-10.88,262.51,-15.71,67.31,2.66,1.79,0.87,1.12,1.79,1323.17,0.94,1.63,31.63',\n",
       " '06.01.2009 13:20:00,996.80,-11.45,261.95,-16.02,68.63,2.54,1.74,0.80,1.09,1.75,1325.96,0.95,1.75,5.07',\n",
       " '06.01.2009 13:30:00,996.63,-11.09,262.32,-15.74,68.25,2.61,1.78,0.83,1.11,1.79,1323.90,1.10,2.00,19.56',\n",
       " '06.01.2009 13:40:00,996.52,-11.07,262.35,-15.77,68.00,2.62,1.78,0.84,1.11,1.79,1323.65,1.10,2.00,30.78',\n",
       " '06.01.2009 13:50:00,996.43,-11.33,262.10,-15.85,68.97,2.56,1.77,0.80,1.10,1.77,1324.84,1.67,2.50,29.80',\n",
       " '06.01.2009 14:00:00,996.32,-11.90,261.54,-16.10,70.70,2.45,1.73,0.72,1.08,1.74,1327.61,1.75,3.13,34.70',\n",
       " '06.01.2009 14:10:00,996.37,-12.43,261.00,-16.28,72.70,2.34,1.70,0.64,1.06,1.71,1330.39,1.74,2.38,25.81',\n",
       " '06.01.2009 14:20:00,996.37,-12.47,260.96,-16.13,73.90,2.34,1.73,0.61,1.08,1.73,1330.58,1.61,2.50,26.04',\n",
       " '06.01.2009 14:30:00,996.31,-12.27,261.17,-15.92,74.00,2.38,1.76,0.62,1.10,1.76,1329.47,1.90,3.00,32.44',\n",
       " '06.01.2009 14:40:00,996.36,-12.19,261.24,-15.90,73.60,2.39,1.76,0.63,1.10,1.77,1329.12,1.87,3.00,16.61',\n",
       " '06.01.2009 14:50:00,996.33,-11.96,261.47,-15.75,73.20,2.44,1.78,0.65,1.11,1.79,1327.90,1.62,2.50,6.49',\n",
       " '06.01.2009 15:00:00,996.27,-12.05,261.39,-15.83,73.20,2.42,1.77,0.65,1.11,1.78,1328.29,1.87,2.38,18.27',\n",
       " '06.01.2009 15:10:00,996.26,-12.66,260.78,-16.09,75.30,2.30,1.73,0.57,1.08,1.74,1331.39,2.05,2.63,20.35',\n",
       " '06.01.2009 15:20:00,996.36,-13.09,260.34,-16.18,77.40,2.22,1.72,0.50,1.07,1.73,1333.74,1.65,2.50,25.66',\n",
       " '06.01.2009 15:30:00,996.38,-13.26,260.17,-16.16,78.60,2.19,1.72,0.47,1.08,1.73,1334.64,1.47,2.00,11.95',\n",
       " '06.01.2009 15:40:00,996.31,-13.28,260.16,-16.16,78.70,2.19,1.72,0.47,1.08,1.73,1334.64,1.10,2.50,9.58',\n",
       " '06.01.2009 15:50:00,996.25,-13.28,260.16,-16.19,78.50,2.19,1.72,0.47,1.07,1.72,1334.57,0.68,1.63,5.66',\n",
       " '06.01.2009 16:00:00,996.25,-13.52,259.92,-16.37,78.90,2.14,1.69,0.45,1.06,1.70,1335.82,0.99,1.38,340.80',\n",
       " '06.01.2009 16:10:00,996.25,-14.05,259.39,-16.65,80.50,2.05,1.65,0.40,1.03,1.66,1338.57,1.04,1.38,324.40',\n",
       " '06.01.2009 16:20:00,996.22,-14.40,259.04,-16.76,82.10,1.99,1.64,0.36,1.02,1.64,1340.35,0.95,1.50,341.10',\n",
       " '06.01.2009 16:30:00,996.25,-14.62,258.82,-16.83,83.10,1.96,1.63,0.33,1.02,1.63,1341.54,0.33,0.75,47.89',\n",
       " '06.01.2009 16:40:00,996.24,-14.94,258.50,-17.10,83.40,1.91,1.59,0.32,0.99,1.60,1343.20,0.98,1.50,350.90',\n",
       " '06.01.2009 16:50:00,996.19,-14.88,258.56,-16.79,85.20,1.92,1.63,0.28,1.02,1.64,1342.81,1.27,1.50,335.60',\n",
       " '06.01.2009 17:00:00,996.14,-14.88,258.57,-16.79,85.20,1.92,1.63,0.28,1.02,1.64,1342.73,0.63,1.25,298.10',\n",
       " '06.01.2009 17:10:00,996.07,-15.59,257.86,-17.69,83.70,1.81,1.51,0.29,0.94,1.52,1346.40,0.97,2.25,245.50',\n",
       " '06.01.2009 17:20:00,996.01,-15.97,257.48,-17.92,84.80,1.75,1.48,0.27,0.93,1.49,1348.33,0.68,1.13,8.72',\n",
       " '06.01.2009 17:30:00,996.08,-15.92,257.53,-17.69,86.10,1.76,1.51,0.24,0.95,1.52,1348.15,0.50,0.88,47.57',\n",
       " '06.01.2009 17:40:00,995.97,-16.15,257.31,-17.91,86.10,1.72,1.48,0.24,0.93,1.49,1349.21,1.25,2.38,264.10',\n",
       " '06.01.2009 17:50:00,995.95,-16.11,257.35,-17.82,86.50,1.73,1.50,0.23,0.93,1.50,1348.97,0.97,2.25,300.30',\n",
       " '06.01.2009 18:00:00,995.91,-16.49,256.97,-18.34,85.40,1.67,1.43,0.24,0.89,1.44,1350.95,2.72,3.38,249.70',\n",
       " '06.01.2009 18:10:00,995.93,-16.56,256.90,-18.36,85.80,1.66,1.43,0.24,0.89,1.43,1351.34,1.54,2.50,227.50',\n",
       " '06.01.2009 18:20:00,995.88,-16.61,256.85,-18.41,85.80,1.66,1.42,0.24,0.89,1.43,1351.55,1.96,2.50,250.70',\n",
       " '06.01.2009 18:30:00,995.88,-16.75,256.71,-18.61,85.30,1.64,1.40,0.24,0.87,1.40,1352.29,1.74,2.25,235.50',\n",
       " '06.01.2009 18:40:00,995.86,-16.93,256.53,-18.84,84.90,1.61,1.37,0.24,0.86,1.38,1353.23,1.72,2.38,231.30',\n",
       " '06.01.2009 18:50:00,995.79,-17.07,256.40,-18.94,85.20,1.59,1.36,0.24,0.85,1.36,1353.89,1.65,2.13,229.40',\n",
       " '06.01.2009 19:00:00,995.74,-17.21,256.26,-19.01,85.70,1.58,1.35,0.23,0.84,1.36,1354.55,0.90,1.88,225.30',\n",
       " '06.01.2009 19:10:00,995.64,-17.40,256.08,-19.20,85.70,1.55,1.33,0.22,0.83,1.33,1355.44,1.44,2.00,227.70',\n",
       " '06.01.2009 19:20:00,995.59,-17.58,255.90,-19.40,85.50,1.53,1.31,0.22,0.82,1.31,1356.34,1.40,2.00,228.20',\n",
       " '06.01.2009 19:30:00,995.51,-17.79,255.70,-19.61,85.50,1.50,1.28,0.22,0.80,1.29,1357.36,1.54,1.88,224.10',\n",
       " '06.01.2009 19:40:00,995.48,-17.99,255.50,-19.82,85.40,1.47,1.26,0.22,0.79,1.26,1358.39,1.60,2.00,211.20',\n",
       " '06.01.2009 19:50:00,995.50,-18.18,255.31,-20.02,85.30,1.45,1.24,0.21,0.77,1.24,1359.44,1.32,2.38,234.10',\n",
       " '06.01.2009 20:00:00,995.54,-18.35,255.14,-20.16,85.50,1.43,1.22,0.21,0.76,1.23,1360.41,0.62,1.00,224.90',\n",
       " '06.01.2009 20:10:00,995.52,-18.32,255.17,-19.96,86.80,1.43,1.24,0.19,0.78,1.25,1360.21,0.29,0.75,42.02',\n",
       " '06.01.2009 20:20:00,995.48,-17.83,255.66,-19.43,87.10,1.49,1.30,0.19,0.81,1.31,1357.52,0.52,0.88,140.20',\n",
       " '06.01.2009 20:30:00,995.47,-18.25,255.24,-20.10,85.20,1.44,1.23,0.21,0.77,1.23,1359.78,0.43,0.88,236.40',\n",
       " '06.01.2009 20:40:00,995.41,-18.37,255.13,-20.10,86.10,1.43,1.23,0.20,0.77,1.23,1360.34,0.23,0.50,71.50',\n",
       " '06.01.2009 20:50:00,995.39,-18.50,255.00,-20.24,86.00,1.41,1.21,0.20,0.76,1.22,1361.01,0.20,0.63,93.60',\n",
       " '06.01.2009 21:00:00,995.50,-18.62,254.87,-20.34,86.10,1.40,1.20,0.19,0.75,1.21,1361.81,0.35,1.00,186.90',\n",
       " '06.01.2009 21:10:00,995.55,-18.88,254.60,-20.64,85.80,1.37,1.17,0.19,0.73,1.18,1363.29,0.61,1.13,226.30',\n",
       " '06.01.2009 21:20:00,995.52,-18.89,254.60,-20.62,86.00,1.36,1.17,0.19,0.73,1.18,1363.29,0.55,1.25,59.17',\n",
       " '06.01.2009 21:30:00,995.40,-19.17,254.33,-21.07,84.70,1.33,1.13,0.20,0.71,1.13,1364.65,1.65,2.88,223.20',\n",
       " '06.01.2009 21:40:00,995.28,-19.47,254.03,-21.38,84.60,1.30,1.10,0.20,0.69,1.10,1366.13,1.07,1.75,220.60',\n",
       " '06.01.2009 21:50:00,995.26,-19.38,254.13,-21.23,85.10,1.31,1.11,0.19,0.70,1.12,1365.60,0.25,0.88,223.30',\n",
       " '06.01.2009 22:00:00,995.23,-19.39,254.12,-21.34,84.30,1.31,1.10,0.21,0.69,1.11,1365.62,0.60,2.00,205.80',\n",
       " '06.01.2009 22:10:00,995.07,-19.88,253.64,-21.93,83.50,1.25,1.05,0.21,0.65,1.05,1368.07,1.37,2.25,204.10',\n",
       " '06.01.2009 22:20:00,994.95,-20.34,253.19,-22.35,83.70,1.20,1.01,0.20,0.63,1.01,1370.41,0.43,0.88,193.90',\n",
       " '06.01.2009 22:30:00,994.88,-20.67,252.86,-22.59,84.40,1.17,0.99,0.18,0.62,0.99,1372.12,1.01,1.50,165.70',\n",
       " '06.01.2009 22:40:00,994.81,-20.84,252.70,-22.63,85.30,1.15,0.98,0.17,0.61,0.99,1372.95,0.68,1.38,188.70',\n",
       " '06.01.2009 22:50:00,994.76,-20.86,252.68,-22.61,85.60,1.15,0.98,0.17,0.62,0.99,1373.00,0.42,0.88,208.50',\n",
       " '06.01.2009 23:00:00,994.67,-20.85,252.70,-22.62,85.50,1.15,0.98,0.17,0.62,0.99,1372.81,0.69,1.13,184.80',\n",
       " '06.01.2009 23:10:00,994.74,-21.08,252.46,-22.90,85.10,1.13,0.96,0.17,0.60,0.96,1374.18,0.32,0.75,204.60',\n",
       " '06.01.2009 23:20:00,994.66,-21.07,252.48,-22.83,85.50,1.13,0.96,0.16,0.60,0.97,1374.01,0.69,1.13,145.10',\n",
       " '06.01.2009 23:30:00,994.60,-20.51,253.04,-22.14,86.60,1.19,1.03,0.16,0.64,1.03,1370.84,0.58,1.50,158.20',\n",
       " '06.01.2009 23:40:00,994.65,-20.48,253.07,-22.28,85.30,1.19,1.01,0.17,0.63,1.02,1370.77,0.73,1.25,177.60',\n",
       " '06.01.2009 23:50:00,994.55,-20.75,252.80,-22.61,84.80,1.16,0.98,0.18,0.62,0.99,1372.11,0.76,1.50,188.70',\n",
       " '07.01.2009 00:00:00,994.51,-21.09,252.47,-23.01,84.30,1.13,0.95,0.18,0.59,0.95,1373.91,0.48,1.00,168.40',\n",
       " '07.01.2009 00:10:00,994.43,-21.31,252.25,-23.23,84.30,1.10,0.93,0.17,0.58,0.94,1375.02,0.39,0.88,205.60',\n",
       " '07.01.2009 00:20:00,994.29,-20.97,252.60,-22.74,85.50,1.14,0.97,0.17,0.61,0.98,1372.94,0.56,1.00,211.60',\n",
       " '07.01.2009 00:30:00,994.17,-20.48,253.10,-22.18,86.10,1.19,1.02,0.17,0.64,1.03,1370.09,0.56,1.13,179.40',\n",
       " '07.01.2009 00:40:00,994.16,-20.26,253.32,-22.01,85.70,1.21,1.04,0.17,0.65,1.04,1368.88,0.53,1.00,159.20',\n",
       " '07.01.2009 00:50:00,994.09,-21.17,252.42,-23.22,83.30,1.12,0.93,0.19,0.58,0.94,1373.78,1.10,1.50,210.20',\n",
       " '07.01.2009 01:00:00,994.14,-21.56,252.02,-23.54,83.80,1.08,0.91,0.18,0.57,0.91,1376.00,0.68,1.25,173.30',\n",
       " '07.01.2009 01:10:00,994.12,-21.65,251.93,-23.60,84.00,1.07,0.90,0.17,0.56,0.91,1376.46,0.66,1.13,203.50',\n",
       " '07.01.2009 01:20:00,994.03,-21.49,252.10,-23.38,84.50,1.09,0.92,0.17,0.58,0.92,1375.46,0.81,1.25,204.20',\n",
       " '07.01.2009 01:30:00,994.11,-21.46,252.13,-23.41,84.00,1.09,0.92,0.17,0.57,0.92,1375.40,0.34,0.88,181.90',\n",
       " '07.01.2009 01:40:00,994.06,-21.57,252.02,-23.52,84.00,1.08,0.91,0.17,0.57,0.91,1375.94,0.24,0.63,177.80',\n",
       " '07.01.2009 01:50:00,993.96,-21.24,252.36,-22.99,85.60,1.11,0.95,0.16,0.60,0.96,1373.98,0.24,1.00,335.70',\n",
       " '07.01.2009 02:00:00,993.87,-20.99,252.61,-22.73,85.70,1.14,0.97,0.16,0.61,0.98,1372.47,0.38,1.13,196.90',\n",
       " '07.01.2009 02:10:00,993.90,-20.94,252.66,-22.69,85.60,1.14,0.98,0.16,0.61,0.98,1372.25,0.36,0.75,213.20',\n",
       " '07.01.2009 02:20:00,993.97,-20.90,252.70,-22.67,85.50,1.15,0.98,0.17,0.61,0.99,1372.12,0.50,0.75,166.40',\n",
       " '07.01.2009 02:30:00,993.87,-21.32,252.28,-23.33,83.60,1.10,0.92,0.18,0.58,0.93,1374.30,0.74,1.50,184.60',\n",
       " '07.01.2009 02:40:00,993.84,-22.07,251.53,-24.17,82.80,1.03,0.86,0.18,0.54,0.86,1378.39,0.77,1.38,159.60',\n",
       " '07.01.2009 02:50:00,993.77,-22.55,251.06,-24.63,82.90,0.99,0.82,0.17,0.51,0.83,1380.96,1.16,2.25,184.50',\n",
       " '07.01.2009 03:00:00,993.75,-22.76,250.85,-24.80,83.20,0.97,0.81,0.16,0.51,0.81,1382.10,1.83,2.75,154.10',\n",
       " '07.01.2009 03:10:00,993.67,-23.01,250.60,-25.01,83.40,0.95,0.79,0.16,0.50,0.80,1383.38,0.80,1.50,137.60',\n",
       " '07.01.2009 03:20:00,993.67,-22.91,250.71,-24.85,83.90,0.96,0.80,0.15,0.50,0.81,1382.81,0.61,1.25,106.80',\n",
       " '07.01.2009 03:30:00,993.56,-22.64,250.98,-24.49,84.60,0.98,0.83,0.15,0.52,0.84,1381.15,0.96,2.13,114.80',\n",
       " '07.01.2009 03:40:00,993.40,-22.38,251.26,-24.30,84.10,1.00,0.84,0.16,0.53,0.85,1379.49,0.77,1.75,172.00',\n",
       " '07.01.2009 03:50:00,993.33,-22.22,251.42,-24.16,84.00,1.02,0.86,0.16,0.54,0.86,1378.51,0.81,1.38,205.50',\n",
       " '07.01.2009 04:00:00,993.36,-22.39,251.25,-24.39,83.50,1.00,0.84,0.17,0.52,0.84,1379.50,1.10,2.13,181.60',\n",
       " '07.01.2009 04:10:00,993.28,-22.10,251.54,-24.04,84.00,1.03,0.87,0.16,0.54,0.87,1377.78,1.34,2.63,177.40',\n",
       " '07.01.2009 04:20:00,993.14,-21.80,251.86,-23.73,84.10,1.06,0.89,0.17,0.56,0.90,1375.93,0.94,1.75,150.10',\n",
       " '07.01.2009 04:30:00,993.01,-22.25,251.41,-24.36,82.70,1.02,0.84,0.18,0.53,0.85,1378.25,1.35,1.63,196.40',\n",
       " '07.01.2009 04:40:00,993.07,-22.49,251.17,-24.50,83.40,0.99,0.83,0.17,0.52,0.84,1379.65,0.71,1.75,192.70',\n",
       " '07.01.2009 04:50:00,992.90,-22.31,251.36,-24.34,83.30,1.01,0.84,0.17,0.53,0.85,1378.42,0.84,1.88,188.80',\n",
       " '07.01.2009 05:00:00,992.91,-22.42,251.25,-24.46,83.20,1.00,0.83,0.17,0.52,0.84,1379.04,0.70,1.50,81.70',\n",
       " '07.01.2009 05:10:00,992.78,-22.31,251.37,-24.33,83.40,1.01,0.84,0.17,0.53,0.85,1378.25,0.79,1.63,188.00',\n",
       " '07.01.2009 05:20:00,992.69,-22.47,251.22,-24.52,83.10,1.00,0.83,0.17,0.52,0.83,1379.02,0.70,1.25,174.40',\n",
       " '07.01.2009 05:30:00,992.65,-22.26,251.43,-24.20,84.00,1.02,0.85,0.16,0.53,0.86,1377.79,0.68,1.38,140.40',\n",
       " '07.01.2009 05:40:00,992.49,-21.93,251.77,-23.93,83.60,1.05,0.87,0.17,0.55,0.88,1375.75,1.21,1.88,182.80',\n",
       " '07.01.2009 05:50:00,992.54,-22.31,251.39,-24.37,83.10,1.01,0.84,0.17,0.53,0.85,1377.92,0.48,1.63,149.90',\n",
       " '07.01.2009 06:00:00,992.37,-22.13,251.58,-24.14,83.50,1.03,0.86,0.17,0.54,0.86,1376.69,0.81,2.00,180.90',\n",
       " '07.01.2009 06:10:00,992.28,-22.49,251.23,-24.61,82.60,0.99,0.82,0.17,0.52,0.83,1378.56,0.78,1.38,187.10',\n",
       " '07.01.2009 06:20:00,992.28,-22.41,251.31,-24.41,83.50,1.00,0.84,0.17,0.52,0.84,1378.11,0.45,0.88,142.70',\n",
       " '07.01.2009 06:30:00,992.38,-22.41,251.30,-24.47,83.10,1.00,0.83,0.17,0.52,0.84,1378.25,0.48,1.13,230.20',\n",
       " '07.01.2009 06:40:00,992.39,-21.93,251.78,-23.81,84.50,1.05,0.88,0.16,0.55,0.89,1375.61,0.54,1.63,145.20',\n",
       " '07.01.2009 06:50:00,992.18,-21.46,252.27,-23.35,84.50,1.09,0.92,0.17,0.58,0.93,1372.73,0.87,2.00,167.60',\n",
       " '07.01.2009 07:00:00,992.10,-21.42,252.31,-23.41,83.70,1.09,0.92,0.18,0.57,0.92,1372.40,0.83,1.13,191.30',\n",
       " '07.01.2009 07:10:00,992.06,-21.84,251.89,-23.93,82.90,1.05,0.87,0.18,0.55,0.88,1374.66,1.11,1.63,201.40',\n",
       " '07.01.2009 07:20:00,992.11,-22.20,251.53,-24.27,83.00,1.02,0.85,0.17,0.53,0.85,1376.72,0.88,1.50,184.70',\n",
       " '07.01.2009 07:30:00,992.13,-22.30,251.43,-24.37,83.00,1.01,0.84,0.17,0.53,0.85,1377.30,0.66,1.38,130.60',\n",
       " '07.01.2009 07:40:00,992.24,-22.55,251.17,-24.66,82.70,0.99,0.82,0.17,0.51,0.82,1378.84,0.67,1.13,173.90',\n",
       " '07.01.2009 07:50:00,992.25,-22.63,251.09,-24.71,82.90,0.98,0.81,0.17,0.51,0.82,1379.29,0.35,0.75,196.30',\n",
       " '07.01.2009 08:00:00,992.18,-22.50,251.22,-24.55,83.10,0.99,0.83,0.17,0.52,0.83,1378.47,0.67,1.25,182.50',\n",
       " '07.01.2009 08:10:00,992.23,-22.54,251.18,-24.58,83.20,0.99,0.82,0.17,0.52,0.83,1378.76,0.54,1.38,204.60',\n",
       " '07.01.2009 08:20:00,992.42,-21.68,252.03,-23.54,84.70,1.07,0.91,0.16,0.57,0.91,1374.27,0.73,1.50,138.90',\n",
       " '07.01.2009 08:30:00,992.50,-21.43,252.27,-23.45,83.50,1.09,0.91,0.18,0.57,0.92,1373.01,0.69,1.50,47.72',\n",
       " '07.01.2009 08:40:00,992.49,-21.17,252.53,-23.21,83.40,1.12,0.93,0.19,0.58,0.94,1371.57,0.47,1.13,180.90',\n",
       " '07.01.2009 08:50:00,992.32,-21.16,252.56,-23.27,82.90,1.12,0.93,0.19,0.58,0.94,1371.28,0.63,1.00,197.90',\n",
       " '07.01.2009 09:00:00,992.14,-21.17,252.56,-23.29,82.80,1.12,0.93,0.19,0.58,0.93,1371.09,0.79,1.38,198.20',\n",
       " '07.01.2009 09:10:00,992.14,-20.85,252.88,-22.94,83.10,1.15,0.96,0.19,0.60,0.96,1369.34,0.82,1.63,181.20',\n",
       " '07.01.2009 09:20:00,992.17,-21.35,252.38,-23.67,81.30,1.10,0.89,0.21,0.56,0.90,1372.13,1.66,2.25,141.40',\n",
       " '07.01.2009 09:30:00,992.38,-22.14,251.57,-24.40,81.60,1.03,0.84,0.19,0.53,0.84,1376.77,0.97,2.00,138.20',\n",
       " '07.01.2009 09:40:00,992.37,-22.10,251.61,-24.24,82.50,1.03,0.85,0.18,0.53,0.86,1376.53,1.41,2.00,195.90',\n",
       " '07.01.2009 09:50:00,992.40,-21.44,252.27,-23.47,83.40,1.09,0.91,0.18,0.57,0.92,1372.93,0.89,2.00,200.40',\n",
       " '07.01.2009 10:00:00,992.45,-20.34,253.37,-22.44,83.10,1.20,1.00,0.20,0.63,1.01,1366.98,0.79,1.38,211.90',\n",
       " '07.01.2009 10:10:00,992.35,-19.55,254.17,-21.63,83.30,1.29,1.07,0.22,0.67,1.08,1362.54,1.19,2.25,208.40',\n",
       " '07.01.2009 10:20:00,992.42,-18.44,255.27,-20.67,82.40,1.42,1.17,0.25,0.73,1.18,1356.65,1.15,2.25,177.30',\n",
       " '07.01.2009 10:30:00,992.44,-18.77,254.94,-21.13,81.40,1.38,1.12,0.26,0.70,1.13,1358.46,0.70,1.63,172.70',\n",
       " '07.01.2009 10:40:00,992.52,-18.87,254.84,-21.31,80.80,1.37,1.10,0.26,0.69,1.11,1359.12,1.01,1.88,181.90',\n",
       " '07.01.2009 10:50:00,992.47,-19.04,254.67,-21.44,81.10,1.35,1.09,0.25,0.68,1.10,1359.96,0.89,1.75,189.70',\n",
       " '07.01.2009 11:00:00,992.41,-18.71,255.01,-21.02,81.80,1.39,1.13,0.25,0.71,1.14,1358.10,0.73,1.50,216.90',\n",
       " '07.01.2009 11:10:00,992.32,-18.64,255.08,-20.95,81.80,1.39,1.14,0.25,0.72,1.15,1357.60,1.05,2.00,172.10',\n",
       " '07.01.2009 11:20:00,992.30,-18.05,255.67,-20.16,83.30,1.47,1.22,0.24,0.77,1.23,1354.39,0.93,2.50,106.00',\n",
       " '07.01.2009 11:30:00,992.14,-17.65,256.09,-19.74,83.50,1.52,1.27,0.25,0.79,1.28,1352.03,1.19,2.50,162.50',\n",
       " '07.01.2009 11:40:00,992.02,-17.27,256.48,-19.43,83.10,1.57,1.30,0.26,0.82,1.31,1349.84,1.01,1.50,213.60',\n",
       " '07.01.2009 11:50:00,992.02,-17.12,256.63,-19.35,82.60,1.59,1.31,0.28,0.82,1.32,1349.04,1.00,1.88,185.20',\n",
       " '07.01.2009 12:00:00,992.05,-16.50,257.25,-18.64,83.30,1.67,1.39,0.28,0.87,1.41,1345.78,0.56,1.13,183.30',\n",
       " '07.01.2009 12:10:00,991.76,-15.86,257.91,-18.02,83.30,1.77,1.47,0.29,0.92,1.48,1342.00,0.52,1.38,209.70',\n",
       " '07.01.2009 12:20:00,991.72,-15.58,258.19,-17.74,83.30,1.81,1.51,0.30,0.95,1.52,1340.47,0.67,2.13,124.00',\n",
       " '07.01.2009 12:30:00,991.62,-15.32,258.46,-17.46,83.50,1.85,1.54,0.30,0.97,1.56,1338.96,0.89,1.50,196.40',\n",
       " '07.01.2009 12:40:00,991.71,-15.04,258.74,-17.14,83.80,1.89,1.58,0.31,0.99,1.60,1337.61,1.10,2.25,189.70',\n",
       " '07.01.2009 12:50:00,991.65,-14.86,258.92,-16.95,83.90,1.92,1.61,0.31,1.01,1.62,1336.59,0.88,1.88,228.70',\n",
       " '07.01.2009 13:00:00,991.60,-14.39,259.40,-16.39,84.60,2.00,1.69,0.31,1.06,1.70,1334.05,0.99,1.75,139.90',\n",
       " '07.01.2009 13:10:00,991.56,-13.93,259.86,-15.91,84.80,2.07,1.76,0.32,1.10,1.77,1331.59,0.69,1.75,174.80',\n",
       " '07.01.2009 13:20:00,991.52,-13.08,260.71,-14.99,85.40,2.22,1.90,0.32,1.19,1.92,1327.12,0.74,1.88,90.00',\n",
       " '07.01.2009 13:30:00,991.58,-12.33,261.46,-14.44,84.10,2.36,1.99,0.38,1.25,2.00,1323.34,0.70,1.63,186.30',\n",
       " '07.01.2009 13:40:00,991.62,-12.05,261.74,-14.46,82.10,2.42,1.99,0.43,1.25,2.00,1321.97,0.67,1.25,153.70',\n",
       " '07.01.2009 13:50:00,991.58,-11.96,261.83,-14.43,81.70,2.44,1.99,0.45,1.25,2.01,1321.46,0.71,1.63,144.30',\n",
       " '07.01.2009 14:00:00,991.58,-12.15,261.64,-14.63,81.60,2.40,1.96,0.44,1.23,1.97,1322.44,1.10,2.00,156.80',\n",
       " '07.01.2009 14:10:00,991.50,-12.28,261.52,-14.45,83.70,2.37,1.99,0.39,1.25,2.00,1322.98,1.25,2.75,123.90',\n",
       " '07.01.2009 14:20:00,991.58,-12.06,261.73,-13.98,85.50,2.42,2.07,0.35,1.30,2.08,1321.93,1.79,3.75,138.30',\n",
       " '07.01.2009 14:30:00,991.69,-11.89,261.89,-13.81,85.50,2.45,2.09,0.36,1.31,2.11,1321.20,1.14,2.38,146.70',\n",
       " '07.01.2009 14:40:00,991.75,-11.44,262.34,-13.21,86.60,2.54,2.20,0.34,1.38,2.22,1318.96,0.67,1.63,135.30',\n",
       " '07.01.2009 14:50:00,991.92,-10.84,262.93,-12.61,86.70,2.67,2.31,0.35,1.45,2.33,1316.11,0.75,1.50,150.00',\n",
       " '07.01.2009 15:00:00,991.85,-10.57,263.21,-12.38,86.40,2.72,2.35,0.37,1.48,2.37,1314.64,0.73,1.63,129.20',\n",
       " '07.01.2009 15:10:00,991.93,-10.34,263.43,-12.21,86.00,2.77,2.39,0.39,1.50,2.41,1313.58,0.78,1.88,124.80',\n",
       " '07.01.2009 15:20:00,991.98,-10.19,263.58,-12.11,85.70,2.81,2.41,0.40,1.51,2.43,1312.89,0.56,1.13,147.10',\n",
       " '07.01.2009 15:30:00,992.14,-10.03,263.72,-12.04,85.10,2.84,2.42,0.42,1.52,2.44,1312.29,1.28,2.38,149.80',\n",
       " '07.01.2009 15:40:00,992.19,-10.12,263.63,-11.98,86.10,2.82,2.43,0.39,1.53,2.45,1312.80,1.25,3.38,128.90',\n",
       " '07.01.2009 15:50:00,992.25,-9.84,263.91,-11.47,87.80,2.89,2.54,0.35,1.59,2.55,1311.44,0.82,1.75,158.20',\n",
       " '07.01.2009 16:00:00,992.21,-9.58,264.17,-11.12,88.40,2.95,2.61,0.34,1.64,2.63,1310.05,0.93,2.00,126.20',\n",
       " '07.01.2009 16:10:00,992.34,-9.40,264.34,-10.95,88.40,2.99,2.64,0.35,1.66,2.66,1309.31,0.88,1.75,110.60',\n",
       " '07.01.2009 16:20:00,992.49,-9.44,264.29,-11.00,88.30,2.98,2.63,0.35,1.65,2.65,1309.72,0.70,1.88,198.00',\n",
       " '07.01.2009 16:30:00,992.60,-9.18,264.54,-10.66,88.90,3.04,2.70,0.34,1.70,2.72,1308.53,0.56,1.38,222.90',\n",
       " '07.01.2009 16:40:00,992.59,-8.95,264.77,-10.43,88.90,3.10,2.75,0.34,1.73,2.77,1307.36,0.90,1.63,178.60',\n",
       " '07.01.2009 16:50:00,992.61,-8.65,265.07,-10.14,88.90,3.17,2.82,0.35,1.77,2.84,1305.87,0.51,1.00,159.40',\n",
       " '07.01.2009 17:00:00,992.79,-8.55,265.16,-10.15,88.10,3.20,2.82,0.38,1.77,2.84,1305.61,0.51,1.38,116.20',\n",
       " '07.01.2009 17:10:00,992.95,-8.47,265.23,-10.11,87.80,3.22,2.82,0.39,1.77,2.84,1305.43,0.63,1.25,170.50',\n",
       " '07.01.2009 17:20:00,992.97,-8.46,265.23,-10.12,87.70,3.22,2.82,0.40,1.77,2.84,1305.40,1.07,2.13,182.10',\n",
       " '07.01.2009 17:30:00,993.04,-8.35,265.34,-9.97,88.00,3.25,2.86,0.39,1.79,2.88,1304.94,0.87,2.00,127.60',\n",
       " '07.01.2009 17:40:00,993.25,-8.43,265.24,-10.05,88.00,3.23,2.84,0.39,1.78,2.86,1305.62,0.48,1.13,109.10',\n",
       " '07.01.2009 17:50:00,993.38,-8.26,265.40,-9.82,88.40,3.27,2.89,0.38,1.81,2.91,1304.92,0.47,1.25,169.20',\n",
       " '07.01.2009 18:00:00,993.53,-7.98,265.67,-9.52,88.60,3.34,2.96,0.38,1.86,2.98,1303.71,0.79,1.38,128.70',\n",
       " '07.01.2009 18:10:00,993.73,-8.02,265.62,-9.64,88.00,3.33,2.93,0.40,1.84,2.95,1304.17,0.46,1.00,159.80',\n",
       " '07.01.2009 18:20:00,993.91,-7.67,265.95,-9.30,88.00,3.43,3.01,0.41,1.89,3.03,1302.65,0.47,1.13,135.90',\n",
       " '07.01.2009 18:30:00,994.07,-7.65,265.96,-9.52,86.30,3.43,2.96,0.47,1.85,2.98,1302.80,0.68,1.25,156.90',\n",
       " '07.01.2009 18:40:00,994.13,-7.66,265.95,-9.45,86.90,3.43,2.98,0.45,1.87,3.00,1302.92,0.41,1.13,160.80',\n",
       " '07.01.2009 18:50:00,994.26,-7.53,266.07,-9.32,86.90,3.46,3.01,0.45,1.88,3.03,1302.43,0.54,1.38,110.00',\n",
       " '07.01.2009 19:00:00,994.38,-7.75,265.84,-9.58,86.60,3.40,2.95,0.46,1.85,2.96,1303.69,0.61,1.25,114.90',\n",
       " '07.01.2009 19:10:00,994.40,-7.46,266.13,-9.02,88.50,3.48,3.08,0.40,1.93,3.10,1302.24,0.41,0.88,86.50',\n",
       " '07.01.2009 19:20:00,994.49,-7.45,266.13,-9.17,87.40,3.48,3.05,0.44,1.91,3.06,1302.32,0.45,1.63,99.70',\n",
       " '07.01.2009 19:30:00,994.62,-7.81,265.76,-9.48,87.70,3.39,2.97,0.42,1.86,2.99,1304.30,1.14,1.75,132.10',\n",
       " '07.01.2009 19:40:00,994.89,-7.88,265.67,-9.42,88.60,3.37,2.99,0.38,1.87,3.00,1304.99,0.42,1.00,113.30',\n",
       " '07.01.2009 19:50:00,994.91,-7.92,265.63,-9.44,88.70,3.36,2.98,0.38,1.86,2.99,1305.21,0.58,1.38,74.80',\n",
       " '07.01.2009 20:00:00,995.07,-7.85,265.69,-9.36,88.80,3.38,3.00,0.38,1.88,3.01,1305.07,0.88,1.50,43.83',\n",
       " '07.01.2009 20:10:00,995.23,-8.27,265.25,-9.89,88.00,3.27,2.88,0.39,1.80,2.89,1307.41,0.51,0.88,217.20',\n",
       " '07.01.2009 20:20:00,995.40,-8.56,264.95,-10.01,89.20,3.19,2.85,0.35,1.78,2.86,1309.09,0.75,1.75,151.10',\n",
       " '07.01.2009 20:30:00,995.59,-8.62,264.87,-9.92,90.20,3.18,2.87,0.31,1.79,2.88,1309.62,0.49,1.25,153.80',\n",
       " '07.01.2009 20:40:00,995.76,-8.51,264.97,-9.69,91.10,3.21,2.92,0.29,1.83,2.93,1309.27,0.44,0.88,108.60',\n",
       " '07.01.2009 20:50:00,995.95,-8.50,264.97,-9.68,91.10,3.21,2.92,0.29,1.83,2.94,1309.47,0.39,1.13,120.90',\n",
       " '07.01.2009 21:00:00,996.09,-8.44,265.02,-9.57,91.50,3.22,2.95,0.27,1.84,2.96,1309.35,0.23,0.75,85.20',\n",
       " '07.01.2009 21:10:00,996.32,-8.56,264.88,-9.80,90.70,3.19,2.90,0.30,1.81,2.91,1310.27,0.30,0.63,129.00',\n",
       " '07.01.2009 21:20:00,996.46,-8.89,264.54,-10.18,90.30,3.11,2.81,0.30,1.76,2.82,1312.13,0.13,0.50,173.40',\n",
       " '07.01.2009 21:30:00,996.55,-9.30,264.12,-10.72,89.30,3.01,2.69,0.32,1.68,2.70,1314.35,0.54,1.00,150.60',\n",
       " '07.01.2009 21:40:00,996.83,-10.12,263.28,-11.50,89.50,2.82,2.53,0.30,1.58,2.54,1318.90,0.77,1.25,143.20',\n",
       " '07.01.2009 21:50:00,997.00,-10.07,263.32,-11.14,91.80,2.84,2.60,0.23,1.63,2.61,1318.83,0.50,1.13,218.40',\n",
       " '07.01.2009 22:00:00,997.11,-9.41,263.97,-10.27,93.40,2.99,2.79,0.20,1.74,2.80,1315.59,0.52,1.38,188.10',\n",
       " '07.01.2009 22:10:00,997.28,-9.30,264.07,-10.28,92.50,3.01,2.79,0.23,1.74,2.80,1315.27,0.29,1.13,85.40',\n",
       " '07.01.2009 22:20:00,997.48,-9.89,263.46,-10.96,91.80,2.88,2.64,0.24,1.65,2.65,1318.56,0.29,1.00,119.30',\n",
       " '07.01.2009 22:30:00,997.58,-10.44,262.90,-11.59,91.20,2.75,2.51,0.24,1.57,2.52,1321.51,0.55,1.00,130.50',\n",
       " '07.01.2009 22:40:00,997.70,-10.83,262.50,-11.99,91.10,2.67,2.43,0.24,1.52,2.44,1323.67,0.74,1.25,132.00',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f03962f9e48>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "temp = float_data[:, 1] #<1> temperature (in degrees Celsius)\n",
    "plt.plot(range(len(temp)), temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more narrow plot of the first 10 days of temperature data.\n",
    "Because the data is recorded every 10 minutes, you get 144 data points per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0394249cc0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1440), temp[:1440])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this plot, you can see daily periodicity, especially evident for the last 4 days. Also\n",
    "note that this 10-day period must be coming from a fairly cold winter month.\n",
    "\n",
    "If you were trying to predict average temperature for the next month given a few\n",
    "months of past data, the problem would be easy, due to the reliable year-scale period-\n",
    "icity of the data.\n",
    "\n",
    "The exact formulation of the problem will be as follows: given data going as far back\n",
    "as lookback timesteps (a timestep is 10 minutes) and sampled every steps timesteps,\n",
    "can you predict the temperature in delay timesteps? You’ll use the following parame-\n",
    "ter values:\n",
    "* lookback = 720 —Observations will go back 5 days.\n",
    "* steps = 6 —Observations will be sampled at one data point per hour.\n",
    "* delay = 144 —Targets will be 24 hours in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, you need to do two things:\n",
    "    \n",
    "* Preprocess the data to a format a neural network can ingest. This is easy: the\n",
    "data is already numerical, so you don’t need to do any vectorization. But each\n",
    "timeseries in the data is on a different scale (for example, temperature is typi-\n",
    "cally between -20 and +30, but atmospheric pressure, measured in mbar, is\n",
    "around 1,000). You’ll normalize each timeseries independently so that they all\n",
    "take small values on a similar scale.\n",
    "* Write a Python generator that takes the current array of float data and yields\n",
    "batches of data from the recent past, along with a target temperature in the\n",
    "future. Because the samples in the dataset are highly redundant (sample N and\n",
    "sample N + 1 will have most of their timesteps in common), it would be wasteful\n",
    "to explicitly allocate every sample. Instead, you’ll generate the samples on the\n",
    "fly using the original data.\n",
    "\n",
    "You’ll preprocess the data by subtracting the mean of each timeseries and dividing by\n",
    "the standard deviation. You’re going to use the first 200,000 timesteps as training data,\n",
    "so compute the mean and standard deviation only on this fraction of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* data —The original array of floating-point data, which you normalized in listing 6.32.\n",
    "* lookback —How many timesteps back the input data should go.\n",
    "* delay —How many timesteps in the future the target should be.\n",
    "* min_index and max_index —Indices in the data array that delimit which time-\n",
    "steps to draw from. This is useful for keeping a segment of the data for valida-\n",
    "tion and another for testing.\n",
    "* shuffle —Whether to shuffle the samples or draw them in chronological order.\n",
    "* batch_size —The number of samples per batch.\n",
    "* step —The period, in timesteps, at which you sample data. You’ll set it to 6 in\n",
    "order to draw one data point every hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator yielding timeseries samples and their targets\n",
    "def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    \n",
    "    i = min_index + lookback\n",
    "    \n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        \n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        \n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preparing the training, validation, and test generators\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                        lookback=lookback,\n",
    "                        delay=delay,\n",
    "                        min_index=0,\n",
    "                        max_index=200000,\n",
    "                        shuffle=True,\n",
    "                        step=step,\n",
    "                        batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "                            lookback=lookback,\n",
    "                            delay=delay,\n",
    "                            min_index=300001,\n",
    "                            max_index=None,\n",
    "                            step=step,\n",
    "                            batch_size=batch_size)\n",
    "#How many steps to draw from val_gen in order to see the\n",
    "#entire validation set\n",
    "val_steps = (300000 - 200001 - lookback)\n",
    "test_steps = (len(float_data) - 300001 - lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98559"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A common-sense, non-machine-learning baseline\n",
    "\n",
    "Before you start using black-box deep-learning models to solve the temperature-\n",
    "prediction problem, let’s try a simple, common-sense approach. It will serve as a sanity\n",
    "check, and it will establish a baseline that you’ll have to beat in order to demonstrate\n",
    "the usefulness of more-advanced machine-learning models. Such common-sense base-\n",
    "lines can be useful when you’re approaching a new problem for which there is no\n",
    "known solution (yet). A classic example is that of unbalanced classification tasks,\n",
    "where some classes are much more common than others. If your dataset contains 90%\n",
    "instances of class A and 10% instances of class B, then a common-sense approach to\n",
    "the classification task is to always predict “A” when presented with a new sample. Such\n",
    "a classifier is 90% accurate overall, and any learning-based approach should therefore\n",
    "beat this 90% score in order to demonstrate usefulness. Sometimes, such elementary\n",
    "baselines can prove surprisingly hard to beat.\n",
    "\n",
    "In this case, the temperature timeseries can safely be assumed to be continuous\n",
    "(the temperatures tomorrow are likely to be close to the temperatures today) as well\n",
    "as periodical with a daily period. Thus a common-sense approach is to always predict\n",
    "that the temperature 24 hours from now will be equal to the temperature right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, targets = next(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 240, 14)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "print(samples.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Computing the common-sense baseline MAE\n",
    "def evaluate_naive_method():\n",
    "    batch_maes = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(val_gen)\n",
    "        preds = samples[:, -1, 1]\n",
    "        mae = np.mean(np.abs(preds - targets))\n",
    "        batch_maes.append(mae)\n",
    "    print(np.mean(batch_maes))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_naive_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields an MAE of 0.29. Because the temperature data has been normalized to be\n",
    "centered on 0 and have a standard deviation of 1, this number isn’t immediately inter-\n",
    "pretable. It translates to an average absolute error of 0.29 × temperature_std degrees\n",
    "Celsius: 2.57 ̊C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic machine-learning approach\n",
    "In the same way that it’s useful to establish a common-sense baseline before trying\n",
    "machine-learning approaches, it’s useful to try simple, cheap machine-learning mod-\n",
    "els (such as small, densely connected networks) before looking into complicated and\n",
    "computationally expensive models such as RNN s. This is the best way to make sure any\n",
    "further complexity you throw at the problem is legitimate and delivers real benefits.\n",
    "\n",
    "The following listing shows a fully connected model that starts by flattening the\n",
    "data and then runs it through two Dense layers. Note the lack of activation function on\n",
    "the last Dense layer, which is typical for a regression problem. You use MAE as the loss.\n",
    "Because you evaluate on the exact same data and with the exact same metric you did\n",
    "with the common-sense approach, the results will be directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##Training and evaluating a densely connected model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                                steps_per_epoch=500,\n",
    "                                epochs=20,\n",
    "                                validation_data=val_gen,\n",
    "                                validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the validation losses are close to the no-learning baseline, but not reliably.\n",
    "This goes to show the merit of having this baseline in the first place: it turns out to be\n",
    "not easy to outperform. Your common sense contains a lot of valuable information\n",
    "that a machine-learning model doesn’t have access to.\n",
    "\n",
    "You may wonder, if a simple, well-performing model exists to go from the data to\n",
    "the targets (the common-sense baseline), why doesn’t the model you’re training find it\n",
    "and improve on it? Because this simple solution isn’t what your training setup is look-\n",
    "ing for. The space of models in which you’re searching for a solution—that is, your\n",
    "hypothesis space—is the space of all possible two-layer networks with the configuration\n",
    "you defined. These networks are already fairly complicated. When you’re looking for a\n",
    "solution with a space of complicated models, the simple, well-performing baseline may\n",
    "be unlearnable, even if it’s technically part of the hypothesis space. That is a pretty sig-\n",
    "nificant limitation of machine learning in general: unless the learning algorithm is\n",
    "hardcoded to look for a specific kind of simple model, parameter learning can some-\n",
    "times fail to find a simple solution to a simple problem.\n",
    "\n",
    "## A first recurrent baseline\n",
    "The previous approach first flattened the\n",
    "timeseries, which removed the notion of time from the input data. Let’s instead look\n",
    "at the data as what it is: a sequence, where causality and order matter. You’ll try a\n",
    "recurrent-sequence processing model—it should be the perfect fit for such sequence\n",
    "data, precisely because it exploits the temporal ordering of data points, unlike the first\n",
    "approach.\n",
    "Instead of the LSTM layer introduced in the previous section, you’ll use the GRU\n",
    "layer, developed by Chung et al. in 2014. 5 **Gated recurrent unit ( GRU )** layers work\n",
    "using the same principle as LSTM , but they’re somewhat streamlined and thus\n",
    "cheaper to run (although they may not have as much representational power as\n",
    "LSTM ). This trade-off between computational expensiveness and representational\n",
    "power is seen everywhere in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and evaluating a GRU-based model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                                steps_per_epoch=500,\n",
    "                                epochs=20,\n",
    "                                validation_data=val_gen,\n",
    "                                validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! You can significantly beat the common-\n",
    "sense baseline, demonstrating the value of machine learning as well as the superiority\n",
    "of recurrent networks compared to sequence-flattening dense networks on this type\n",
    "of task.\n",
    "\n",
    "The new validation MAE of ~0.265 (before you start significantly overfitting) translates\n",
    "to a mean absolute error of 2.35 ̊C after denormalization. That’s a solid gain on the\n",
    "initial error of 2.57 ̊C, but you probably still have a bit of a margin for improvement.\n",
    "\n",
    "## Using recurrent dropout to fight overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to regularize the representations formed by the recurrent gates\n",
    "of layers such as GRU and LSTM , a temporally constant dropout mask should be applied\n",
    "to the inner recurrent activations of the layer (a recurrent dropout mask). Using the\n",
    "same dropout mask at every timestep allows the network to properly propagate its\n",
    "learning error through time; a temporally random dropout mask would disrupt this\n",
    "error signal and be harmful to the learning process.\n",
    "Yarin Gal did his research using Keras and helped build this mechanism directly\n",
    "into Keras recurrent layers. Every recurrent layer in Keras has two dropout-related\n",
    "arguments: dropout , a float specifying the dropout rate for input units of the layer,\n",
    "    and recurrent_dropout , specifying the dropout rate of the recurrent units. Let’s add\n",
    "dropout and recurrent dropout to the GRU layer and see how doing so impacts overfit-\n",
    "ting. Because networks being regularized with dropout always take longer to fully con-\n",
    "verge, you’ll train the network for twice as many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and evaluating a dropout-regularized GRU-based model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "dropout=0.2,\n",
    "recurrent_dropout=0.2,\n",
    "input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                            steps_per_epoch=500,\n",
    "                            epochs=40,\n",
    "                            validation_data=val_gen,\n",
    "                            validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking recurrent layers\n",
    "Because you’re no longer overfitting but seem to have hit a performance bottleneck,\n",
    "you should consider increasing the capacity of the network. Recall the description of\n",
    "the universal machine-learning workflow: it’s generally a good idea to increase the\n",
    "capacity of your network until overfitting becomes the primary obstacle (assuming\n",
    "you’re already taking basic steps to mitigate overfitting, such as using dropout). As\n",
    "long as you aren’t overfitting too badly, you’re likely under capacity.\n",
    "Increasing network capacity is typically done by increasing the number of units in\n",
    "the layers or adding more layers. Recurrent layer stacking is a classic way to build\n",
    "more-powerful recurrent networks: for instance, what currently powers the Google\n",
    "Translate algorithm is a stack of seven large LSTM layers—that’s huge.\n",
    "To stack recurrent layers on top of each other in Keras, all intermediate layers\n",
    "should return their full sequence of outputs (a 3D tensor) rather than their output at\n",
    "the last timestep. This is done by specifying return_sequences=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training and evaluating a dropout-regularized, stacked GRU model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,\n",
    "dropout=0.1,\n",
    "recurrent_dropout=0.5,\n",
    "return_sequences=True,\n",
    "input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',\n",
    "dropout=0.1,\n",
    "recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "steps_per_epoch=500,\n",
    "epochs=40,\n",
    "validation_data=val_gen,\n",
    "validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because you’re still not overfitting too badly, you could safely increase the size of\n",
    "your layers in a quest for validation-loss improvement. This has a non-negligible\n",
    "computational cost, though.\n",
    "* Adding a layer didn’t help by a significant factor, so you may be seeing diminish-\n",
    "ing returns from increasing network capacity at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using bidirectional RNNs\n",
    "\n",
    "RNN s are notably order dependent, or time dependent: they process the timesteps\n",
    "of their input sequences in order, and shuffling or reversing the timesteps can com-\n",
    "pletely change the representations the RNN extracts from the sequence. This is pre-\n",
    "cisely the reason they perform well on problems where order is meaningful, such as\n",
    "the temperature-forecasting problem. A bidirectional RNN exploits the order sensitiv-\n",
    "ity of RNN s: it consists of using two regular RNN s, such as the GRU and LSTM layers\n",
    "you’re already familiar with, each of which processes the input sequence in one direc-\n",
    "tion (chronologically and antichronologically), and then merging their representa-\n",
    "tions. By processing a sequence both ways, a bidirectional RNN can catch patterns that\n",
    "may be overlooked by a unidirectional RNN .\n",
    "\n",
    "Remarkably, the fact that the RNN layers in this section have processed sequences in\n",
    "chronological order (older timesteps first) may have been an arbitrary decision. At least,\n",
    "it’s a decision we made no attempt to question so far. Could the RNN s have performed\n",
    "well enough if they processed input sequences in antichronological order, for instance\n",
    "(newer timesteps first)? Let’s try this in practice and see what happens. All you need to\n",
    "do is write a variant of the data generator where the input sequences are reverted along\n",
    "the time dimension (replace the last line with yield samples[:, ::-1, :], targets )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying GRU layer will typically be better at\n",
    "remembering the recent past than the distant past, and naturally the more recent\n",
    "weather data points are more predictive than older data points for the problem (that’s\n",
    "what makes the common-sense baseline fairly strong). Thus the chronological version\n",
    "of the layer is bound to outperform the reversed-order version. Importantly, this isn’t\n",
    "true for many other problems, including natural language: intuitively, the importance\n",
    "of a word in understanding a sentence isn’t usually dependent on its position in the sen-\n",
    "tence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "num_words=max_features)\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_test = [x[::-1] for x in x_test]\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='binary_crossentropy',\n",
    "metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "epochs=10,\n",
    "batch_size=128,\n",
    "validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get performance nearly identical to that of the chronological-order LSTM .\n",
    "Remarkably, on such a text dataset, reversed-order processing works just as well as\n",
    "chronological processing, confirming the hypothesis that, although word order does\n",
    "matter in understanding language, which order you use isn’t crucial. Importantly, an\n",
    "RNN trained on reversed sequences will learn different representations than one\n",
    "trained on the original sequences, much as you would have different mental models if\n",
    "time flowed backward in the real world—if you lived a life where you died on your first\n",
    "day and were born on your last day. In machine learning, representations that are dif-\n",
    "ferent yet useful are always worth exploiting, and the more they differ, the better: they\n",
    "offer a new angle from which to look at your data, capturing aspects of the data that\n",
    "were missed by other approaches, and thus they can help boost performance on a\n",
    "task. This is the intuition behind ensembling, a concept we’ll explore in chapter 7.\n",
    "A bidirectional RNN exploits this idea to improve on the performance of chronological-\n",
    "order RNNs.\n",
    "\n",
    "To instantiate a bidirectional RNN in Keras, you use the Bidirectional layer, which takes\n",
    "as its first argument a recurrent layer instance. Bidirectional creates a second, separate\n",
    "instance of this recurrent layer and uses one instance for processing the input sequences\n",
    "in chronological order and the other instance for processing the input sequences in\n",
    "reversed order. Let’s try it on the IMDB sentiment-analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and evaluating a bidirectional LSTM\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "epochs=10,\n",
    "batch_size=128,\n",
    "validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs slightly better than the regular LSTM you tried in the previous section,\n",
    "achieving over 89% validation accuracy. It also seems to overfit more quickly, which is\n",
    "unsurprising because a bidirectional layer has twice as many parameters as a chrono-\n",
    "logical LSTM . With some regularization, the bidirectional approach would likely be a\n",
    "strong performer on this task.\n",
    "Now let’s try the same approach on the temperature-prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training a bidirectional GRU\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(layers.Bidirectional(\n",
    "layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "steps_per_epoch=500,\n",
    "epochs=40,\n",
    "validation_data=val_gen,\n",
    "validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs about as well as the regular GRU layer. It’s easy to understand why: all the\n",
    "predictive capacity must come from the chronological half of the network, because the\n",
    "antichronological half is known to be severely underperforming on this task (again,\n",
    "because the recent past matters much more than the distant past in this case).\n",
    "\n",
    "## Going even further\n",
    "There are many other things you could try, in order to improve performance on the\n",
    "temperature-forecasting problem:\n",
    "* Adjust the number of units in each recurrent layer in the stacked setup. The\n",
    "current choices are largely arbitrary and thus probably suboptimal.\n",
    "* Adjust the learning rate used by the RMSprop optimizer.\n",
    "* Try using LSTM layers instead of GRU layers.\n",
    "* Try using a bigger densely connected regressor on top of the recurrent layers:\n",
    "that is, a bigger Dense layer or even a stack of Dense layers.\n",
    "* Don’t forget to eventually run the best-performing models (in terms of valida-\n",
    "tion MAE ) on the test set! Otherwise, you’ll develop architectures that are over-\n",
    "fitting to the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, **deep learning is more an art than a science**. We can provide guidelines that\n",
    "suggest what is likely to work or not work on a given problem, but, ultimately, every\n",
    "problem is unique; you’ll have to evaluate different strategies empirically. There is\n",
    "currently no theory that will tell you in advance precisely what you should do to opti-\n",
    "mally solve a problem. You must iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "Here’s what you should take away from this section:\n",
    "* As you first learned in chapter 4, when approaching a new problem, it’s good to\n",
    "first establish common-sense baselines for your metric of choice. If you don’t\n",
    "have a baseline to beat, you can’t tell whether you’re making real progress.\n",
    "* Try simple models before expensive ones, to justify the additional expense.\n",
    "Sometimes a simple model will turn out to be your best option.\n",
    "* When you have data where temporal ordering matters, recurrent networks are\n",
    "a great fit and easily outperform models that first flatten the temporal data.\n",
    "* To use dropout with recurrent networks, you should use a time-constant drop-\n",
    "out mask and recurrent dropout mask. These are built into Keras recurrent lay-\n",
    "ers, so all you have to do is use the dropout and recurrent_dropout arguments\n",
    "of recurrent layers.\n",
    "* Stacked RNN s provide more representational power than a single RNN layer.\n",
    "They’re also much more expensive and thus not always worth it. Although they\n",
    "offer clear gains on complex problems (such as machine translation), they may\n",
    "not always be relevant to smaller, simpler problems.\n",
    "* Bidirectional RNN s, which look at a sequence both ways, are useful on natural-\n",
    "language processing problems. But they aren’t strong performers on sequence\n",
    "data where the recent past is much more informative than the beginning of the\n",
    "sequence.\n",
    "\n",
    "### NOTE\n",
    "There are two important concepts we won’t cover in detail here: recur-\n",
    "rent attention and sequence masking. Both tend to be especially relevant for\n",
    "natural-language processing, and they aren’t particularly applicable to the\n",
    "temperature-forecasting problem. We’ll leave them for future study outside of\n",
    "this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SEQUENCE PROCESSING WITH COVNETS P.238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
