{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AUTOENCODERS**\n",
    "\n",
    "### **First Ideas**\n",
    "\n",
    "An Autoencoder is a data compression algorithm which takes the input and going through a compressed representation and gives the reconstructed output.\n",
    "\n",
    "\n",
    "The auto-encoder is an unsupervised method. There is no target feature you’re trying to predict, what you’re trying to do is find a low-dimensional representation of the inputs instead.\n",
    "\n",
    "It is worth noting that auto-encoders are often used in supervised learning; you train an auto-encoder on the inputs alone, and then use the low-dimensional encoding as inputs to the supervised learning network. This is not dissimilar to first using PCA to reduce the dimensionality, and then training a prediction model on the reduced dimensionality inputs.\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "Applications of autoencoders include:\n",
    "\n",
    "* Anomaly detection\n",
    "* Data denoising (ex. images, audio)\n",
    "* Image inpainting\n",
    "* Information retrieval\n",
    "\n",
    "### **Description**\n",
    "\n",
    "Here, we are going to study four different types of autoencoders.\n",
    "\n",
    "* Vainilla Autoencoders\n",
    "* Variational Autoencoders (VAE) (2013)\n",
    "* Denoising Autoencoders (DAE) (2008)\n",
    "* Sparse Autoencoders (SAE) (2008)\n",
    "\n",
    "\n",
    "\n",
    "![alt text](fig/autoencoders.png \"Title\")\n",
    "\n",
    "\n",
    "Although, there are more ..\n",
    "\n",
    "\n",
    "*  Contractive Autoencoders (CAE) (2011)\n",
    "* Stacked Convolutional Autoencoders (SCAE) (2011)\n",
    "* Recursive Autoencoders (RAE) (2011)\n",
    "* Adversarial Autoencoders (AAE) (2015)\n",
    "* Wasserstein Autoencoders (WAE) (2017)\n",
    "* Autoencoders for Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Types of autoencoders**\n",
    "\n",
    "* **Vainilla Autoencoder (AE)**\n",
    " \n",
    "Here, the neural network architecture has a **bottleneck** in the network which forces a compressed knowledge representation of the original input. If the input features were each independent of one another, this compression and subsequent reconstruction would be a very difficult task. However, if some sort of structure exists in the data (ie. correlations between input features), this structure can be learned and consequently leveraged when forcing the input through the network's bottleneck.\n",
    "\n",
    "The **bottleneck** is a key attribute of our network design; without the presence of an information bottleneck, our network could easily learn to simply memorize the input values by passing these values along through the network.\n",
    "\n",
    "A bottleneck constrains the amount of information that can traverse the full network, forcing a learned compression of the input data.\n",
    "\n",
    "**NOTE:** if we were to construct a linear network (ie. without the use of nonlinear activation functions at each layer) we would observe a similar dimensionality reduction as observed in PCA. Because neural networks are capable of learning nonlinear relationships, this can be thought of as a more powerful (nonlinear) generalization of PCA. Whereas PCA attempts to discover a lower dimensional hyperplane which describes the original data, autoencoders are capable of learning nonlinear structures\n",
    "\n",
    "![alt text](fig/autoencoders_2.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that we'd like our model to discover latent attributes within our data, it's important to ensure that the autoencoder model is not simply learning an efficient way to memorize the training data. **Similar to supervised learning problems, we can employ various forms of regularization to the network in order to encourage good generalization properties**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **Variational Autoencoder (VAE)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Denoising AutoEncoder (DAE)**\n",
    "\n",
    "One possible approach towards developing a generalizable model is to slightly corrupt the input data but still maintain the uncorrupted data as our target output.\n",
    "\n",
    "With this approach, our model isn't able to simply develop a mapping which memorizes the training data because our input and target output are no longer the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](fig/denoising.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sparse Autoencoder (SAE)**\n",
    "\n",
    "The structure is similar to AE, but the hidden cells count are equal or bigger than the input/output layer cells count. \n",
    "\n",
    "Sparse autoencoders offer us an alternative method for introducing an information bottleneck without requiring a reduction in the number of nodes at our hidden layers. Rather, we'll construct our loss function such that we penalize activations within a layer.It's worth noting that this is a different approach towards regularization, as we normally regularize the weights of a network, not the activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **References**\n",
    "\n",
    "* https://www.jeremyjordan.me/autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
